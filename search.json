[{"title":"Introduction to TLU and Perceptron","url":"/2022/05/19/Introduction-to-TLU-and-Perceptron/","content":"Threshold Logic Unit阈值逻辑单元 (TLU) 是机器学习模型的基本形式，由单个输入单元（和相应的权重）和激活函数组成。请注意，TLU 是 AI 神经元/计算单元的最基本形式，其知识将为机器学习和深度学习的高级主题奠定基础。TLU 基于在高水平上模仿生物神经元的功能。一个典型的神经元接收来自传入神经元的大量输入，每个输入都与权重相关。加权输入在接收神经元（传出）中进行调制，并且神经元相应地做出响应——触发/产生脉冲 (1) 或不触发/无脉冲 (0)。这是在 TLU 中通过激活函数实现的，该激活函数采用激活函数a 作为生成预测的输入y。定义了一个阈值θ，如果超过该阈值，模型将产生一个输出，否则没有输出。\n在 TLU 中，每个输入 xᵢ都与一个权重wᵢ相关联，其中计算加权输入的总和（输入权重的乘积）来决定激活 a。\n\na = \\sum^N_{i=1} w_i·x_i虽然输入保持不变，但权重是随机初始化的，并通过训练技术进行调整。对于 TLU，训练过程依赖于一对示例xᵢ,yᵢ，对应于任意数据点xᵢ 及其类yᵢ。这种学习形式被称为监督学习，因为数据实例和目标都用于指导学习过程。最终输出或预测基于加权输入的总和\n$y = 1$  $if$  $a ≥ θ$   $otherwise$  $y=0$ 。\nTLU 中的权重调整从学习模型的角度来看，数据点属于由一个或多个决策面划分的组。因此，学习模型的目标是完成特定任务，例如训练后的对象分类。在训练期间，模型识别一组参数或自由参数（例如权重），与输入结合使用，通过识别决策面来实现预期目标。在任何基于 ML 的模型中，识别使模型能够识别显着特征的自由参数都是至关重要的。\nTLU 的阈值被初始化为一个标量，用作基线或偏差，即在神经元触发之前要达到的基线。为了统一，阈值被视为具有恒定输入的权重，将其乘以-1与xᵢ×wᵢ&gt; θ相加到模型中进行训练，即xᵢ×wᵢ+(-1)×θ=0。因此，定义并重复应用学习规则，直到获得权重向量的正确设置。对权重向量的调整是给定实例输出的函数。在此基础上，调整参数——根据学习规则增加或减少。对于每个训练时期或制度，权重都会发生微小的变化。一个训练有素的模型应该能够正确分类新的例子。\n一些技术说明在增加阈值的情况下，TLU 的动作要么是正的，要么是负的，由下式给出：$w⋅x ≥ 0 → y = 1$   or   $ w⋅x &lt; 0 → y = 0$ 。由于在训练过程中输入向量不受影响（保持不变），因此仅调整权重向量以与输入向量正确对齐。使用学习率 α (0&lt;α&lt;1) 来控制过程，形成一个更接近输入向量 x 的新向量 w`。根据决策规则，调整权重可以基于权重向量的加法或减法；由于两者都有可能，因此使用结合两者的学习规则。因此， w`= w - αx 或w`= w + αx 导致w`= w + α ( t - y`) x ，其中( t - y`)用于决定调整方向（增加或减少）。这种关系可以用几种方式表示：\n\n在权重向量的变化方面：因为 δw = w`- w 而且 w` = w + α ( t - y ) x ，所以 δw = α( t - y )x\n就权重向量的分量而言： $δw_i = α( t_i - y_i )x_i$  其中 $i = 1$ 到 $n+1$。\n\nTLU 实施建立理论基础后，下一步是描述和实施模型的训练阶段。基本上，实现基于以下步骤：\n\n识别输入和相应的表示\n识别问题中的自由参数\n指定学习规则\n调整自由参数进行优化\n评估模型\n\n感知器学习规则：这里的实现是基于感知器训练规则，它保证生成一个有效的权重向量来分离线性可分的数据。\n\nIf two classes of vector X,Y are linearly separable, then application of the perceptron training algorithm will eventually result in a weight vector w₀ such that w₀ define a TLU whose decision hyperplane separate X and Y — Gurney (1997)*, pp. 43.\n\nA basic python class to implement the TLU:\n#import relevant package(s)import numpy as npclass TLU(object):    #initialise the parameter(s) for class operationalisation    def __init__(self, input_size):        self.weights = np.zeros(input_size+1)        def activate_tlu(self, x):        return 1 if x&gt;= 0 else 0           def predict_tlu(self, row):        # this predicts individual row in a given dataset        xw = np.array(row).dot(self.weights)            a = self.activate_tlu(xw)        return a                   def train_tlu(self, data, targets, epochs, lrate):        #training to identify the right setting for the weights        for epoch in range(epochs):            for row, t in zip(data, targets):                row = np.insert(row, 0,-1) #inserts the bias                pred = self.predict_tlu(row)                error = t - pred                # adjust the weights vector ...                if pred != t:                    for r in range(len(self.weights)):                        self.weights[r] = self.weights[r] +   (lrate*error*row[r])                else:                    continue        return self.weights             def __str__(self):        return(&#x27;TLU Iteration!\\n&#x27;)\nInstantiate the class object for training and prediction:\ndef tlu_pred(model,data,targets,epochs, lrate=0.2,toPrint=True):    adj_w = model.train_tlu(data, targets, epochs, lrate)     if toPrint:        print(model)    return adj_w\nDataset: Irrespective of the problem to solve, the input needs to be transformed into numeric (usually real or binary values). Consider a basic input vectors: x₁=[0011] and x₂=[0101]. The free parameter to search is the weights vector, which is randomly initialised to kick-start the learning.\n# Logical AND Data:andData = np.array([[0,0],[0,1],[1,0],[1,1]])andTargets = np.array([0,0,0,1])# Logical OR Data:orData = np.array([[0,0],[0,1],[1,0],[1,1]])orTargets = np.array([0,1,1,1])\nClass instantiation:\nmodel = TLU(input_size= 2)# class instantiation ...#AND Data:tlu_prediction(model, andData, andTargets, epochs=11,lrate=0.3)#OR Data:tlu_prediction(model, orData, orTargets, epochs=11,lrate=0.3)\nSample outputs using AND data:\nTLU Iteration!Main Targets:  [0 0 0 1] Main Inputs:  [[0 0] [0 1] [1 0] [1 1]] Adjusted weights:  [0.9 0.6 0.3]\nUsing the OR data:\nTLU Iteration!Main Targets:  [0 1 1 1] Main Inputs:  [[0 0] [0 1] [1 0] [1 1]] Adjusted weights:  [0.6 0.6 0.6]\n原文：https://medium.com/@isa.inuwa/introduction-to-tlu-and-perceptron-e0d9c351e6a6\n","categories":["Natural Network"]},{"title":"Dynamic Bayesian Networks for Student Modeling","url":"/2022/03/12/Knowledge%20Tracing-Dynamic-Bayesian-Networks-for-Student-Modeling/","content":"作者Tanja K€aser , Severin Klingler, Alexander G. Schwing, and Markus Gross\n摘要智能教学系统使课程适应学生个人的需求。因此，学生知识的准确表达和预测至关重要。贝叶斯知识追踪（BKT）是一种流行的学生建模方法。然而，BKT模型的结构使其无法表示学习领域不同技能之间的层次结构和关系。另一方面，动态贝叶斯网络（DBN）能够在一个模型中联合表示多种技能。在这项工作中，我们建议使用DBN进行学生建模。我们引入了一种约束优化算法来学习这类模型的参数。我们在数学、拼写学习和物理等不同学习领域的五个大规模数据集上广泛评估和解释了我们方法的预测准确性。此外，我们还提供了与以往学生建模方法的比较，并分析了不同学生建模技术对教学策略的影响。我们证明，我们的方法在预测所有学习领域的未知数据的准确性方面优于以前的技术，并产生有意义的教学策略。\n1 简介建模和预测学生知识是智能辅导系统 (ITS) 的基本组成部分。 在这些系统中，下一步要处理的挑战的选择是基于学生模型对学生当前知识的估计和预测。 学生模型的预测准确性和行为直接影响系统的教学策略 [1]，进而影响教学决策的质量。 因此，准确的学生模型对于个性化至关重要，即学习内容和难度级别对每个学生的适应。 在建模方面进行了大量的研究，即构建准确的学生模型，在评估方面，即定义适当的误差度量来评估学生模型的预测准确性。\n","categories":["Knowledge Tracing"]},{"title":"KPT","url":"/2022/03/23/Knowledge%20Tracing-KPT/","content":"\n\n\n\n\n","categories":["Knowledge Tracing"]},{"title":"1 Machine Learning","url":"/2022/02/28/bert%20huang%20Fall%202015%20Machine%20Learning-1-Machine-Learning/","content":"Machine Learning\n\nTypes of ML\n\nModel Selection in Machine Learning\n\n","categories":["bert huang Fall 2015 Machine Learning"]},{"title":"从数据视角透析认知追踪：框架、问题及启示","url":"/2022/02/28/Knowledge%20Tracing-%E4%BB%8E%E6%95%B0%E6%8D%AE%E8%A7%86%E8%A7%92%E9%80%8F%E6%9E%90%E8%AE%A4%E7%9F%A5%E8%BF%BD%E8%B8%AA-%E6%A1%86%E6%9E%B6%E3%80%81%E9%97%AE%E9%A2%98%E5%8F%8A%E5%90%AF%E7%A4%BA/","content":"作者孙建文 栗大智 彭 晛 邹 睿 王 佩\n摘要认知追踪是一种数据驱动的学习者建模技术，被广泛应用于智能导学、智能课堂编排等系统。尤其是2015年深度神经网络被引入认知追踪任务以来，认知追踪成为智能教育领域的研究热点。针对当前研究普遍存在的“重模型、轻数据”以及数据处理不一致等问题，本研究基于近六年国内外35篇有关认知追踪的论文，全面梳理和分析其中被高频使用的数据集，提出以学生、知识、问题三个对象及六类交互关系为核心的认知追踪概念框架，为深层次理解数据内涵和统一数据操作提供指导。本研究还运用该框架对数据集特征进行分类，围绕数据重复、数据顺序、支架题目、技能缺失以及多技能题目等关键问题进行数据一致性分析，特别是针对多技能题目，提出了基于多热编码的表示方法。本研究最后从五方面讨论了认知追踪及未来智能教育的发展趋势:从个体自主学习到多模式混合学习、从单一学习行为到多模态数据融合、从深度学习算法黑箱到可解释分析、从数据驱动到数据与知识联合驱动，以及从技术意识垄断回归教育价值本位，为拓展认知追踪研究边界、促进智能教育创新突破提供参考与指引。\n一、引言教育情境可计算、学习主体可理解、学习服务可定制是个性化学习面临的三大挑战，其中，学习主体是教育系统的核心要素，对学习主体的精准洞察是开展“因材施教”的前提( 刘三女牙等，2020)。\n【定义与来源】认知追踪(Knowledge Tracing，KT)，也被译为“知识追踪”，本研究认为译作“认知追踪”更能表达其追踪对象是主体的人，而非客体的知识这一意蕴。\n认知追踪的思想源于美国著名心理学家阿特金森( Atkinson ＆ Paulson，1972) ，1995年被美国卡耐基梅隆大学科比特等(Corbett ＆ Anderson，1995) 引入智能导学系统，并提出贝叶斯认知追踪方法( Bayesian Knowledge Tracing，BKT) ，其任务是根据学生的答题记录，对学生的知识掌握状态进行建模，目标是预测学生答对下一道题的概率。\n【基本数据处理】问题：多项研究使用相同的数据集和模型，却得到不同的实验结果，原因是不同学者对数据集的处理操作不一，导致实验结果出现差异。\n普遍处理方法：威尔逊等( Wilsonet al.，2016) 对深度认知追踪方法的数据处理方法提出疑问，认为应删除数据集的重复记录，按照学习系统使用过程实际顺序进行排列。后续研究大多采纳了这一建议。\n【数据角度分析预测性能】有学者( Xiong et al. ，2016) 从数据角度就深度认知追踪方法大幅提升预测性能提出三点质疑，并从数据重复性、支架题目影响、多技能题目处理三方面，将数据分为三个子集，并通过实验证明数据处理方式对实验结果的显著影响。这对后续研究有较大的参考价值，如GIKT、qDKT 等模型均直接采用这一数据集划分方式( Yang et al.  ，2020；Sonkar et al.，2020) 。近年，多位学者从不同角度讨论如何对数据进行更合理的操作( Zhang et al.，2017; Lee ＆ Yeung，2019; Xu ＆ Davenport，2020) 。\n【数据一致性挑战】\n研究偏好——重模型、轻数据\n人工智能与教育的交叉促进了认知追踪的发展，同时也沿袭了人工智能研究领域普遍存在的“重模型、轻数据”惯性。谷歌研究员桑巴希万等( Sambasivan et al. ，2021）指出，学者们往往青睐模型创新，很少专门围绕数据展开研究，但数据质量在很大程度上决定了模型及后续应用的成败，造成数据级联问题。\n\n数据认知\n人们对数据的操作是否合理很大程度源于对数据内涵的理解是否准确，而数据的含义通常由其逻辑和业务背景决定。\n\n\n不同的认知追踪数据集产生于不同的学习系统，背后有不同的教学设计与策略，由此带来数据一致性认知挑战。\n二、概念框架（一）数据集调研【收集方法及数据集分布结构】本研究聚焦于2015 年深度认知追踪提出以来，新模型不断涌现但数据处理不一致现象愈发严重这一问题展开调研。研究者以 2015—2021 年为文献检索时间范围，在中国知网、Springer、ACM、arXiv 和Web of Science 等数据库中，分别以“知识追踪”“认知追踪”“Knowledge Tracing”为关键词，搜得 93 篇文献，排除未明确描述数据集的文献，最终得到论文35 篇，涉及18 个数据集、29 个模型及改进算法( 见图1) ，包括ASSISTments 系列数据集( 50.5% ) 、Statics 数据集( 13.3% ) 、Algebra 系列数据集( 12.4% )和 Synthetic-5 数据集( 10.5% ) 。\n\n\n【1. ASSIS Tments系列】ASSISTments是美国伍斯特理工学院开发的在线学习平台(Feng et al.  ，2009) ，其数据集包含四个子集: ASSISTments2009-2010 ( ASS09) 、ASSISTments2012-2013( ASS12)、ASSISTments2014-2015 ( ASS15) 、ASSISTments Challenge。\n\n\n总体情况\n\n每一次发布任务有唯一assignment_id，每一次发布是从一个题库中抽取的题目，因此 共享一个sequence_id。\n每一大题（包括主要问题和脚手架问题）共享一个assistment_id。\n题目分为两类：main problem和scaffolding problem。\n在数据集中，main problem由字段original=1表示，scaffolding problem由字段original=0表示。\n不论是main problem还是scaffolding problem，都有唯一problem_id标识。\ncorrect：correct的值只与第一次操作有关，对1错0。\nattempt_count：尝试的次数。\nhint_count：点击提示的次数。\nfirst_action：进入新题的第一次操作。\n\n其他字段\n\nBottom_hint：\n0-学生没有用完所有hint\n1-学生用完了所有hint\n空白-没有用hint\n\nOpportunity：学生有多少次机会回答该问题\n\nOpportunity_original：只计算main problem有多少次机会回答\n\nms_first_response**:**学生从看到题目到第一次操作之间的时间\n\ntutor_mode：builder在设置题目时，可选择该题目是tutor模式还是test模式\n\nPosition：该题目在发布任务中的位置\n\nType：题目的展示方式，有linear，random，mastery三种\n\nBase_sequence_id：一个题库可能被发布多次，这里指向原始题库的id\n\nOverlap_time*：学生做题花费的时间\nTemplate_id：同一模板可生成相似的问题\nSkill_id*:题目包含技能的id\n\n\n\nASS09\n由2009—2010 年采集的数据构成，该数据集以题目—日志的形式收集学生数据，其中行是学生回答某道题的记录，内容包括学生和题目的交互特征，如是否回答正确、是否求助等。此外，数据集还记录了学校、班级等学生属性特征，以及题目编号、位置等题目特征。ASS09 由2009—2010 年采集的数据构成，被分成“非技能建构数据( Non-skill builder data) ”和“技能建构数据( Skill builder data) ”两部分，后者又被称为掌握学习数据，即学生必须连续正确回答三道题才算掌握了该项技能。从频次分布图看，该数据集使用最为广泛。\n\nASS12\n由 2012—2013 年采集的数据构成，在 ASS09 数据集基础上增加了挫折程度、困惑程度、注意力集中程度、厌倦程度等特征描述学生的情感状态。\n\nASS15\n由 2014—2015 年采集的数据构成，仅包含100 个单技能题目，没有支架题目，特征数较少。\n\nASSISTments Challenge\nASSISTments Challenge 源于2017 年国际数据挖掘竞赛，其特点是特征较为丰富，共82 个特征，但较少被使用。\n\n\n【2. Algebra 系列】Algebra 是2010 年国际知识发现和数据挖掘竞赛 KDD Cup 发布的公开数据集( Stamper et al.  ，2010) ，包含 Algebra Ⅰ 2005-2006、Algebra Ⅰ 2006-2007、Bridge to Algebra 2006-2007 等三个开发数据集，以及 Algebra Ⅰ 2008-2009、Bridge to Algebra2008-2009 两个挑战数据集。其中，开发数据集包含学生真实答题结果在内的所有完整信息，旨在帮助参赛者熟悉数据格式和训练模型。挑战数据集不含学生答题结果，需要参赛者给出预测结果并提交。多数研究只使用 Algebra 开发数据集。\n【3. Statics】Statics 是一门大学在线课程收集的数据，包含361092 条记录，涉及 335 名学生和 85 项技能，共包含46 个特征( Steif ＆ Bier，2014; Koedinger et al.，2010)。\n【4. Synthetic-5】Synthetic-5( 又称为 Simulated-5) 是深度认知追踪方法的提出者皮希构造的模拟数据集，模拟了4000 名学生 50 道题的答题情况，且学生答题序列相同。题目从五个模拟技能中抽取，每道题对应一项技能，重复实验 20 次，最后评估平均准确度和标准误差。\n(二) 认知追踪概念框架综上，当前研究主要面向自主学习场景，以学生做题为主要学习活动，以预测学生的技能掌握状态或者答对下一题的概率为目标。从系统论角度看，学生、技能与题目构成了认知追踪问题域的三大核心要素，但认知追踪的适用场景并不限于此。随着智能教育技术的发展，认知追踪可广泛应用于多主体协作学习、多步骤问题解决、多层次知识能力诊断等更开放、复杂、高阶的学习场景。为了不失一般性，本研究将“知识”和“问题”分别作为“技能”和“题目”两个元素的泛化概念，建立了以“学生( Student) —知识( Knowledge) —问题( Problem) ”三个对象以及六类关系为核心的认知追踪概念框架( SKP，见图2) 。\n\n\n【1. 基本对象】学生、知识和问题构成了认知追踪数据处理过程的三个基本对象。其中：\n\n学生对象包括个人基本信息及其在不同学习场景留下的学习行为信息; \n知识对象包括知识名称、类型、层次等基本属性，以及知识描述等信息; \n问题对象包括问题类型、难度、区分度等基本属性，以及题干、答案、提示、解析等内容语义信息。\n\n【2. 交互关系**】认知追踪就是对三类对象之间关系的量化分析与建模。例如，学生知识掌握状态预测是利用“学生—问题”之间的交互和“问题—知识”之间的关联关系信息计算“学生—知识”之间的掌握概率。认知追踪概念框架包含的交互关系可分为两类:\n\n同类对象之间的交互，常被用作辅助信息融入认知追踪建模过程。其中，“学生—学生”交互关系主要体现在多主体协作学习场景，包括学生协作过程中形成的社交角色、互动行为和内容等信息;“知识—知识”交互主要用于描述知识结构或性质关系，包括知识图谱的上下位、先后修等关系，以及具有互逆性质的知识之间的关系等; “问题—问题”交互主要体现在问题序列背后的教学设计思想，如多个主干问题之间的递进关系、主干问题与支架问题之间的主次关系、多步骤问题之间的依赖关系等。\n不同对象之间的交互，是认知追踪建模过程使用的主要信息。其中，“学生—问题”交互主要用于描述学生回答或解决问题过程中产生的各类内容或行为信息，包括作答内容以及请求提示、查看答案或解析过程等行为信息; “问题—知识”交互主要用于描述问题与知识之间的关联关系，包括一对一、一对多、多对一和多对多等，如一对一表示一个问题仅关联一个知识点，一对多表示一个问题关联多个知识点; “学生—知识”交互用于描述学生对简单技能、高阶能力等不同层次知识的掌握状态。\n\n(三) 基于认知追踪概念框架的特征分类认知追踪数据集大多含有丰富的特征，对特征的概念化分类是对数据内涵理解及后续处理分析的认知基础。数据集虽然源于不同的学习平台和应用场景，其特征较为丰富，但均可按认知追踪概念框架统一分类，从而为在更高层次建立对特征含义的共识性理解提供可能。\n三、关键问题\n\n(一)数据重复表一中近一半的研究工作涉及数据去重处理。ASS09、Alge05等数据集均存在大量的学习行为重复记录。虽然研究者无法从源头查证重复记录如何产生，但学生在学习平台的操作属于时序行为，学生不会在同一时间回答多个问题，产生多条记录。有研究( Xiong et al.  ，2016) 表明，这些重复数据实为冗余信息，可直接删除。一般而言，学生在学习平台上进行答题等操作后，后台数据库会生成语义丰富的学习行为记录，并在此基础上构建基于多特征的认知追踪模型( Sun et al.  ，2021) ，这是提升学习者建模能力的有效途径。因此，数据采集的特征丰富性、记录完整性是保障数据质量的基本要求，也是影响模型性能的关键因素。对于开放、复杂的智能学习环境以及大规模用户使用场景，如何高保真、高效地记录海量学习者的并发行为，是智能教育系统设计与实现需解决的基础性问题。\n(二)数据顺序数据顺序指学习行为记录输入模型的先后。通常，认知追踪建模会以学生真实答题顺序组织数据，这有利于模型捕捉数据中蕴含的学生认知状态变化规律，从而获得精准的预测结果。但也有部分研究考虑模型输入的答题序列长度对齐、按相同技能或学生重组答题序列等因素对数据进行专门处理，截断或打乱学生答题行为数据的原始顺序。例如，表一的数据集 a-1，将题目对应技能按编号重新排序后再输入模型( Piech et al.  ，2015)。\n从智能教育系统视角看，学生的学习行为序列可能蕴涵了特定的教学策略或设计思想。例如，ASSISTments 平台依据掌握学习理论设计和组织题目序列，学生按题目预设顺序作答反映了一种预期的认知状态变化模式或规律，即连续答对多道相同技能的题目意味着该生大概率掌握了该技能。其次，教育心理学研究发现( Ｒohrer et al.  ，2015) ，学生做数学题的过程中，不同技能的题目交叉出现比连续出现更有助于提升学习效果。\n(三)支架题目支架题目( Xiong et al. ，2016) 的设计思想源于支架式教学理论，旨在帮助学生穿越最近发展区，把认知引到更高水平。在学习平台上，支架题目与主题目所考察的技能相同或相近，但通常难度较低。表一中，部分研究认为，认知追踪的建模过程不应同等对待支架题目，因此直接删除了支架题目( Xiong et al.  ，2016; Liu et al.  ，2020; Xu ＆ Davenport，2020; Sonkar et al.  ，2020) 。实际上，支架题目作答行为是学生做题序列的有机组成部分，从数据真实性和完整性角度看，保留支架题目数据有利于更准确地建模学生认知状态的变化。\n对智能教育系统而言，支架是一种广泛用于多种学习场景的导学策略，其形式不限于题目，包括推进、提示、暗示等，目的是推动学生思考、保持学习动力。在认知追踪应用场景中，支架题目的设计也体现了这一思想。当学生遇到难题时，引导其解决支架类问题可有效促进学习。因此，支架题目作答行为数据通常隐含了学生认知水平提升这一重要信息。以 ASSISTments 平台为例，若学生答题记录既包含主题目，又包含支架题目，且连续答对多个支架题目，这意味着学生经历了最近发展区的认知发展过程，并从支架题目的引导中掌握了相应的技能。\n(四)技能缺失针对部分答题记录缺失对应的技能信息，相关研究一般直接删除记录( Zhang et al.  ，2017; Ha et al.  ，2018; Abdelrahman ＆ Wang，2019; Lee ＆ Ye-ung，2019; Ghosh et al.  ，2020; Gan et al.  ，2020) ，或填充一个固定值( Piech et al.  ，2015; Khajah et al.  ，2016; Wilson et al.  ，2016) 。前者虽然保证了数据的真实性，但会丢失大量有用信息，尤其是缺失记录占比较高对模型性能影响较大。后者为了处理简单，把所有技能缺失记录填充为一个新值，这会带来大量数据噪声，导致模型学到更多的错误模式。\n技能缺失折射出智能教育领域普遍存在的一个难题———智能化知识组织与资源标注。对学科知识的精细化组织，以及资源内容的深层次加工和语义化标注，是构建智能教育知识基础设施的核心任务。教育知识图谱正是该方向的研究热点，即在传统人工构建知识体系、标注知识资源的基础上，利用数据和知识双向驱动的方式往半自动或自动化方向发展。因此，对于技能缺失问题，简单删除记录或者填充固定值的做法都不合理，应考虑如何有效利用数据集的已有信息，运用相应策略补全缺失值。\n( 五) 多技能题目【1. 问题描述】在智能导学、自适应学习等认知追踪应用场景中，一道题通常不只关联一项技能，而是关联多项技能，此类题目可被称为多技能题目( Xiong et al. ，2016)。例如，一道求矩阵点积的题目可能关联向量运算、多项式运算等技能。认知追踪建模首先需要对特征进行编码，研究者通常采用机器学习领域主流的独热编码方式处理单技能题目( one-hot encoding) ，但对多技能题目的编码尚未达成共识。\n【2. 典型方法】\n\n多技能题目的编码方法可分为两种: 一是使用拆分策略，即将“一条包含多技能的记录”拆成“多条只含单技能的记录”;二是采取组合策略，即将“同一道题目包含的多个技能”组合为“一个新的技能”并重新编码。以两条答题记录为例( 见图 3左) : 第一行记录了某学生回答第 11 题的信息，该题与编号为 21、22 号的两项技能相关，学生回答正确; 第二行记录了该生回答第 12 题的信息，该题对应编号为21、22 和23 等三项技能，学生回答错误。\n若按拆分策略处理，研究者会得到拆分格式数据，原始样本数据第一行记录被拆分为两行，除技能编号不同外，其余信息都相同。同理，拆分原始样本数据第二行可得到三行数据( 见图 3 右上) 。若按组合策略处理，研究者会得到组合格式数据，原始样本数据第一行技能( 21，22) 被视为一种新的技能，并被编码为31。同理，第二行技能被编码为 32( 见图3 右下) 。\n对于拆分格式数据，以深度神经网络为代表的模型容易学到两项技能交替出现的模式，由此带来额外且显著的性能优势( Khajah et al.  ，2016) ，但同时也造成数据冗余，因此近年的研究逐渐转向组合方式( Nakagawa et al.  ，2018; Choffin et al.  ，2019; Xu Davenport，2020; Sonkar et al.  ，2020) 。组合格式数据虽然保证了一条记录描述一道题，但同时也造成技能间关联信息的缺失。例如，两个组合而成的新技能31 和32，虽然都包含了技能 21 和 22，但重新编码后无法体现这一关联信息。\n【3. 多热编码方法**】\n从模型输入编码角度，以上两种数据处理方法均采用基于技能编号的独热编码。为解决这两种方法存在的固有弊端，研究者提出多热编码方法( multi-hot encoding) ，即通过构建一个矩阵，保存题目和技能之间的交互关系，然后通过内积运算得到题目对应的多热编码，模型输出可采用多个技能对应预测概率的平均值。多热编码方法的优势包括: 一是既保存了题目和多个技能的原始对应关系，又可以表示不同题目与多个技能之间的关联性，有助于模型发现组合技能和其他技能的关系，更具可解释性; 二是每个时间步只需输入一道题的记录，而不是用多个时间步处理同一道题，更符合认知追踪序列化建模的内在逻辑; 三是减少数据重复出现的同时保证了信息的完整性，并能够与拆分、组合两种格式的数据相互转换。\n四、启示( 一) 场景拓展: 从自主学习到多模式混合学习\n( 二) 模态跨越: 从单一学习行为到多模态数据融合\n( 三) 模型白化: 从深度学习算法黑箱到可解释分析\n( 四) 范式转变: 从数据驱动到数据与知识联合驱动\n( 五) 价值回归: 从技术意识垄断回归教育价值本位\n","categories":["Knowledge Tracing"]},{"title":"17 Probabilistic Graphical Models and Bayesian Networks","url":"/2022/02/28/bert%20huang%20Fall%202015%20Machine%20Learning-17-Probabilistic-Graphical-Models-and-Bayesian-Networks/","content":"Probabilistic Graphical Models and Bayesian Networks\n\n\n\n","categories":["bert huang Fall 2015 Machine Learning"]},{"title":"2 Probability and Naive Bayes","url":"/2022/02/28/bert%20huang%20Fall%202015%20Machine%20Learning-2-Probability-and-Naive-Bayes/","content":"Probability and Naive Bayes\n\n","categories":["bert huang Fall 2015 Machine Learning"]},{"title":"3 Decision Tree","url":"/2022/03/22/bert%20huang%20Fall%202015%20Machine%20Learning-3-Decision-Tree/","content":"Decision Tree\n\n","categories":["bert huang Fall 2015 Machine Learning"]},{"title":"Hello World","url":"/2022/01/04/hello-world/","content":"2022，你好。\n","categories":["hello"]},{"title":"BASIC SKILLS","url":"/2022/09/19/pl-BASIC-SKILLS/","content":"摘自 BASIC SKILLSTRAIN A MODELAdd importsimport osimport torchfrom torch import nnimport torch.nn.functional as Ffrom torchvision import transformsfrom torchvision.datasets import MNISTfrom torch.utils.data import DataLoader, random_splitimport pytorch_lightning as pl\nDefine the PyTorch nn.Modules利用pytorch构建网络层，ModuleList和Sequential 详见 详解PyTorch中的ModuleList和Sequential\nclass Encoder(nn.Module):    def __init__(self):        super().__init__()        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))    def forward(self, x):        return self.l1(x)class Decoder(nn.Module):    def __init__(self):        super().__init__()        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))    def forward(self, x):        return self.l1(x)\nDefine a LightningModuleThe LightningModule is the full recipe that defines how your nn.Modules interact.\n\nThe training_step defines how the nn.Modules interact together.\nIn the configure_optimizers define the optimizer(s) for your models.\n\nclass LitAutoEncoder(pl.LightningModule):  # 构建LightningModule，去融合、交互pytorch层    def __init__(self, encoder, decoder):        super().__init__()        self.encoder = encoder        self.decoder = decoder        \t# 确定train的loss    def training_step(self, batch, batch_idx):        # training_step defines the train loop.        x, y = batch        x = x.view(x.size(0), -1)        z = self.encoder(x)        x_hat = self.decoder(z)        loss = F.mse_loss(x_hat, x)        return loss  # 优化器    def configure_optimizers(self):        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)        return optimizer\nDefine the training datasetDefine a PyTorch DataLoader which contains your training dataset.\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())train_loader = DataLoader(dataset)\n注：在dataloader设置batch_size,和后续的epoch确定每一个epoch的iteration数和每轮的数据量\nTrain the model# modelautoencoder = LitAutoEncoder(Encoder(), Decoder())# train modeltrainer = pl.Trainer()trainer.fit(model=autoencoder, train_dataloaders=train_loader)\nEliminate the training loopautoencoder = LitAutoEncoder(encoder, decoder)optimizer = autoencoder.configure_optimizers()for batch_idx, batch in enumerate(train_loader):    loss = autoencoder(batch, batch_idx)    loss.backward()    optimizer.step()    optimizer.zero_grad()\nThe power of Lightning comes when the training loop gets complicated as you add validation/test splits, schedulers, distributed training and all the latest SOTA techniques.\nWith Lightning, you can add mix all these techniques together without needing to rewrite a new loop every time.\nVALIDATE AND TEST A MODELThe test set is NOT used during training, it is ONLY used once the model has been trained to see how the model will do in the real-world.\nFind the train and test splitsimport torch.utils.data as datafrom torchvision import datasets# Load data setstrain_set = datasets.MNIST(root=&quot;MNIST&quot;, download=True, train=True)test_set = datasets.MNIST(root=&quot;MNIST&quot;, download=True, train=False)\nDefine the test loop在training_step后增加test_step即可\nclass LitAutoEncoder(pl.LightningModule):    def training_step(self, batch, batch_idx):        ...    def test_step(self, batch, batch_idx):        # this is the test loop        x, y = batch        x = x.view(x.size(0), -1)        z = self.encoder(x)        x_hat = self.decoder(z)        test_loss = F.mse_loss(x_hat, x)        self.log(&quot;test_loss&quot;, test_loss)\nTrain with the test loopfrom torch.utils.data import DataLoader# initialize the Trainertrainer = Trainer()# test the modeltrainer.test(model, dataloaders=DataLoader(test_set))\nAdd a validation loopDuring training, it’s common practice to use a small portion of the train split to determine when the model has finished training.\nSplit the training data也可以事先分好\n# use 20% of training data for validationtrain_set_size = int(len(train_set) * 0.8)valid_set_size = len(train_set) - train_set_size# split the train set into twoseed = torch.Generator().manual_seed(42)train_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\nDefine the validation loopclass LitAutoEncoder(pl.LightningModule):    def training_step(self, batch, batch_idx):        ...    def validation_step(self, batch, batch_idx):        # this is the validation loop        x, y = batch        x = x.view(x.size(0), -1)        z = self.encoder(x)        x_hat = self.decoder(z)        test_loss = F.mse_loss(x_hat, x)        self.log(&quot;val_loss&quot;, test_loss)\nTrain with the validation loopfrom torch.utils.data import DataLoadertrain_set = DataLoader(train_set)val_set = DataLoader(val_set)# train with both splitstrainer = Trainer()trainer.fit(model, train_set, val_set)\n","categories":["pytorch-lightning"]},{"title":"Lightning 工作流程的 7 个关键步骤","url":"/2022/09/19/pl-Lightning-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E7%9A%84-7-%E4%B8%AA%E5%85%B3%E9%94%AE%E6%AD%A5%E9%AA%A4/","content":"翻译自：LIGHTNING IN 15 MINUTES1: Install PyTorch Lightning# For pip userspip install pytorch-lightning# For conda usersconda install pytorch-lightning -c conda-forge\n2: Define a LightningModuleA LightningModule enables your PyTorch nn.Module to play together in complex ways inside the training_step (there is also an optional validation_step and test_step).\nimport osfrom torch import optim, nn, utils, Tensorfrom torchvision.datasets import MNISTfrom torchvision.transforms import ToTensorimport pytorch_lightning as pl# define any number of nn.Modules (or use your current ones)encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))# define the LightningModuleclass LitAutoEncoder(pl.LightningModule):    def __init__(self, encoder, decoder):        super().__init__()        self.encoder = encoder        self.decoder = decoder    def training_step(self, batch, batch_idx):        # training_step defines the train loop.        # it is independent of forward        x, y = batch        x = x.view(x.size(0), -1)        z = self.encoder(x)        x_hat = self.decoder(z)        loss = nn.functional.mse_loss(x_hat, x)        # Logging to TensorBoard by default        self.log(&quot;train_loss&quot;, loss)        return loss    def configure_optimizers(self):        optimizer = optim.Adam(self.parameters(), lr=1e-3)        return optimizer# init the autoencoderautoencoder = LitAutoEncoder(encoder, decoder)\n3: Define a datasetLightning supports ANY iterable (DataLoader, numpy, etc…) for the train/val/test/predict splits.\n# setup datadataset = MNIST(os.getcwd(), download=True, transform=ToTensor())train_loader = utils.data.DataLoader(dataset)\n4: Train the modelThe Lightning Trainer “mixes” any LightningModule with any dataset and abstracts away all the engineering complexity needed for scale.\n# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)trainer = pl.Trainer(limit_train_batches=100, max_epochs=1)trainer.fit(model=autoencoder, train_dataloaders=train_loader)\nThe Lightning Trainer automates 40+ tricks including:\n\nEpoch and batch iteration. (设置epoch和batchsize\noptimizer.step(), loss.backward(), optimizer.zero_grad() calls\nCalling of model.eval(), enabling/disabling grads during evaluation\nCheckpoint Saving and Loading\nTensorboard (see loggers options)\nMulti-GPU support\nTPU\n16-bit precision AMP support\n\n5: Use the model ？Once you’ve trained the model you can export to onnx, torchscript and put it into production or simply load the weights and run predictions.\n# load checkpointcheckpoint = &quot;./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt&quot;autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)# choose your trained nn.Moduleencoder = autoencoder.encoderencoder.eval()# embed 4 fake images!fake_image_batch = Tensor(4, 28 * 28)embeddings = encoder(fake_image_batch)print(&quot;⚡&quot; * 20, &quot;\\nPredictions (4 image embeddings):\\n&quot;, embeddings, &quot;\\n&quot;, &quot;⚡&quot; * 20)\n6: Visualize trainingtensorboard --logdir .\n7: Supercharge trainingEnable advanced training features using Trainer arguments. These are state-of-the-art techniques that are automatically integrated into your training loop without changes to your code.\n# train on 4 GPUstrainer = Trainer(    devices=4,    accelerator=&quot;gpu&quot;, )# train 1TB+ parameter models with Deepspeed/fsdptrainer = Trainer(    devices=4,    accelerator=&quot;gpu&quot;,    strategy=&quot;deepspeed_stage_2&quot;,    precision=16 )# 20+ helpful flags for rapid idea iterationtrainer = Trainer(    max_epochs=10,    min_epochs=5,    overfit_batches=1 )# access the latest state of the art techniquestrainer = Trainer(callbacks=[StochasticWeightAveraging(...)])\n","categories":["pytorch-lightning"]},{"title":"earlystopping","url":"/2022/09/25/pl-earlystopping/","content":"earlystop相关详见：早停法\ndefinitionCLASS pytorch_lightning.callbacks.EarlyStopping(monitor, min_delta=0.0, patience=3, verbose=False, mode=&#x27;min&#x27;, strict=True, check_finite=True, stopping_threshold=None, divergence_threshold=None, check_on_train_epoch_end=None, log_rank_zero_only=False)\nBases: pytorch_lightning.callbacks.callback.Callback\nMonitor a metric and stop training when it stops improving.（监控指标并在停止改进时停止训练。）\nParameters\nmonitor (str) – 监控指标。quantity to be monitored.\n\nmin_delta (float) – 最小变化量。minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than or equal to min_delta, will count as no improvement.\n\npatience (int) – 没有改善的检查次数，默认一个epoch检查一次，即一个patience等于一个epoch。number of checks with no improvement after which training will be stopped. Under the default configuration, one check happens after every training epoch. However, the frequency of validation can be modified by setting various parameters on the Trainer, for example check_val_every_n_epoch and val_check_interval.\n\nNOTE\nIt must be noted that the patience parameter counts the number of validation checks with no improvement, and not the number of training epochs. Therefore, with parameters check_val_every_n_epoch=10 and patience=3, the trainer will perform at least 40 training epochs before being stopped.\n\n\nverbose (bool) – 详细输出。verbosity mode.\n\nmode (str) – 停止变化的方向。one of &#39;min&#39;, &#39;max&#39;. In &#39;min&#39; mode, training will stop when the quantity monitored has stopped decreasing and in &#39;max&#39; mode it will stop when the quantity monitored has stopped increasing.\nstrict (bool) – whether to crash the training if monitor is not found in the validation metrics.\ncheck_finite (bool) – When set True, stops training when the monitor becomes NaN or infinite.\nstopping_threshold (Optional[float]) – Stop training immediately once the monitored quantity reaches this threshold.\ndivergence_threshold (Optional[float]) – Stop training as soon as the monitored quantity becomes worse than this threshold.\ncheck_on_train_epoch_end (Optional[bool]) – whether to run early stopping at the end of the training epoch. If this is False, then the check runs at the end of the validation.\nlog_rank_zero_only (bool) – When set True, logs the status of the early stopping callback only for rank 0 process.\n\nRaises\nMisconfigurationException – If mode is none of &quot;min&quot; or &quot;max&quot;.\nRuntimeError – If the metric monitor is not available.\n\nExamples&gt;&gt;&gt; from pytorch_lightning import Trainer&gt;&gt;&gt; from pytorch_lightning.callbacks import EarlyStopping&gt;&gt;&gt; early_stopping = EarlyStopping(&#x27;val_loss&#x27;)&gt;&gt;&gt; trainer = Trainer(callbacks=[early_stopping])\n# monitor：根据什么指标earlystop# mode、mindelta、patience：停止条件，monitor在patience轮验证集验证后仍然没有朝mode方向变化mindelta就earlystop# 如果一个epoch会检验10轮验证集，那么20个epoch不变好则需要patience=200.（一般是20个epoch不变好就可以earlystopcheckpoint_callback = [EarlyStopping(monitor=&quot;val_ACC&quot;, mode=&quot;max&quot;,min_delta=0.00, patience=20)]trainer = pl.Trainer(max_epochs=train_params.iterations, logger=tb_logger, callbacks = checkpoint_callback,val_check_interval=0.1)\n\nTIPSaving and restoring multiple early stopping callbacks at the same time is supported under variation in the following arguments:\nmonitor, mode\nRead more: Persisting Callback State\n\npracticeStopping an Epoch EarlyYou can stop and skip the rest of the current epoch early by overriding on_train_batch_start() to return -1 when some condition is met.\nIf you do this repeatedly, for every epoch you had originally requested, then this will stop your entire training.\nEarlyStopping CallbackThe EarlyStopping callback can be used to monitor a metric and stop the training when no improvement is observed.\nTo enable it:\n\nImport EarlyStopping callback.\nLog the metric you want to monitor using log() method.\nInit the callback, and set monitor to the logged metric of your choice.\nSet the mode based on the metric needs to be monitored.\nPass the EarlyStopping callback to the Trainer callbacks flag.\n\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStoppingclass LitModel(LightningModule):    def validation_step(self, batch, batch_idx):        loss = ...        self.log(&quot;val_loss&quot;, loss)model = LitModel()trainer = Trainer(callbacks=[EarlyStopping(monitor=&quot;val_loss&quot;, mode=&quot;min&quot;)])trainer.fit(model)\nYou can customize the callbacks behaviour by changing its parameters.\nearly_stop_callback = EarlyStopping(monitor=&quot;val_accuracy&quot;, min_delta=0.00, patience=3, verbose=False, mode=&quot;max&quot;)trainer = Trainer(callbacks=[early_stop_callback])\nAdditional parameters that stop training at extreme points:\n\nstopping_threshold: Stops training immediately once the monitored quantity reaches this threshold. It is useful when we know that going beyond a certain optimal value does not further benefit us.\ndivergence_threshold: Stops training as soon as the monitored quantity becomes worse than this threshold. When reaching a value this bad, we believes the model cannot recover anymore and it is better to stop early and run with different initial conditions.\ncheck_finite: When turned on, it stops training if the monitored metric becomes NaN or infinite.\ncheck_on_train_epoch_end: When turned on, it checks the metric at the end of a training epoch. Use this only when you are monitoring any metric logged within training-specific hooks on epoch-level.\n\nIn case you need early stopping in a different part of training, subclass EarlyStopping and change where it is called:\nclass MyEarlyStopping(EarlyStopping):    def on_validation_end(self, trainer, pl_module):        # override this to disable early stopping at the end of val loop        pass    def on_train_end(self, trainer, pl_module):        # instead, do it at the end of training loop        self._run_early_stopping_check(trainer)\n\nNOTE\nThe EarlyStopping callback runs at the end of every validation epoch by default. However, the frequency of validation can be modified by setting various parameters in the Trainer, for example check_val_every_n_epoch and val_check_interval. It must be noted that the patience parameter counts the number of validation checks with no improvement, and not the number of training epochs. Therefore, with parameters check_val_every_n_epoch=10 and patience=3, the trainer will perform at least 40 training epochs before being stopped.\n\n\nhttps://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.EarlyStopping.html\n\nhttps://pytorch-lightning.readthedocs.io/en/stable/common/early_stopping.html?highlight=earlystop\n\n\n","categories":["pytorch-lightning"]},{"title":"Mac","url":"/2022/03/30/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-Mac/","content":"\n恢复失灵的Type-c接口\n关机\n按下键盘右侧的 Shift键，左侧的Control和Option键不放。此时电脑会开机，等进入显示白条的状态，不要松开手，同时按下电源按钮 10 秒钟。此时电脑将关闭。\n松开所有按键\n等待10秒钟，重新启动电脑\n\n\n\n","categories":["tools解决方案"]},{"title":"Tensorfow","url":"/2022/05/26/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-Tensorflow/","content":"给指定某一层权重赋值# 给第一隐藏层赋值为例optimizer2 = tf.keras.optimizers.SGD(lr=1.0)layer1 = model.layers[0]# theta1是网络原本带的权重 theta2 = [1,1,1,2,85,54,41] # 自己定义的权重optimizer2.apply_gradients(zip([theta1-theta2], layer1.trainable_variables))\ntf.Variable是只读类型，但是可以通过使用assign方法可以直接赋值、加、减。\nv = tf.Variable(1.)v.assign(2.)v.assign_sub(0.5)v.assign_add(0.5)\nTensorFlow: Unimplemented: Cast string to float is not supported由于训练网络涉及计算损失值（这是一个数字值），然后反向传播它以更新权重（也是数字值），因此应该有数字输出和标签。将标签数值化。\nlabel = tf.constant(1.) if tf.math.equal(string_label, tf.constant(&#x27;good&#x27;, dtype=tf.string)) else tf.constant(0.)\nTensorflow2.0对不同层指定学习率在使用Tensorflow编写深度学习模型的时候往往会考虑对不同的层采用不一样的优化器以及学习率，以下为其中一个案例\nimport tensorflow as tffrom zh.model.mnist.mlp import MLPfrom zh.model.utils import MNISTLoadernum_epochs = 5batch_size = 50learning_rate_1 = 0.001learning_rate_2 = 0.01model = MLP()data_loader = MNISTLoader()# 声明两个优化器，设定不同的学习率，分别用于更新MLP模型的第一层和第二层optimizer_1 = tf.keras.optimizers.Adam(learning_rate=learning_rate_1)optimizer_2 = tf.keras.optimizers.Adam(learning_rate=learning_rate_2)num_batches = int(data_loader.num_train_data // batch_size * num_epochs)for batch_index in range(num_batches):    X, y = data_loader.get_batch(batch_size)    with tf.GradientTape(persistent=True) as tape:  # 声明一个持久的GradientTape，允许我们多次调用tape.gradient方法        y_pred = model(X)        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)        loss = tf.reduce_mean(loss)        print(&quot;batch %d: loss %f&quot; % (batch_index, loss.numpy()))    grads = tape.gradient(loss, model.dense1.variables)    # 单独求第一层参数的梯度    optimizer_1.apply_gradients(grads_and_vars=zip(grads, model.dense1.variables)) # 单独对第一层参数更新，学习率0.001    grads = tape.gradient(loss, model.dense2.variables)    # 单独求第二层参数的梯度    optimizer_1.apply_gradients(grads_and_vars=zip(grads, model.dense2.variables)) # 单独对第二层参数更新，学习率0.01\nTensorflow2 加载别人模型权重失败时的一种解决方法：每一层分别设置值自定义权重初始化tensorflow tf.layers.dense\n创建自己的图层\n\nW1 = tf.Variable(YOUR_WEIGHT_MATRIX, name=&#x27;Weights&#x27;)b1 = tf.Variable(tf.zeros([YOUR_LAYER_SIZE]), name=&#x27;Biases&#x27;) #or pass your ownh1 = tf.add(tf.matmul(X, W1), b1)\n\n使用 tf.constant_initializer\n\ninit = tf.constant_initializer(YOUR_WEIGHT_MATRIX)l1 = tf.layers.dense(X, o, kernel_initializer=init)\n\n使用random中的 initializer\n\ntf.keras.layers.dense(i, kernel_initializer=tf.random_uniform_initializer(0, 0.1),                    activation=&#x27;relu&#x27;, input_shape=(n_features,)))\nTensor与Numpy的ndarray转换Tensorflow - ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float)most fixed with x = np.asarray(x).astype(&#39;float32&#39;).\nimport numpy as npX = np.asarray(X).astype(np.float32)\n问题的根源在于使用列表作为输入，而不是 Numpy 数组；Keras/TF 不支持前者。一个简单的转换是：x_array = np.asarray(x_list).\ntensor转化为ndarray&#x27;&#x27;&#x27;利用session.run&#x27;&#x27;&#x27;import tensorflow as tf# 创建张量t = tf.constant([1, 2, 3, 4], tf.float32)# 创建会话session = tf.Session()# 张量转化为ndarrayarray = session.run(t)# 打印其数据类型与其值print(type(array))print(array)&lt;class &#x27;numpy.ndarray&#x27;&gt;[ 1.  2.  3.  4.]&#x27;&#x27;&#x27;利用eval&#x27;&#x27;&#x27;import tensorflow as tf# 创建张量t = tf.constant([1, 2, 3, 4], tf.float32)# 创建会话session = tf.Session()# 张量转化为ndarrayarray = t.eval(session=session)# 打印其数据类型与其值print(type(array))print(array)&lt;class &#x27;numpy.ndarray&#x27;&gt;[ 1.  2.  3.  4.]\nndarray转化为tensor&#x27;&#x27;&#x27;利用convert_to_tensor函数&#x27;&#x27;&#x27;import tensorflow as tfimport numpy as np# 创建ndarrayarray = np.array([1, 2, 3, 4], np.float32)# 将ndarray转化为tensort = tf.convert_to_tensor(array, tf.float32, name=&#x27;t&#x27;)# 打印输出print(t)Tensor(&quot;t:0&quot;, shape=(4,), dtype=float32)\nModule: tf.keras.metrics 所有有关矩阵的类和方法。\n有关tensorflow2.0的文章\nTensorflow 2.0 实现神经网络\nTensorFlow 2.0中的多标签图像分类\nTensorFlow2.0（11）：tf.keras建模三部曲 _—— 全版\n\n","categories":["tools解决方案"]},{"title":"jupyter","url":"/2022/03/22/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-jupyter/","content":"\nJupyter notebook 运行时出现 “服务似乎挂掉了，但是会立刻重启的”：\n\nimport osos.environ[&#x27;KMP_DUPLICATE_LIB_OK&#x27;] = &#x27;TRUE&#x27;\n\nJupyterNotebook 输出窗口的显示效果调整\n\n\n\nJupyter Notebook中的漂亮打印\n\nimport pprintpp = pprint.PrettyPrinter(indent=2)pp.pprint(myTree)\n\n\nJupyter Notebook显示行号菜单View -&gt; Toggle Line Numbers\n自动补全&amp;代码格式化\n安装 nbextensions安装：pip install jupyter_contrib_nbextensions -i https://pypi.mirrors.ustc.edu.cn/simple激活：jupyter contrib nbextension install —user\n\n安装 nbextensions_configurator安装：pip install jupyter_nbextensions_configurator激活：jupyter nbextensions_configurator enable —user\n\n启动 jupyter notebook 点击新增加的Nbextensions项\n\n勾选Hinterland（代码提示/自动补全）和Autopep8（代码格式化）\n\n","categories":["tools解决方案"]},{"title":"keras","url":"/2022/09/15/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-keras/","content":"Keras model.fit()fit( x, y, batch_size=32, epochs=10, verbose=1, callbacks=None,validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n\nx：输入数据。如果模型只有一个输入，那么x的类型是numpyarray，如果模型有多个输入，那么x的类型应当为list，list的元素是对应于各个输入的numpy array\ny：标签，numpy array，注意和x的适配\nbatch_size：整数，指定进行梯度下降时每个batch包含的样本数。训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步。\nepochs：整数，训练终止时的epoch值，训练将在达到该epoch值时停止，当没有设置initial_epoch时，它就是训练的总轮数，否则训练的总轮数为epochs - inital_epoch\nverbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\ncallbacks：list，其中的元素是keras.callbacks.Callback的对象。这个list中的回调函数将会在训练过程中的适当时机被调用，参考回调函数\nvalidation_split：0~1之间的浮点数，用来指定训练集的一定比例数据作为验证集。验证集将不参与训练，并在每个epoch结束后测试的模型的指标，如损失函数、精确度等。注意，validation_split的划分在shuffle之前，因此如果你的数据本身是有序的，需要先手工打乱再指定validation_split，否则可能会出现验证集样本不均匀。\nvalidation_data：形式为（X，y）的tuple，是指定的验证集。此参数将覆盖validation_spilt。\nshuffle：布尔值或字符串，一般为布尔值，表示是否在训练过程中随机打乱输入样本的顺序。若为字符串“batch”，则是用来处理HDF5数据的特殊情况，它将在batch内部将数据打乱。\nclass_weight：字典，将不同的类别映射为不同的权值，该参数用来在训练过程中调整损失函数（只能用于训练）\nsample_weight：权值的numpyarray，用于在训练时调整损失函数（仅用于训练）。可以传递一个1D的与样本等长的向量用于对样本进行1对1的加权，或者在面对时序数据时，传递一个的形式为（samples，sequence_length）的矩阵来为每个时间步上的样本赋不同的权。这种情况下请确定在编译模型时添加了sample_weight_mode=’temporal’。\ninitial_epoch: 从该参数指定的epoch开始训练，在继续之前的训练时有用。\n\n\nkeras的model.fit()\n\nKeras model.fit()参数详解 : demo\n\n\n","categories":["tools解决方案"]},{"title":"latex及markdown","url":"/2022/01/09/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-latex%E5%8F%8Amarkdown/","content":"\n关于Hexo添加Mathjax相关问题及解决方法：https://d-veda.top/2018/11/11/Hexo-with-Mathjax/\n\nmarkdown中希腊字母加粗：https://blog.csdn.net/qq_28714865/article/details/103717900\n让公式中的某些数学符号加粗显示，可以使用”\\mathbf{数学符号}​”，比如 $\\mathbf{0123456789}$ .加粗希腊字母（比如书写向量或者矩阵时），使用”\\pmb{希腊字母}”，比如 $\\pmb{\\lambda}$ .\n\nLaTeX\n\n输入单个点：\\cdot\n横向多个点：\\cdots\n竖向多个点：\\vdots\n斜向多个点：\\ddots\n上^号 ：\\hat  或 \\widehat\n上横线： \\overline\n上波浪线： \\widetilde\n波浪线：\\sim\n上点 \\dot{}\n上两个点\\ddot{}\n\n\n使用Markdown写矩阵：https://blog.csdn.net/qq_38228254/article/details/79469727\n\n\n\n\n\n\nlatex\n显示效果\n\n\n\n\n\\uparrow\n↑\n\n\n\\downarrow\n↓\n\n\n\\Uparrow\n⇑\n\n\n\\Downarrow\n⇓\n\n\n\\updownarrow\n↕\n\n\n\\Updownarrow\n⇕\n\n\n\\rightarrow\n→\n\n\n\\leftarrow\n←\n\n\n\\Rightarrow\n⇒\n\n\n\\Leftarrow\n⇐\n\n\n\\leftrightarrow\n↔\n\n\n\\Leftrightarrow\n⇔\n\n\n\\longrightarrow\n⟶\n\n\n\\longleftarrow\n⟵\n\n\n\\Longrightarrow\n⟹\n\n\n\\Longleftarrow\n⟸\n\n\n\\longleftrightarrow\n⟷\n\n\n\\Longleftrightarrow\n⟺\n\n\n\\rightrightarrows\n⇉\n\n\n\\rightleftarrows\n⇄\n\n\n\\rightrightarrows\n⇉\n\n\n\\rightleftarrows\n⇄\n\n\n\\rightleftharpoons\n⇌\n\n\n\n\nusepackage{amsmath,amssymb}集合的大括号：          { …  }\\集合中的“|”：        \\mid属于：          \\in不属于：          \\not\\inA包含于B：        A\\subset BA真包含于B：        A\\subsetneqq BA包含B：          A\\supset BA真包含B：        A\\supsetneqq BA不包含于B：        A\\not\\subset BA交B：          A\\cap BA并B：          A\\cup BA的闭包：          \\overline{A}A减去B:          A\\setminus B实数集合：        \\mathbb{R}空集：          \\emptyset\n","categories":["tools解决方案"]},{"title":"matplotlib","url":"/2022/05/25/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-matplotlib/","content":"图例matplotlib可以为可见对象（Artist，比如线条、形状）添加图例。官方建议使用pyplot模块的legend函数简便的创建图例，而不是使用底层的matplotlib.legend类构造图例。\n使用图例的基础有两个：\n\nhandles：可见对象（Artist，比如线条、形状）序列，与labels配合使用。\nlabels：类型为字符串列表，即图例中显示的文本集合。\n\n两者的长度最好一致，否则按照长度最小的进行截断。\n调用方式有三种：\n\nlegend()：根据可见对象和标签自动构造图例。推荐。\nlegend(labels)：自动匹配可见对象和labels，官方不建议使用这种方式，因为可见对象和标签的对应关系并不明确！\nlegend(handles, labels)：指定可见对象和标签的对应关系。\n\ncodingimport matplotlib.pyplot as pltplt.figure(figsize=(13,4))plt.subplot(131)# legend()调用方式，****建议plt.plot([1, 1],label=&#x27;1&#x27;)plt.plot([2, 2],label=&#x27;2&#x27;)plt.legend()plt.subplot(132)# legend(labels)调用方式plt.plot([1, 1])plt.plot([2, 2])plt.legend([&#x27;1&#x27;,&#x27;2&#x27;])plt.subplot(133)# legend(handles, labels)调用方式# 注意plot函数返回的为Line2D对象列表line1, = plt.plot([1, 1])line2, = plt.plot([2, 2])print(type(line1))plt.legend((line1,line2),[&#x27;1st&#x27;,&#x27;2nd&#x27;])plt.show()\n参数\nloc：图例显示的位置，类型为浮点数或字符串，默认值为rcParams[&quot;legend.loc&quot;] ( &#39;best&#39;)。浮点数和字符串之间具有以下对应关系：\n\n\n\n\n\n位置字符串\n位置编码\n\n\n\n\n‘best’\n0\n\n\n‘upper right’\n1\n\n\n‘upper left’\n2\n\n\n‘lower left’\n3\n\n\n‘lower right’\n4\n\n\n‘right’\n5\n\n\n‘center left’\n6\n\n\n‘center right’\n7\n\n\n‘lower center’\n8\n\n\n‘upper center’\n9\n\n\n‘center’\n10\n\n\n\n\n\nfontsize：标签字体大小。整数或&#123;&#39;xx-small&#39;, &#39;x-small&#39;, &#39;small&#39;, &#39;medium&#39;, &#39;large&#39;, &#39;x-large&#39;, &#39;xx-large&#39;&#125;。只有设置了prop参数，fontsize属性才生效。\n\nlabelcolor：标签颜色。字符串或字符串列表，可以为颜色格式或&#39;linecolor&#39;， &#39;markerfacecolor&#39; (&#39;mfc&#39;)，&#39;markeredgecolor&#39; ( &#39;mec&#39;)。\n\nshadow：图例区域是否有阴影。类型为布尔值。默认值为：rcParams[&quot;legend.shadow&quot;] ( False)。\nfacecolor：图例区域背景色。&quot;inherit&quot;或 色彩值，默认值为rcParams[&quot;legend.facecolor&quot;] (&#39;inherit&#39;)。如果使用&quot;inherit&quot;，使用rcParams[&quot;axes.facecolor&quot;] ( &#39;white&#39;)。\nlabelspacing：图例条目之间的垂直距离。浮点值，单位为字体大小单位。默认值为rcParams[&quot;legend.labelspacing&quot;] ( 0.5)。\nhandlelength：图例标记的长度。浮点值，单位为字体大小单位。默认值为rcParams[&quot;legend.handlelength&quot;] (2.0)。\n\n更多见：matplotlib之pyplot模块之图例（legend）基础（legend()的调用方式，图例外观设置）\n","categories":["tools解决方案"]},{"title":"numpy and pandas","url":"/2022/03/29/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-numpy-and-pandas/","content":"数据类型及转换Pandas数据类型\nfloat\nint\nbool\ndatetime64[ns]\ndatetime64[ns, tz]\ntimedelta[ns]\ncategory\nobject默认的数据类型是int64,float64.\n\n查看数据类型\ndf.dtypes\nseries.dtype\nget_dtype_counts()如果一列中含有多个类型,则该列的类型会是object,同样字符串类型的列也会被当成object类型.不同的数据类型也会被当成object,比如int32,float32\n\narray &lt;—&gt; listimport numpyls = [1,2,3,4,5,6]arr = numpy.array(ls)arr = numpy.array([1,2,3,4,5,6])ls = arr.tolist()\ndataframe/series —&gt; listdf = pd.DataFrame(&#123;&#x27;a&#x27;:[1,3,5,7,4,5,6,4,7,8,9], &#x27;b&#x27;:[3,5,6,2,4,6,7,8,7,8,9]&#125;) # 方法1df[&#x27;a&#x27;].values.tolist() # 方法2df[&#x27;a&#x27;].tolist()#把a列的元素转换成list：df[&#x27;a&#x27;].drop_duplicates().values.tolist()\nDf—&gt;numpy\n使用DataFrame中的values方法\n\ndf.values\n\n使用DataFrame中的as_matrix()方法\n\ndf.as_matrix()\n\n使用Numpy中的array方法\n\nnp.array(df)\nnumpy array type —&gt; intarray.astype(int)\n稀疏矩阵—&gt;普通矩阵df.todense()  或者  df.to_dense()\n\nnumpy数组中值的替换array[条件（包括索引，表达式，逻辑运算、组合运算）] = 替换的值\npd读取文件read_csv()常用参数：1.filepath_or_buffer:（这是唯一一个必须有的参数，其它都是按需求选用的）文件所在处的路径\n2.sep：指定分隔符，默认为逗号’,’\n3.delimiter : str, default None定界符，备选分隔符（如果指定该参数，则sep参数失效）\n4.header：int or list of ints, default ‘infer’指定哪一行作为表头。默认设置为0（即第一行作为表头），如果没有表头的话，要修改参数，设置header=None\n5.names：指定列的名称，用列表表示。一般我们没有表头，即header=None时，这个用来添加列名就很有用啦！\n6.index_col:指定哪一列数据作为行索引，可以是一列，也可以多列。多列的话，会看到一个分层索引\n7.prefix:给列名添加前缀。如prefix=”x”,会出来”x1”、”x2”、”x3”酱纸\n8.nrows : int, default None需要读取的行数（从文件头开始算起）\n9.encoding:乱码的时候用这个就是了，官网文档看看用哪个：https://docs.python.org/3/library/codecs.html#standard-encodings\n10.skiprows : list-like or integer, default None需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始）。\ndata = pd.read_csv(r&quot;train.csv&quot;)#002使用pd.read_csv(&#x27;text.csv&#x27;) 则会2，前面的0不见了。# code列中的数据读取为strdf＝pd.read_csv(&#x27;text.csv&#x27;, dtype=&#123;&#x27;code&#x27;:str&#125;)df＝pd.read_csv(&#x27;text.csv&#x27;, dtype=str)\nto_csv常用参数：1.path_or_buf：字符串，放文件名、相对路径、文件流等；\n2.sep：字符串，分隔符，跟read_csv()的一个意思\n3.na_rep：字符串，将NaN转换为特定值\n4.columns：列表，指定哪些列写进去\n5.header：默认header=0，如果没有表头，设置header=None，表示我没有表头呀！\n6.index：关于索引的，默认True,写入索引\n# import os# father_path = os.getcwd()# father_pathdf.to_csv(&quot;train.csv&quot;)\npandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3, saw 12# 删除该行data = pd.read_csv(&#x27;file1.csv&#x27;, on_bad_lines=&#x27;skip&#x27;)# 指定最多列col_names = [&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;, ...]#col_names = [i for i in range(maxcol)]df = pd.read_csv(filename, names=col_names)\npython怎么写多列长度不等的csv文件\n生成的数据集，csv文件列数不同，无法正常读取的解决方法\n杂按行遍历Dataframeiterrows(): 按行遍历，将DataFrame的每一行迭代为(index, Series)对，可以通过row[name]对元素进行访问。\nfor index, row in df.iterrows():    print(index, row) \nsortsort()已弃用 DataFrames 支持：\n\nsort_values(by=[&#39;A&#39;])  按列排序\n\n```pythonDataFrame.sort_values(by=’##’,axis=0,ascending=True, inplace=False, na_position=’last’)参数说明by：指定列名(axis=0或’index’，对行进行排序)或索引值(axis=1或’columns’，对列进行排序)axis：axis=0或’index’，则按照指定列中数据大小排序；若axis=1或’columns’，则按照指定索引中数据大小排序，默认axis=0ascending：是否按指定列的数组升序排列，默认为True，即升序排列inplace：是否用排序后的数据集替换原来的数据，默认为False，即不替换na_position：first，last，设定缺失值的显示位置```pythondf.sort_values(by=[&#x27;1&#x27;,&#x27;2&#x27;],axis=0,ascending=[1,1], inplace=False, na_position=&#x27;last&#x27;)\n\n\n\nsort_index()  按索引排序\n\n\nDf 查找df.loc[index, column_name],选取指定行和列的数据df.loc[0,&#x27;id&#x27;] # &#x27;Snow&#x27;df.loc[0:2, [&#x27;id&#x27;,&#x27;title&#x27;]]  # 选取第0行到第2行，id和title列的数据, 注意这里的行选取是包含下标的。df.loc[[2,3],[&#x27;id&#x27;,&#x27;title&#x27;]]  # 选取指定的第2行和第3行，id和title列的数据df.loc[df[&#x27;webname&#x27;]==&#x27;中国货币网&#x27;,&#x27;id&#x27;]  # 选取webname列是中国货币网，id列的数据df.loc[df[&#x27;webname&#x27;]==&#x27;中国货币网&#x27;,[&#x27;id&#x27;,&#x27;title&#x27;]]  # 选取webname列是中国货币网，id和title列的数据\ndf是否为空 : df.empty (bool)打印整个 Pandas Series/DataFrameimport pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(5, 10))pd.set_option(&#x27;display.max_rows&#x27;, None)pd.set_option(&#x27;display.max_columns&#x27;, None)pd.set_option(&#x27;display.width&#x27;, None)print(df)\npython 计算df中每一列/行的非缺失值数量df.count(axis=0) # 统计每列的缺失值df.count(axis=1) # 统计每行的缺失值# 缺失值个数df.shape[0] - df.count()\npandas中关于DataFrame行，列显示不完全# 显示所有列pd.set_option(&#x27;display.max_columns&#x27;, None)# 显示所有行pd.set_option(&#x27;display.max_rows&#x27;, None)# 设置value的显示长度为100，默认为50pd.set_option(&#x27;max_colwidth&#x27;,100)\n拼接多列的字符串的方法df[&#x27;f4&#x27;]= [&#x27;&#x27;.join(i) for i in df.values]\ndataframe类型数据的复制\n浅复制  df=df1或者df=df1.copy(deep=False) ，指向同一地址，即一个变量改变，另一个变量也会改变。\n深复制  df = df1.copy(deep=True) ，仅拷贝值。\n\n查看矩阵对角线上的元素# 左上———右下np.diagonal(array)\n转置、乘积、通用函数\n数组转置和轴对换：数组不仅有transpose方法，还有一个特殊的T属性：\n\narr = np.arange(15).reshape(3,5)arrarray([[ 0,  1,  2,  3,  4],       [ 5,  6,  7,  8,  9],       [10, 11, 12, 13, 14]])arr.Tarray([[ 0,  5, 10],       [ 1,  6, 11],       [ 2,  7, 12],       [ 3,  8, 13],       [ 4,  9, 14]])\n\n进行矩阵计算时，经常需要用到该操作，比如利用 np.dot计算矩阵内积XTX：\n\narr = np.random.randn(6,3)arrarray([[-0.83790345, -1.13304154, -0.42567014],       [ 0.75742538,  1.24634357, -1.00116761],       [ 0.54168995, -0.83717253, -1.11580943],       [-0.13315165,  0.0331654 ,  0.70605975],       [-2.57536154, -0.68951735,  1.16959181],       [-1.26193272, -1.24703158,  0.3183666 ]])np.dot(arr.T,arr)array([[ 9.81189403,  4.78491411, -4.51395404],       [ 4.78491411,  5.56963513, -1.01142215],       [-4.51395404, -1.01142215,  4.39638499]])\n对于高维数组， transpose需要得到一个由轴编号组成的元组才能对这些轴进行转至（比较难理解）：arr = np.ar\nange(16).reshape((2,2,4))arrarray([[[ 0,  1,  2,  3],      [ 4,  5,  6,  7]],           [[ 8,  9, 10, 11],      [12, 13, 14, 15]]])arr.transpose((1,0,2))array([[[ 0,  1,  2,  3],        [ 8,  9, 10, 11]],       [[ 4,  5,  6,  7],        [12, 13, 14, 15]]])\n提示：transpose(1,0,2)把原来的shape由(2,2,4)变成了(2,2,4)，就是第一个轴和第二个轴上面的元素互换。\n比如原来位置（0,1,0）上的元素为4，现在把它放到了（1,0,0）这个位置，就是下面那个位置由8变成了4，标出了红色。\n4.ndarray还有一个 swapaxes方法，它接受一对轴变换：\narrarray([[[ 0,  1,  2,  3],        [ 4,  5,  6,  7]],       [[ 8,  9, 10, 11],        [12, 13, 14, 15]]])arr.swapaxes(1,2)array([[[ 0,  4],        [ 1,  5],        [ 2,  6],        [ 3,  7]],       [[ 8, 12],        [ 9, 13],        [10, 14],        [11, 15]]])\n5.通用函数sqrt、exp、maximum\narr = np.arange(10)arrarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])np.sqrt(arr)array([ 0.        ,  1.        ,  1.41421356,  1.73205081,  2.        ,        2.23606798,  2.44948974,  2.64575131,  2.82842712,  3.        ])np.exp(arr)array([  1.00000000e+00,   2.71828183e+00,   7.38905610e+00,         2.00855369e+01,   5.45981500e+01,   1.48413159e+02,         4.03428793e+02,   1.09663316e+03,   2.98095799e+03,         8.10308393e+03])x = np.random.randn(8)xarray([-0.24726724,  0.69709717,  0.9658356 ,  1.89019088, -0.28912795,       -0.09235779,  0.37690775,  0.9102138 ])y = np.random.randn(8)yarray([-0.05048326, -0.02207697, -0.59940773, -1.32029941,  0.30894105,       -0.05807405, -1.5019804 ,  0.12918562])np.maximum(x,y) #元素级最大值array([-0.05048326,  0.69709717,  0.9658356 ,  1.89019088,  0.30894105,       -0.05807405,  0.37690775,  0.9102138 ])\n\nmodf函数可以把数组分别提取出 整数部分和小数部分\n\narr = np.random.randn(7)*5arrarray([ -1.53462646,   6.15168006,   4.32588912,  -0.05408803,     -2.98953481, -10.83013834,   1.13673478])     np.modf(arr)(array([-0.53462646,  0.15168006,  0.32588912, -0.05408803, -0.98953481,     -0.83013834,  0.13673478]), array([ -1.,   6.,   4.,  -0.,  -2., -10.,   1.]))\n数组升维import numpy as np                  #导入numpy模块，并重命名为npx = np.array([1,2,3,4,5,6,7,8])     #x是一维数组 d = x.reshape((2,4))                #将x重塑为2行4列的二维数组print(d)d = x.reshape((2,2,2))              #将x重塑为2行2列2元素的三维数组print(d)\n","categories":["tools解决方案"]},{"title":"pytorch","url":"/2022/09/15/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-pytorch/","content":"with torch.no_grad() 详解torch.no_grad() 是一个上下文管理器，被该语句 wrap 起来的部分将不会track 梯度。\na = torch.tensor([1.1], requires_grad=True)b = a * 2\n打印b可看到其 grad_fn 为 mulbackward 表示是做的乘法。\nbOut[63]: tensor([2.2000], grad_fn=&lt;MulBackward0&gt;)b.add_(2)Out[64]: tensor([4.2000], grad_fn=&lt;AddBackward0&gt;)\n可以看到不被wrap的情况下，b.grad_fn 为 addbackward 表示这个add 操作被track了\nwith torch.no_grad():    b.mul_(2)#在被包裹的情况下可以看到 b.grad_fn 还是为 add，mul 操作没有被 track. 但是注意，乘法操作是被执行了的。(4.2 -&gt; 8.4)bOut[66]: tensor([8.4000], grad_fn=&lt;AddBackward0&gt;)\n所以如果有不想被track的计算部分可以通过这么一个上下文管理器包裹起来。这样可以执行计算，但该计算不会在反向传播中被记录。\n同时 torch.no_grad() 还可以作为一个装饰器。比如在网络测试的函数前加上\n@torch.no_grad()def eval():\t...\n扩展：同样还可以用 torch.set_grad_enabled()来实现不计算梯度。例如：\ndef eval():\ttorch.set_grad_enabled(False)\t...\t# your test code\ttorch.set_grad_enabled(True)\n详解PyTorch中的ModuleList和SequentialPyTorch的nn.Linear（）详解PyTorch的nn.Linear（）是用于设置网络中的全连接层的，需要注意在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]，不同于卷积层要求输入输出是四维张量。  具体：\nCLASS torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n\nApplies a linear transformation to the incoming data: $y = xA^T + b$\nThis module supports TensorFloat32.\nOn certain ROCm devices, when using float16 inputs this module will use different precision for backward.\n\nParameters\nin_features – size of each input sample\nout_features – size of each output sample\nbias – If set to False, the layer will not learn an additive bias. Default: True\n\n\n\n\nin_features指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。\nout_features指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。\n从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。\n\n[ ] ```pythonimport torch as tfrom torch import nn\nin_features由输入张量的形状决定，out_features则决定了输出张量的形状connected_layer = nn.Linear(in_features = 64643, out_features = 1)\n假定输入的图像形状为[64,64,3]input = t.randn(1,64,64,3)\n将四维张量转换为二维张量之后，才能作为全连接层的输入input = input.view(1,64643)print(input.shape)output = connected_layer(input) # 调用全连接层print(output.shape)\ninput shape is %s torch.Size([1, 12288])output shape is %s torch.Size([1, 1])```\n\n\n","categories":["tools解决方案"]},{"title":"sklearn","url":"/2022/05/19/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-sklearn/","content":"多层感知器分类器： MLPCLassifierclass sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,), activation=&#x27;relu&#x27;, *, solver=&#x27;adam&#x27;, alpha=0.0001, batch_size=&#x27;auto&#x27;, learning_rate=&#x27;constant&#x27;, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)[source]\n该模型使用 LBFGS 或随机梯度下降优化对数损失函数。\n参数\nhidden_layer_sizes ：默认(100, )，第 i 个元素表示第 i 个隐藏层中的神经元数量。\nactivation：激活函数，默认relu 。\n\nidentity：$f(x) = x$\nlogistic：其实就是$sigmod,f(x) = 1 / (1 + exp(-x)). $\ntanh：$f(x) = tanh(x)$\nrelu：$f(x) = max(0, x) $\n\n\nsolver： {‘lbfgs’, ‘sgd’, ‘adam’}，默认adam，用来优化权重。\n\nlbfgs：quasi-Newton方法的优化器 \n\nsgd：随机梯度下降 \n\nadam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 \n\n\n\n\n注意：默认solver ‘adam’在相对较大的数据集上效果比较好（几千个样本或者更多），对小数据集来说，lbfgs收敛更快效果也更好。 \n\nalpha：默认0.0001，正则化项参数 \nbatch_size ： int , 默认’auto’，随机优化器的小批量大小。如果求解器是“lbfgs”，分类器将不使用 minibatch。当设置为“自动”时，batch_size=min(200, n_samples)。\nlearning_rate ：学习率，用于权重更新，只有当solver为’sgd’时使用，默认constant。\n\n‘constant’： 有 ‘learning_rate_init’ 给定的恒定学习率 \n‘invscaling’：随着时间 t 使用’power_t’（需设置，默认0.5）的逆标度指数不断降低 effective_learning_rate = learning_rate_init / pow(t, power_t) .\n‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. \n\n\nlearning_rate_int：double，默认0.001，初始学习率，只有当solver=’sgd’ 或’adam’时使用。 \n\nmax_iter：int，可选，默认200，最大迭代次数。 \n\nrandom_state：int 或RandomState，可选，默认None，随机数生成器的状态或种子。 \n\nverbose ：bool, 默认False，是否打印过程 \n\n\n属性\nclasses_：ndarray or list of ndarray of shape (n_classes,)，输出类标签。 \n\nloss_：float，损失函数计算出来的当前损失值 。\n\nbestloss：最小损失值。\nlosscurve：损失值列表。\n\ncoefs_：列表中的第i个元素表示i层的权重矩阵。\n\nintercepts_：列表中第i个元素代表i+1层的偏差向量。\n\nnfeatures_in ：拟合期间看到的特征数。\nfeaturenames_in ：拟合期间看到的特征名称。\n\nniter ：迭代次数。\n\nnlayers：层数。\n\nnoutputs：输出的个数。\n\noutactivation：输出激活函数的名称。 \n\n\n方法\nfit(X,y)：拟合，返回经过训练的 MLP 模型。\npartial_fit(X,y)：对给定数据进行一次迭代更新模型。一般用在如果训练集数据量非常大，一次不能全部载入内存的时候。这时我们可以把训练集分成若干等分，重复调用partial_fit来一步步的学习训练集。\n\nget_params([deep])：获取参数。\n\npredict(X)：使用MLP进行预测。\n\npredic_log_proba(X)：返回概率估计的对数。\n\npredic_proba(X)：返回模型中每个类的样本的预测概率。\n\nscore(X,y[,sample_weight])：返回给定测试数据和标签上的平均准确度，这里的sample_weight和初始化权重不是一回事。\n\nset_params(params)：设置参数。\n\n\n关于权重初始化sklearn中MLPClassifier类没有权重矩阵，只有fit之后才有，即不支持初始化权重矩阵。查看源码可以看出默认采用xavier均匀分布初始化矩阵。同理设置bias。\n\nW～U[-\\frac{\\sqrt{6}}{\\sqrt{n_j+n_{j+1}}},\\frac{\\sqrt{6}}{\\sqrt{n_j+n_{j+1}}}]\n带有 SGD 训练的线性分类器： SGDClassifierclass sklearn.linear_model.SGDClassifier(loss=&#x27;hinge&#x27;, *, penalty=&#x27;l2&#x27;, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate=&#x27;optimal&#x27;, eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n该类实现了用SGD方法进行训练的线性分类器（比如线性SVM，逻辑回归等）。模型每次使用一个样本来估计损失函数梯度。模型的学习速率会随着迭代地进行而减小。\nscoring的参数（模型评估规则）Plotting Learning Curves","categories":["tools解决方案"]},{"title":"torch","url":"/2022/05/11/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-torch/","content":"nn.Embeddingimport torchfrom torch import nn&#x27;生成5行3列的一个table&#x27;embedding=nn.Embedding(5, 3)# Embedding(5, 3)&#x27;服从N(0,1)取点，填充table的weight&#x27;embedding.weight#Parameter containing:#tensor([[ 0.3625, -0.7382, -1.2715],#        [-0.4616, -0.8950, -0.0341],#        [ 0.2797, -2.0152, -0.1537],#        [ 0.9470, -0.1691,  0.8290],#        [ 0.3921, -2.2116, -0.5891]], requires_grad=True)&#x27;&#x27;&#x27;输出的type就是input的type，其中的值就是刚才weight中索引，比如0就是第一行：[ 0.3625, -0.7382, -1.2715]，按照input的格式排列&#x27;&#x27;&#x27;input = torch.LongTensor([[1,2,4],[4,3,2]])embedding(input)# tensor([[[-0.4616, -0.8950, -0.0341],#          [ 0.2797, -2.0152, -0.1537],#          [ 0.3921, -2.2116, -0.5891]],#         [[ 0.3921, -2.2116, -0.5891],#          [ 0.9470, -0.1691,  0.8290],#          [ 0.2797, -2.0152, -0.1537]]], grad_fn=&lt;EmbeddingBackward0&gt;)\nnn.LinearPyTorch的nn.Linear（）是用于设置网络中的全连接层的，需要注意在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量，形状通常为[batch_size, size]，不同于卷积层要求输入输出是四维张量。其用法与形参说明如下：\n对输入数据应用线性变换： $y=xA^T+b$ ，支持浮点数。\ntorch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n\nin_features – size of each input sample，指的是输入的二维张量的大小，即输入的[batch_size, size]中的size。\nout_features – size of each output sample，指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。\nbias – If set to False, the layer will not learn an additive bias. Default: True \n\n从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量\nimport torch as tfrom torch import nn# in_features由输入张量的形状决定，out_features则决定了输出张量的形状 connected_layer = nn.Linear(in_features = 64*64*3, out_features = 1)# 假定输入的图像形状为[64,64,3]input = t.randn(1,64,64,3)# 将四维张量转换为二维张量之后，才能作为全连接层的输入input = input.view(1,64*64*3)print(input.shape)output = connected_layer(input) # 调用全连接层print(output.shape)# input shape is %s torch.Size([1, 12288])# output shape is %s torch.Size([1, 1])\n","categories":["tools解决方案"]},{"title":"一些教程","url":"/2022/04/02/tools%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E4%B8%80%E4%BA%9B%E6%95%99%E7%A8%8B/","content":"pygamehttps://blog.csdn.net/qq2499094166?type=blog  ：2021.9.13系列。\n【梯度下降法】一：快速教程、Python简易实现以及对学习率的探讨【梯度下降法】二：冲量（momentum）的原理与Python实现【梯度下降法】三：学习率衰减因子（decay）的原理与Python实现\n总结  https://blog.csdn.net/zsfcg/article/details/90477938\n\n机器学习实验室：机器学习公式推导与代码实现训练集和测试集的分布不同\n协变量偏移\n划分训练/验证/测试集\n过拟合与数据不匹配\n总结\n\n交叉验证 Cross-validation聊一聊机器学习的MLE和MAP：最大似然估计和最大后验估计(1) 频率学派：存在唯一真值 $\\theta$。\n\n所优化的函数被称为Negative Log Likelihood (NLL)\n深度学习做分类任务时所用的cross entropy loss，其本质也是MLE\n\n(2) 贝叶斯学派： $\\theta$ 是一个随机变量，符合一定的概率分布。在贝叶斯学派里有两大输入和一大输出，输入是先验 (prior)和似然 (likelihood)，输出是后验 (posterior)。\n\n同样是抛硬币的例子，对一枚均匀硬币抛5次得到5次正面，如果先验认为大概率下这个硬币是均匀的 (例如最大值取在0.5处的Beta分布)，那么  ，即  ，是一个distribution，最大值会介于0.5~1之间，而不是武断的  = 1。\n\n这里有两点值得注意的地方：\n\n随着数据量的增加，参数分布会越来越向数据靠拢，先验的影响力会越来越小\n\n如果先验是uniform distribution，则贝叶斯方法等价于频率方法。因为直观上来讲，先验是uniform distribution本质上表示对事物没有任何预判\n\n\n\n\ni.i.d.表示Independent and identical distribution，独立同分布。\n\n\n开发Encoder-Decoder LSTM模型的简单教程（附代码）\nPyTorch30——LSTM和LSTMP的原理及其逐行代码实现\npycharm上传更新内容至linux服务器\npycharm远程服务器运行Can‘t run remote python interpreter:Can‘t get remote credentials for deployment server\nssh远程链接服务器，利用screen命令，避免因为断网而中断模型训练\n令人不悦的Error–requests.exceptions.ProxyError\nssl.CertificateError\n","categories":["tools解决方案"]},{"title":"01简介至06矩阵运算","url":"/2022/01/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0v2-01-06/","content":"\n\n\n筛选的Q&amp;A1、copy和clone的区别copy分深拷贝和浅拷贝，不一定会复制内存；clone一定会复制内存。\n2、torch不区分行向量与列向量吗行向量 $[1, 2, 3]$ ， 列向量 $ [[1], [2], [3]]$，行向量的shape是 $[n]$，列向量的shape是 $[n,1]$ .\n如果一个问题能够得到最优解，那么它就是p问题，机器学习不关心p问题，只关心np的问题。\n","categories":["动手学深度学习v2"]},{"title":"07自动求导至08线性回归+基础优化算法","url":"/2022/02/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0v2-07-08/","content":"筛选的Q&amp;A1、为什么深度学习中一般对标量求导而不是对矩阵或者向量？因为loss通常是一个标量，精度和loss在机器学习中都是一个标量。如果loss变成一个向量，那就会很麻烦。而且向量关于矩阵的loss就会变成一个矩阵，那么，矩阵再往下走，就会变成一个四维矩阵，那么神经网络一深，那就会变成一个特别特别大的张量，就算不出来了，所以loss通常是一个标量。\n2、多个loss分别反向的时候是不是要累计梯度？是的，假设在之后的神经网络有多个损失函数的话，是需要累积梯度的。也是为什么torch是默认累计梯度的。\n3、为什么获取grad前需要backward？不做backward的话不会去计算梯度，计算梯度是很贵的一件事情（时间和内存上）。\n4、求导过程一般来说是不是有向图？有没有环状的图结构？循环神经网络，但是计算上还是会展开，虽然逻辑上是有环的图。\n5、为什么用平方损失而不用绝对差值呢？因为绝对差值不可导。\n6、为什么机器学习优化算法都采用梯度下降，而不采用牛顿法？收敛速度更快，一般能算出一阶导，二阶导也能算。首先，二阶导不好算，不是总是能算出来的。 统计模型（损失函数），优化模型（我用什么算法优化），所以说把统计模型的最优解求出来的意义并不大，收敛快不快不那么关心。关心的是收敛导哪个地方，泛化性不一定比梯度下降好，不一定更实用。\n7、detach是什么作用告诉程序不用算梯度了。\n8、每次都随机取出一部分，怎么保证最后所有数据都被拿过了？bootstraping采样。次数多的情况下会遇到所有的数据，数据多的话也没必要所有的数据都用到。\n9、如果样本大小不是批量数的整数倍，那需要随机剔除多余的样本吗？\n改batch_size，使得epoch迭代完整 \n忽略最后一次epoch的迭代 \n不足部分从原有数据中抽出差额补齐\n\n08 线性回归 + 基础优化算法【动手学深度学习v2】\n","categories":["动手学深度学习v2"]},{"title":"1 介绍","url":"/2022/03/27/%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E5%A5%BD%E7%A8%8B%E5%BA%8F-1-%E4%BB%8B%E7%BB%8D/","content":"最重要的是建立一个良性循环。\n\n写出好程序  $\\rightarrow$ 享受编程  $\\rightarrow$ 工作成就感  $\\rightarrow$ 写出好程序\n\n本书适合：\n\n正在学习编程\n正在帮助其他人学习编程\nnot 专业的程序员\n\n\nnote ： 编写计算机程序时会使用美式拼写。\n\n术语：编码，编程，软件工程\n编码，编程，软件工程三者互有重叠，都涉及给计算机下指令。它们的复杂程度依次递增。\n\n编码员可能只会把精确的英文指令翻译成编程语言。\n\n程序员要负责决定编写什么、什么时候完成。\n\n软件工程师通常作为团队的一员参与工作，用高质量的软件解决实际问题。\n\n\n学会跳出目前所学的任何一种语言的局限去思考，并在不同的语言之间灵活运用编程技能。\n","categories":["如何写出好程序"]},{"title":"LSTM","url":"/2022/03/10/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0v2-LSTM/","content":"\n\n\n\n\n\n\n参考：\n\n57 长短期记忆网络（LSTM）【动手学深度学习v2】\n【LSTM长短期记忆网络】3D模型一目了然，带你领略算法背后的逻辑\n【重温经典】大白话讲解LSTM长短期记忆网络 如何缓解梯度消失，手把手公式推导反向传播\n\n","categories":["动手学深度学习v2"]},{"title":"2 什么是好程序","url":"/2022/07/09/%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E5%A5%BD%E7%A8%8B%E5%BA%8F-2-%E4%BB%80%E4%B9%88%E6%98%AF%E5%A5%BD%E7%A8%8B%E5%BA%8F/","content":"好程序好程序做它们应该做的事情，它必须是正确的。\n好程序都编写得非常清晰，尽可能易于阅读和理解。\n好程序都非常简洁？\nsomeother：高效、可移植、灵活、可测试、内存效率高、可并行等等\n道德编程包括坦诚地面对你对于自己的程序到底有多大的信心，坚持质量控制流程，以及确保如果你犯了错误就在它造成损失之前发现并纠正这个错误。 （更多移步：编程伦理\n","categories":["如何写出好程序"]},{"title":"4 如何理解编程语言","url":"/2022/07/12/%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E5%A5%BD%E7%A8%8B%E5%BA%8F-4-%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/","content":"假定你必须学习别人为你选择的语言。不过，你需要了解所有编程语言的整体格局，以及你所学的语言在其中的位置。\nQ：理解语言不就是学习并掌握它写程序吗？\nA：不，并不仅仅是这样。了解一些语言的设计决策也很有用：它如何以及为什么与其他语言有所区别。\n一些问题：\n这种语言是出于什么目的而开发的？在什么时候由谁开发的？\n现在谁在使用它，为了什么？\n使用这种语言的人拥有什么样的社区？他们在网上哪些地方活动？\n你的语言是编译型的还是解释型的？\n它实行什么样的类型系统？\n用这种语言编写的程序有怎样的层次结构？\n人们遵守什么约定或惯例？你会惊讶于这些约定的重要性，比如单词如何大写，程序各个部分有多长才会决定切分，到使用哪些库。\n\n1 编译与解释一旦我写好了我的程序，如何让它运行？\n\n直接运行\n适用于解释性语言，例如Python，JavaScript，PHP和Perl。这意味着存在一些称为解释器的程序，会读取并解释执行你的程序。如果程序中的某个地方出现了问题，就意味着程序的一部分不能被解释，解释器会在读到那个部分时给出一些错误信息，并停止执行。然而，程序的前面部分可能在那时已经被执行了。\n\n先编译，再运行\n适用于编译型语言，比如Java，Haskell和所有C的变体，它们读取你的程序并将其转换为更底层的形式。程序中某些类型的错误可以在编译过程中检测到，如果没有发现这样的错误，那么你将得到一个编译后的程序，他被保存为一个单独的文件，然后可以像上面那样运行。由于运行中所需的部分工作已经由编译器完成，因此编译后的程序通常比具有相同功能的解释型程序运行的更迅捷。然而更重要的是，由于编译器对某些类型的错误进行了检查，你就可以确保如果程序能够正确的编译，那么这些错误将不会在运行中发生。\n\n\n严格来说，一种语言是编译型还是解释型，是该语言实现层面的属性，而不是该语言本身的属性。例如python也可以编译成.pyc的形式。\n2 类型程序中值的类型告诉你可以合法地使用它做什么。\n\n类型检查是检查程序个部分的形态是否能正确组合的过程：例如，设计为仅接受整数的函数\n\nto be continued…\n","categories":["如何写出好程序"]},{"title":"Bayesian Networks","url":"/2022/02/22/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-Bayesian%20Networks/","content":"youtube：Bayesian Networks\n\n\n\n","categories":["数学基础"]},{"title":"3 如何开始","url":"/2022/07/10/%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E5%A5%BD%E7%A8%8B%E5%BA%8F-3-%E5%A6%82%E4%BD%95%E5%BC%80%E5%A7%8B/","content":"假设这是第一次练习，你需要创建一个程序。\n1 究竟什么是程序程序是你想让计算机遵循的指令集合。可以这样说，我们最好把程序看作一条可能具有复杂结构的指令。这个集合不是数学意义上的集合，而是类似一个简单的指令列表，告诉你先做一件事，再做一件事，而程序可以有更加有趣的结构。程序的各个部分组合成一个整体的方式，就是不同语言之间的差异之一。\n计算机并不智能，它不是朋友也不是敌人。它只是一台机器。学习编程也包括了解如何使这台机器做你想让它做的事情。\n2 你需要什么\n一种表达指令的方式\n交互式环境\n文本编辑器\n集成开发环境（IDE）：是一种支持程序开发全过程的应用程序。\n\n\n一种让它们在计算机硬件上运行的方法。\n安装环境（搜索 —— 安装 + 你的编程语言）\n\n\n\n2.1 交互式提示在某些语言中——包括Python和Haskell，你可能会在某种交互式的提示下开始探索你的语言，有时被称为“读取—求值—输出”循环，简称为 REPL。在语言安装成功后，可以通过在操作系统命令行种输入命令来启动交互式工具，得到一个交互式提示。\n\n操作系统命令行是启动应用程序的指令。\n语言级交互式提示是在编程语言上下文中有意义的内容。\njupyter notebook\n\n2.2 使用文本编辑器例如windows上的记事本，mac上的TextEdit，linux上的vi或者Emacs。or 其他的例如Atom和Sublime，notepad++等等。\n如果遇到困难，搜索——“你的编辑器名称 + 教程”\n2.3 了解待办任务\n每当有代码可供使用时，你要抓住机会从中学习。你能学到多少？能从中识别出你不知道道某些知识点吗？\n\n在开始敲代码之前，先仔细阅读这些说明，确保你充分理解了题目的要求。切记，计算机只是一台机器，所以像名称这样的细节会至关重要。如果你要编写一个函数，题目会告诉你某种情况下函数应该返回什么。通过找到另外一到两种情况的返回值来验证你的理解。\n2.4 编写程序\n初学者所犯的最大的错误就是在检查其程序是否有效之前编写了太多的代码。\n\n我们可以将任务分解成更小的单元，并在开始之前先检查每个单元是否工作，先编写代码的骨架，再填充细节。\n\n语法和语义：语法就是告诉你，在这门语言中哪些写法是合法的。只有当程序的语法正确时，才有必要进一步了解它的作用或含义，即语义。\n\n一般来说，首先必须了解的是，你所编写的程序将会被调用的部分是什么。请使用最具描述性的名称。\n确保函数的名称正确（包括大小写）外，还需要保证代码显示了正确的参数，以及参数的顺序都是正确的。如果给定类型，那么参数和返回值的类型必须正确。要熟练使用pass，pass是表示什么都不做，之所以有这个词是因为python中不允许存在空白的函数体。以及—Todi的注释。\nQ：为什么从外到内的编写更有益？因为通常外部对于你要做的事情的规范至关重要。例如编写函数时，最重要的事情时了解输入的信息（参数及其类型）和输出的信息——返回值和它的类型，以及产生的一些其他效果。外部世界不关心你是如何实现函数的，他们关心的是，当他们给出合适的参数时，会得到一个合适的结果。\n感到困惑怎么办？可以记录下编程进展。例如note.txt或者note.md的文件。面对一个大规模的问题，必须明白做什么，在不确定如何编写的时候可以先写一个设计说明，规划出代码的结构或计算方法。边写代码边测试，随着对问题和解决方案理解的加深，改进程序的结构，通常更加有效。\n\n优先解决最简单的问题。\n\n当你发现程序不能运行时，要记下当时的情况：你的程序应该做什么，它实际做了什么。（更多移步：自动化测试，第7章\n\nfoo bar是程序员最早使用的两个名称，是俚语FUBAR的缩写，代表“情况糟糕的无法收拾”。\n\n","categories":["如何写出好程序"]},{"title":"矩阵求导与分子、分母布局","url":"/2022/02/05/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E4%B8%8E%E5%88%86%E5%AD%90%E3%80%81%E5%88%86%E6%AF%8D%E5%B8%83%E5%B1%80/","content":"原文见：矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇）\n1 函数与标量、向量、矩阵考虑一个函数\n\n\\text{function}(\\text{input})针对 $function$ 的类型、$input$ 的类型，我们可以将这个函数分为 $function$ 不同的种类。\n1、 $function$ 是一个标量我们称 $function$ 是一个实值标量函数。用细体小写字母 $f$ 表示。\n1.1 $input$ 是一个标量我们称 $function$ 的变元是标量。用细体小写字母 $x$ 表示。\n例1：\n\nf(x)=x+21.2 $input$ 是一个向量我们称 $function$ 的变元是向量。用粗体小写字母 $\\pmb{x}$ 表示。\n例2：设 $\\pmb{x}=[x_1,x_2,x_3]^T$\n\nf(\\pmb{x})=a_1x_1^2+a_2x_2^2+a_3x_3^2+a_4x_1x_21.3 $input$ 是一个矩阵我们称 $function$ 的变元是矩阵。用粗体大写字母 $\\pmb{X}$ 表示。\n例3：设 $\\pmb{X}{3\\times 2}=(x{ij})_{i=1,j=1}^{3,2}$\n\nf(\\pmb{X})=a_1x_{11}^2+a_2x_{12}^2+a_3x_{21}^2+a_4x_{22}^2+a_5x_{31}^2+a_6x_{32}^2\n\\left\\{ \\begin{align*} \\frac{\\partial f}{\\partial x_1} & = 2x_1+x_2 \\\\\\\\ \\frac{\\partial f}{\\partial x_2} & = x_1+x_3 \\\\\\\\ \\frac{\\partial f}{\\partial x_3} & = x_2 \\end{align*} \\right.2、$function$ 是一个向量我们称 $function$ 是一个实向量函数 。用粗体小写字母表 $\\pmb{f}$ 示。\n含义： $\\pmb{f}$ 是由若干个 $f$ 组成的一个向量。\n同样地，变元分三种：标量、向量、矩阵。这里的符号仍与上面相同。\n2.1 标量变元例4：\n\n\\pmb{f}_{3\\times1}(x)= \\left[ \\matrix{ f_1(x)\\\\ f_2(x)\\\\ f_3(x)\\\\ } \\right] = \\left[  \\matrix{ x+1\\\\ 2x+1\\\\ 3x^2+1 } \\right]2.2 向量变元例5：设 $\\pmb{x}=[x_1,x_2,x_3]^T$ \n\n\\pmb{f}_{3\\times1}(\\pmb{x})= \\left[ \\matrix{ f_1(\\pmb{x})\\\\ f_2(\\pmb{x})\\\\ f_3(\\pmb{x})\\\\ } \\right] = \\left[  \\matrix{ x_{1}+x_{2}+x_{3}\\\\ x_{1}^2+2x_{2}+2x_{3}\\\\ x_{1}x_{2}+x_{2}+x_{3} } \\right]2.3 矩阵变元例6：设 $\\pmb{X}{3\\times 2}=(x{ij})_{i=1,j=1}^{3,2}$ \n\n\\pmb{f}_{3\\times1}(\\pmb{X})= \\left[ \\matrix{ f_1(\\pmb{X})\\\\ f_2(\\pmb{X})\\\\ f_3(\\pmb{X})\\\\ } \\right] = \\left[  \\matrix{ x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\ x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}+x_{11}x_{12}\\\\ 2x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}+x_{11}x_{12} } \\right]3、$function$ 是一个矩阵我们称 $function$ 是一个实矩阵函数 。用粗体大写字母 $\\pmb{F}$ 表示。\n含义： $\\pmb{F}$ 是由若干个 $f$ 组成的一个矩阵。\n同样地，变元分三种：标量、向量、矩阵。这里的符号仍与上面相同。\n3.1 标量变元例7：\n\n\\pmb{F}_{3\\times2}(x)= \\left[ \\matrix{ f_{11}(x) & f_{12}(x)\\\\ f_{21}(x) & f_{22}(x)\\\\ f_{31}(x) & f_{32}(x)\\\\ } \\right] = \\left[  \\matrix{ x+1 & 2x+2\\\\ x^2+1 & 2x^2+1\\\\ x^3+1 & 2x^3+1 } \\right]3.2 向量变元例8：设 $\\pmb{x}=[x_1,x_2,x_3]^T$ \n\n\\pmb{F}_{3\\times2}(\\pmb{x})= \\left[ \\matrix{ f_{11}(\\pmb{x}) & f_{12}(\\pmb{x})\\\\ f_{21}(\\pmb{x}) & f_{22}(\\pmb{x})\\\\ f_{31}(\\pmb{x}) & f_{32}(\\pmb{x})\\\\ } \\right] = \\left[  \\matrix{ 2x_{1}+x_{2}+x_{3} & 2x_{1}+2x_{2}+x_{3} \\\\ 2x_{1}+2x_{2}+x_{3} & x_{1}+2x_{2}+x_{3} & \\\\ 2x_{1}+x_{2}+2x_{3} & x_{1}+2x_{2}+2x_{3} & } \\right]3.3 矩阵变元例9：设 $\\pmb{X}{3\\times 2}=(x{ij})_{i=1,j=1}^{3,2}$ \n\n\\begin{align*} \\pmb{F}_{3\\times2}(\\pmb{X})&= \\left[ \\matrix{ f_{11}(\\pmb{X}) & f_{12}(\\pmb{X})\\\\ f_{21}(\\pmb{X}) & f_{22}(\\pmb{X})\\\\ f_{31}(\\pmb{X}) & f_{32}(\\pmb{X})\\\\ } \\right]\\\\\\\\ &= \\left[  \\matrix{ x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 2x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\ 3x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 4x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32}\\\\ 5x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} & 6x_{11}+x_{12}+x_{21}+x_{22}+x_{31}+x_{32} } \\right] \\end{align*}4、总结\n\n\n\nFunction\\input\n标量变元\n向量变元\n矩阵变元\n\n\n\n\n实值标量函数\n$f(x)$\n$f(\\pmb{x})$\n$f(\\pmb{X})$\n\n\n实向量函数\n$\\pmb{f}(x)$\n$\\pmb{f}(\\pmb{x})$\n$\\pmb{f}(\\pmb{X})$\n\n\n实矩阵函数\n$\\pmb{F}(x)$\n$\\pmb{F}(\\pmb{x})$\n$\\pmb{F}(\\pmb{X})$\n\n\n\n\n2 矩阵求导的本质对于一个多元函数\n例10：\n\nf(x_1,x_2,x_3)=x_1^2+x_1x_2+x_2x_3我们可以将 $f$ 对 $x_1,x_2,x_3$ 偏导分别求出来，即：\n\n\\left\\{ \\begin{align*} \\frac{\\partial f}{\\partial x_1} & = 2x_1+x_2 \\\\\\\\ \\frac{\\partial f}{\\partial x_2} & = x_1+x_3 \\\\\\\\ \\frac{\\partial f}{\\partial x_3} & = x_2 \\end{align*} \\right.矩阵求导也是一样的，本质就是 $function$ 中的每个 $f$ 分别对变元中的每个元素逐个求偏导，只不过写成了向量、矩阵形式而已。\n我们把得出的3个结果写成列向量形式：\n\n\\frac{\\partial f(\\pmb{x})}{\\partial \\pmb{x}_{3\\times1}}= \\left[ \\matrix{ \\frac{\\partial f}{\\partial x_1}\\\\ \\frac{\\partial f}{\\partial x_2}\\\\ \\frac{\\partial f}{\\partial x_3}\\\\ } \\right] = \\left[  \\matrix{ 2x_1+x_2\\\\ x_1+x_3\\\\ x_2 } \\right]一个矩阵求导以列向量形式展开的雏形就出现了。\n当然我们也可以以行向量形式展开：\n\n\\frac{\\partial f(\\pmb{x})}{\\partial \\pmb{x}_{3\\times1}^T}=  \\left[  \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\frac{\\partial f}{\\partial x_3}  \\right]  =  \\left[   2x_1+x_2, x_1+x_3, x_2  \\right]所以，如果 $function$ 中有 $m$ 个 $f$ ，变元中有 $n$ 个元素，那么，每个 $f$  对变元中的每个元素逐个求偏导后，我们就会产生 $m\\times n$ 个结果。至于结果的布局，是写成行向量，还是写成列向量，还是写成矩阵，就是我们接下来要讨论的事情。\n3 矩阵求导结果的布局不严谨地说，从直观上看：分子布局，就是分子是列向量形式，分母是行向量形式；分母布局，就是分母是列向量形式，分子是行向量形式。\n不同类型的变元，严谨的布局说明见原文：矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇）\n补充：来自机器学习中的矩阵和向量的求导x表示n维向量，X表示m×n维度的矩阵,y表示m维向量，Y表示p×q 维度的矩阵。\n\n\n解释几点：\n\n对于一个复杂的变量求导的时候，比如复合函数求导涉及到后面要讲的链式求导时候，只能选一种求导布局，不能同时出现分子布局和分母布局,分子布局与分母布局算出的结果只相差一个转置\n\n列向量y对列向量x的求导的分子布局形式可以理解为，分子向量y对分母向量x的第一个标量元素求导后排在第一列，对第二个标量元素求导后排在第二列，以此类推，排成一个m×n的矩阵。 也被称为雅克比矩阵：\n\n\n图中的默认布局是指常用的布局形式(向量和矩阵在分子上就用分子布局，否则用分母布局)，也是符合大家习惯的写法。\n\n\n4 分子布局、分母布局的本质总结：\n1、分子布局的本质：分子是标量、列向量、矩阵向量化后的列向量；分母是标量、列向量转置后的行向量、矩阵的转置矩阵、矩阵向量化后的列向量转置后的行向量。\n2、分母布局的本质：分子是标量、列向量转置后的行向量、矩阵向量化后的列向量转置后的行向量；分母是标量、列向量、矩阵自己、矩阵向量化后的列向量。\n思考一下，其实可以再简洁一些：谁转置了，就是另一方的布局。分子转置了，就是分母布局；分母转置了，就是分子布局。\n","categories":["数学基础"]},{"title":"第1章 日常用语的骗局","url":"/2022/02/13/%E6%9C%89%E6%AF%92%E7%9A%84%E9%80%BB%E8%BE%91-%E7%AC%AC1%E7%AB%A0-%E6%97%A5%E5%B8%B8%E7%94%A8%E8%AF%AD%E7%9A%84%E9%AA%97%E5%B1%80/","content":"\n&nbsp;&nbsp;我只知道它言之无理，却讲不出所以然。\n基本原则首先是一些基本原则。我们不能称之为规律，它们不过是一些行为模式的描述，大致概括了人类反应和思考的倾向方式。比如，人们会：\n\n倾向于相信自己愿意相信的事物。\n倾向于将自身的偏爱或经验投射于现实生活。\n倾向于对特殊事件进行普遍化概括。\n倾向于身临其境地分析事件并且让自身情感超越客观理性。\n不能作为很好的聆听者。人们会有选择地听，并且常常只听自己想听的内容。\n拥有对所观察到的事物进行合理化(文饰)的冲动。\n常常无法从无关紧要的事物中提取相关、重要的事物。\n很容易从正在处理的特定事情上转移注意力。\n通常不愿意彻底地探索主题的盘根错节，倾向于过度简单化。\n一般都以貌取人。人们观察事物时，曲解自己的观察，然后做出可怕的误判。\n总是不知所谓，尤爱泛泛而谈。说话之前很少审慎思考，却让情感、偏见、成见、好恶、希望和挫折代替审慎思考。\n很少坚持一贯的行为准则。人们很少仔细检查证据然后得出结论。相反，人们更倾向于想怎么做就怎么做，想相信什么就相信什么，然后找出能够支持自己行动或信念的证据。人们的思考具有选择性：在评估现实情况时，热衷于寻找支持自己所支持事物的理由，同样热衷于忽视或者漠视不支持他们所支持事物的理由。\n人们往往不会将内心的本意宣之于口，而说出的话通常别有深意。\n\n观察评论针对这些原则，我们再加上由J.A.C.布朗 (J.A.C.Brown) 在《说服的技巧》(Techniques of Persuasion)一书中所引用的四条观察评论：\n①大多数人宁愿觉得事情简单而不复杂。\n②希望自己的偏见得到证实。\n③想要体验其他人无法企及的“归属感”。\n④需要为自己的挫败精确指定背黑锅的假想敌。\n未经训练的大脑更容易选择阻力最小的道路。阻力最小的道路就是几乎从不运用理性思考。\n","categories":["有毒的逻辑"]},{"title":"第3章 感性的语言：宣传鼓动","url":"/2022/02/15/%E6%9C%89%E6%AF%92%E7%9A%84%E9%80%BB%E8%BE%91-%E7%AC%AC3%E7%AB%A0%20%E6%84%9F%E6%80%A7%E7%9A%84%E8%AF%AD%E8%A8%80-%E5%AE%A3%E4%BC%A0%E9%BC%93%E5%8A%A8/","content":"\n&nbsp;&nbsp;我了解你们人类。你们都是乖乖听话的羔羊。\n在广泛意义上，宣传鼓动只不过是一种说服手段，一种诉诸感性而非诉诸理性的形式。它依赖于人们对暗示的感受。它企图诱使我们以特定的方式来行动或思考；它企图影响我们的信念并最终影响我们的态度。它的手段通常狡猾而隐秘。鼓吹者不会当面摊开他的手牌；有一个隐藏的动机，一条底线，隐而不发。\n成功的鼓吹者善于俘获我们的情感。他大量运用第2章所提到的感性诉求。他说着我们想听的话，赢取我们的信任，然后开始巧妙地影响我们的态度。鼓吹者几乎从不为其所倡导的内容提供完好的理由。即使他提供证据，也是精心选择之后的一面之词。他将事情过度简单化，并且常常任意歪曲事实。\n以下详细说明鼓吹者可能采取的一些策略。\n随波逐流。“大家都这么做。所以，你也应该这样做。”显然，此命题过度简单化，并不是大家都这么做。随波逐流策略至少发挥了两方面作用。\n\n第一，它企图造成一种印象，有许多人都在做某件事情或者支持某种立场；然后它暗示群众的判断是合理的：如果这么多人都这么做，那它一定是对的。\n第二，更重要的一点，随波逐流策略是满足我们内心归属需要的一种情感诉求。我们不想被别人抛弃。“所以，跳到乐队花车上来。融入狂欢的人潮，享受快乐和安全感。”\n\n不断重复。鼓吹者反复地诉说某事。他每次可能使用不同的措辞，但主题却始终如一。此种理论就是，如果你重复诉说的次数足够多，人们终究会相信你。\n自信。鼓吹者的谈吐也是自信满满的。他留给人的印象是知道自己在说什么。他的声音铿锵有力，他的表情大义凛然，他的肢体动作坚毅果敢。此处的理论如下：如果有人如此自信，如果他对自己的立场如此确信无疑，那他肯定是对的。人们喜欢支持胜利者，而自信的姿态使人看起来像个胜利者。\n认真和真诚。一个人外表看起来越是认真和真诚，越容易获得人们的信任。\n过度简单化。鼓吹者选取事物的一个方面，并且把它视为宛如仅有的一个方面。比如参与党派竞选的候选人可能只会提及对手的弱点，完全忽略对手的优势所在。他选取一个复杂的问题，然后将其简化到极致，通常以“要么……要么……”二者择其一的方式呈现该问题：“要么你跟随我，那样对大家都好，要么你不跟随我，那样必定损失惨重。”\n污名辱骂。鼓吹者对不喜欢的人物或者观点冠以污辱性的绰号，或者使用引起强烈贬损意味的名称；对喜欢的人物或观点，则致以谄媚般的称呼，或者使用引起强烈褒扬意味的名称。他打算用他的命名来影响我们的态度。他们有时候也会使用“标签”：“种族主义者”“无政府主义者”“社会主义者”“激进分子”“反动派”。\n刻板印象(心理定式)。这是污名辱骂和过度简单化的一种变式。鼓吹者抓住对方的某一性格特征，经过夸张变形，然后视之为唯一的性格特征。这个技巧曾被用来反对已故前副总统休伯特·汉弗莱，不断有人把他画成漫画形象的风囊(比喻饶舌之人)来进行讽刺。刻板印象剥夺一个人的复杂性和独特性，将其简化为一种品质。\n光芒万丈的泛泛而谈。鼓吹者发表大而无当、概括笼统的言论，通常这种言论会带来盘根错节、影响深远的复杂后果，而他却故意忽略这种复杂性和纠纷。他痛斥在野党剥削穷人，没有争取到有效的税制改革立法，但他本身却开不出良方。他沉浸在泛泛而谈的满足中，自鸣得意。光芒万丈的泛泛而谈很安全：如果连具体规划都没提供，又怎么会受到批评呢?\n标语口号。受众会记住讨巧的标语口号，而不去质疑标语口号本身的意义——实际上，甚至不会思考标语口号的意义：美国——要么爱，要么走开。要爱情而不要战争。如果枪械是不合法的，那只有不法之徒才能拥有枪械。\n传递。这个技巧鼓励我们将情感传递下去。香烟广告经常使用这种技巧。香烟总是一成不变地设计在一些豪华的田园风光里，放在一片深绿之中。然后还有万宝路牛仔和维珍妮香烟女郎：“宝贝，你真是远道而来。”传递的技巧在于它了解我们对自己土地的自豪感，我们向往田园的美丽风光，我们崇拜粗犷的万宝路牛仔和苗条的维珍妮香烟女郎，它邀请我们将积极正面的情感传递至所推销的主体上。\n代言。这是传递策略的一种变式。重要或杰出的人物或组织为一个观念或者产品代言。我们仅仅因为杰出的人物为其代言，而被鼓励去支持这种观念或者产品。该理论如下：X是一位重要人物；除非X更杰出或者比我们更有见识，否则他不会如此重要；既然X如此重要，因此他更杰出或者比我们更有见识；因此，他知道自己在说什么；因此，我们应该相信他。我们被鼓励将对此人的肯定传递至他背书的观念或者产品。\n平民百姓。“我和你们一样。”鼓吹者有时候企图通过我们对他的身份认同，从而赢得我们的支持。他参加镇民大会，拜访医院，不拘礼节地摆姿势拍照片。吉米·卡特总统就使用过这种技巧，当他在电视上露面时，他穿便装毛衣而不穿正装。这里的技巧被用于高尚的目标：节约能源。如果总统都能通过明智地选择穿衣而节约能源，我们难道不应该这样吗?\n宣传鼓动可以用于不光彩的目的，也可以用于高尚的目标。因此我需要再重复一遍定义：宣传鼓动仅仅是一种说服手段。平民百姓策略背后的理论认为，如果我们认同某人，我们就会支持他的说法。该策略是传递策略的一种变式。\n讲究派头的诉求。另一方面，宣传鼓动可能利用人们对社会地位的需求，或者利用人们对特殊待遇的渴望：“适合具有独特鉴赏能力的人群”“并非所有人都能拥有”“品质专为有品位的人”“一探高雅”“为在意的人，奉献最好的”……\n缺少背景知识的统计数据。鼓吹者可能给你提供大量的统计数据，但很少会告诉你这些数据的背景知识。他几乎不会告诉你他是如何收集这些数据的，数据来自哪里，或者有多少人参与了投票。只要找对五个人，你几乎可以让任何事情都收获80%的支持率。\n大数字。这是随波逐流(乐队花车)策略的一种变式。欧莱雅染发剂广告宣称：“全世界超过25万名美发师相信欧莱雅染发剂适合你，还需要多说什么?欧莱雅，25万名美发师绝不会错。”\n**人为捏造的困难——坏人，替罪羊。鼓吹者人为捏造困难或者夸大困难，企图让你相信问题如何严重，然后通过将问题归咎于某人，或者暗示他的建议可以解决困难，来为你排忧解难。鼓吹者常常需要一个坏人，一个牺牲品或替罪羊，供其强烈谴责。他的义愤填膺使其听起来真诚可靠，也让他所谈及的问题听起来越发严重而紧急。\n彻底扭曲。有时候鼓吹者筛选信息以呈现片面的观点；有时候他甚至编造数据以达到个人目的；有时候他就直接撒谎。由于我们不知道事实真相，我们就无法质疑他。由于我们无能为力，就像他背对着我们洗牌，而他却能看到底牌；因此，这个策略有时候被称为作弊洗牌法。\n指挥命令。这种策略有危险，因为它很容易适得其反。有些人很乐意按照指令行事。当鼓吹者发出指令，他就在诉诸渴望权威的需求。\n再次说明，这一点必须着重强调，就像感性诉求并非本身有错一样，宣传鼓动也无本质错误。但是，我们必须认识到宣传鼓动的本质，拒绝任其摆布。\n[1]刻板印象主要指人们对某个事物形成一种概括固定的看法，并把这种看法推而广之，认为这个事物或整体都具有该特征，而忽视个体差异。——译者注\n","categories":["有毒的逻辑"]},{"title":"八大常见类型的行列式及其解法","url":"/2022/01/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E5%85%AB%E5%A4%A7%E5%B8%B8%E8%A7%81%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%A1%8C%E5%88%97%E5%BC%8F%E5%8F%8A%E5%85%B6%E8%A7%A3%E6%B3%95/","content":"来源：https://zhuanlan.zhihu.com/p/34685081\n\n  本文记录了八大常见类型的行列式及其解法，解法从一般性到特殊性都有，分享给大家，例子都特别经典好用，希望对线代、高代初学者以及考研党有用。\n\n类型总览：\n\n箭型行列式\n两三角型行列式\n两条线型行列式\n范德蒙德型行列式\n$Hessenberg$型行列式\n三对角型行列式\n各行元素和相等型行列式\n相邻两行对应元素相差K倍型行列式\n\n方法总览：\n\n拆行法\n升阶法\n方程组法\n累加消点法\n累加法\n递推法（特征方程法）\n步步差法\n\n一：箭型行列式最常见最常用的行列式，特征很好辨识，必须掌握，请看下例：\n\neg:D_n= \\left|\\begin{array}{cccc} x_1&1&1 &... &1\\\\ 1&x_2&&&\\\\ 1&&x_3\\\\ ...&&&...\\\\ 1&&&...&x_n \\end{array}\\right|(空白处都为0)$Solution$: 将第一列元素依次减去第$ i$ 列的$\\frac{1}{x_i}$ ,$i=2…n $\n得：\n\nD_n= \\left|\\begin{array}{cccc} x_1-\\frac{1}{x_2}-...-\\frac{1}{x_n}&1&1 &... &1\\\\ 0&x_2&&&\\\\ 0&&x_3\\\\ ...&&&...\\\\ 0&&&...&x_n \\end{array}\\right|所以：\n\nD_n=\\prod_{i=2}^{n}x_i(x_1-\\sum_{i=2}^{n}\\frac{1}{x_i})二：两三角型行列式\n特征为对角线上方元素均为$a$ ,下方元素均为$b$\n\n\n当 $a=b$  时可化为箭型行列式计算，当 $a\\not=b$ 时采用拆行法计算，请看下面两例\n\n\neg1(a=b):D_n=\\left|\\begin{array}{cccc} x_1&b&b &... &b\\\\ b&x_2&b&...&b\\\\ b&b&x_3&...&b\\\\ ...&...&...&...&...\\\\ b&b&b&...&x_n \\end{array}\\right|$Solution$: 将第$i，i=2…n$ 行都减去第一行\n得：\n\nD_n=\\left|\\begin{array}{cccc} x_1&b&b &... &b\\\\ b-x_1&x_2-b&0&...&0\\\\ b-x_1&0&x_3-b&...&0\\\\ ...&...&...&...&...\\\\ b-x_1&0&0&...&x_n-b \\end{array}\\right|即化成了箭型行列式，所以：\n\nD_n=[\\prod_{i=2}^{n}(x_i-b)]\\times[x_1-b(b-x_1)\\sum_{i=2}^{n}\\frac{1}{x_i-b}]\n\neg2(a\\not=b):D_n=\\left|\\begin{array}{cccc} x_1&a&a &... &a\\\\ b&x_2&a&...&a\\\\ b&b&x_3&...&a\\\\ ...&...&...&...&...\\\\ b&b&b&...&x_n \\end{array}\\right|$Solution$:  采用拆行法，目的是为了降阶\n\nD_n=\\left|\\begin{array}{cccc} x_1&a&a &... &a+0\\\\ b&x_2&a&...&a+0\\\\ b&b&x_3&...&a+0\\\\ ...&...&...&...&...\\\\ b&b&b&...&x_n+b-b \\end{array}\\right|\nD_n=\\left|\\begin{array}{cccc} x_1&a&a &... &a\\\\ b&x_2&a&...&a\\\\ b&b&x_3&...&a\\\\ ...&...&...&...&...\\\\ b&b&b&...&b \\end{array}\\right|_{(*)}+\\left|\\begin{array}{cccc} x_1&a&a &... &0\\\\ b&x_2&a&...&0\\\\ b&b&x_3&...&0\\\\ ...&...&...&...&...\\\\ b&b&b&...&x_n-b \\end{array}\\right|将第 $i,i=1…n-1$ 列都减去最后一列，得：\n\nD_n=\\left|\\begin{array}{cccc} x_1-a&0&0 &... &a\\\\ b-a&x_2-a&0&...&a\\\\ b-a&b-a&x_3-a&...&a\\\\ ...&...&...&...&...\\\\ 0&0&0&...&b \\end{array}\\right|+(x_n-b)D_{n-1}所以：\n\nD_n=b\\prod_{i=1}^{n-1}(x_i-a)+(x_n-b)D_{n-1}再由行列式转置不变性得到：\n\nD_n=a\\prod_{i=1}^{n-1}(x_i-b)+(x_n-a)D_{n-1}联立$(11)(12)$ ,得通式：\n\nD_n=\\frac{1}{a-b}[a\\prod_{i=1}^{n}(x_i-b)-b\\prod_{j=1}^{n}(x_j-a)]\n通过适当变换可以化为两三角型行列式的，描述不如大家自己看例子揣摩，也很容易理解的，请看下例\n\n\neg3:D_n=\\left|\\begin{array}{cccc} d&b&b &... &b\\\\ c&x&a&...&a\\\\ c&a&x&...&a\\\\ ...&...&...&...&...\\\\ c&a&a&...&x \\end{array}\\right|$Solution$: 将第一行乘上 $\\frac{a}{b}$ ，第一列乘上 $\\frac{a}{c}$，得：\n\nD_n=\\frac{bc}{a^2}\\left|\\begin{array}{cccc} \\frac{a^2d}{bc}&a&a &... &a\\\\ a&x&a&...&a\\\\ a&a&x&...&a\\\\ ...&...&...&...&...\\\\ a&a&a&...&x \\end{array}\\right|即化成了两三角型行列式\n\n一些每行上有公因子但是无法向上式那样在保持行列式不变得基础上能提出公因子的，采用升阶法，请看下例\n\n\neg4:D_n=\\left|\\begin{array}{cccc} 1+x_{1}^2&x_1x_2&x_1x_3 &... &x_1x_n\\\\ x_2x_1&1+x_{2}^2&x_2x_3&...&x_2x_n\\\\ x_3x_1&x_3x_2&1+x_{3}^2&...&x_3x_n\\\\ ...&...&...&...&...\\\\ x_nx_1&x_nx_2&x_nx_3&...&1+x_{n}^2 \\end{array}\\right|$Solution$:  加边升阶，得：\n\nD_n=\\left|\\begin{array}{cccc} 1&x_1&x_2&x_3&...&x_n\\\\ 0&1+x_{1}^2&x_1x_2&x_1x_3 &... &x_1x_n\\\\ 0&x_2x_1&1+x_{2}^2&x_2x_3&...&x_2x_n\\\\ 0&x_3x_1&x_3x_2&1+x_{3}^2&...&x_3x_n\\\\ 0&...&...&...&...&...\\\\ 0&x_nx_1&x_nx_2&x_nx_3&...&1+x_{n}^2 \\end{array}\\right|再将第 $i,i=2…n+1$ 都减去第一行的$x_i，i=1…n$ 倍，得：\n\nD_n=\\left|\\begin{array}{cccc} 1&x_1&x_2&x_3&...&x_n\\\\ -x_1&1&0&0 &... &0\\\\ -x_2&0&1&0&...&0\\\\ -x_3&0&0&1&...&0\\\\ 0&...&...&...&...&...\\\\ -x_n&0&0&0&...&1 \\end{array}\\right|即又化成了箭型行列式，可得通式：\n\nD_n=1+\\sum_{i=1}^{n}x_{i}^{2}三：两条线型行列式特征是除了主(次)对角线或与其相邻得一条斜线所组成的任意一条线加四个顶点中的某个顶点外，其他元素均为$0$，这类行列式可以直接展开降阶。这段描述有点繁琐，但其实也并不复杂，请看下例理解\n\neg3:D_n=\\left|\\begin{array}{cccc} a_1&b_1& &... &\\\\ &a_2&b_2&...&\\\\ &&a_3&...&\\\\ &&&\\\\ &&...&a_{n-1}&b_{n-1} \\\\ b_n&&...&&a_n \\end{array}\\right| (空白处都为0)$Solution$: 按照第一列两个非$0$元素拉普拉斯展开即可\n\nD_n=\\prod_{i=1}^{n}a_i+(-1)^{n+1}\\prod_{i=1}^{n}b_i四：范德蒙德型行列式范德蒙德行列式大家应该熟悉，而范德蒙德型行列式的特征就是有逐行(列)元素按幂递增(减)，可以将其转化为范德蒙德行列式来计算，请看下例\n\neg:D_n=\\left|\\begin{array}{cccc} a_{1}^n& a_{1}^{n-1}b_1&... &a_1b_1^{n-1}&b_1^n\\\\ a_{2}^n&a_{2}^{n-1}b_2&...&a_2b_2^{n-1}&b_2^n\\\\ ...&...&...&...&...\\\\ a_{n}^n&a_{n}^{n-1}b_n&...&a_nb_n^{n-1}&b_n^n\\\\ a_{n+1}^n&a_{n+1}^{n-1}b_{n+1}&...&a_{n+1}b_{n+1}^{n-1}&b_{n+1}^n \\end{array}\\right|$Solution$:  将每行都提出 $a_i^{n}$,$i=1…n+1 $倍，得：\n\nD_n=\\prod_{i=1}^{n+1}a_i^n\\left|\\begin{array}{cccc} 1& \\frac{b_1}{a_1}&... &(\\frac{b_1}{a_1})^{n-1}&(\\frac{b_1}{a_1})^{n}\\\\ 1&\\frac{b_2}{a_2}&...&(\\frac{b_2}{a_2})^{n-1}&(\\frac{b_2}{a_2})^{n}\\\\ ...&...&...&...&...\\\\ 1&\\frac{b_n}{a_n}&...&(\\frac{b_n}{a_n})^{n-1}&(\\frac{b_n}{a_n})^{n}\\\\ 1&\\frac{b_{n+1}}{a_{n+1}}&...&(\\frac{b_{n+1}}{a_{n+1}})^{n-1}&(\\frac{b_{n+1}}{a_{n+1}})^{n} \\end{array}\\right|上式即为范德蒙德行列式，所以通式为：\n\nD_n=\\prod_{1\\le i","categories":["机器学习数学基础"]},{"title":"机器学习数学基础：微积分","url":"/2022/01/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E5%BE%AE%E7%A7%AF%E5%88%86/","content":"本章介绍微积分中的函数、极限、导数、梯度及积分等基本概念。\n1 函数和极限1.1 函数的定义设 $A$，$B$ 都是非空的数的集合，$f:x \\rightarrow y$ 是从 $A$ 到 $B$ 到一个对应法则，那么从 $A$ 到 $B$ 的映射$f:x \\rightarrow y$ 就叫做函数，记作 $y=f(x)$ ，其中 $x \\in A$，$y \\in B$ .\n1.2 反函数设函数 $ y =f(x) $ 的定义域是 $D$，值域是 $f(D)$ 。如果对于值域 $f(D)$ 中的每一个 $y$ ，在 $D$ 中有且仅有一个 $x$ 使得 $g(y)=x$ ，则按此对应法则得到了一个定义在 $f(D)$ 上的函数，并把该函数称为函数 $y=f(x)$ 的反函数，记作 $x=f^{-1}(y)$，$y \\in f(D)$ .\n1.3 复合函数若 $y$ 是 $u$ 的函数：$y=f(u)$ ，而 $u $ 又是 $x$ 的函数：$u=g(x)$ ，且 $g(x)$ 的函数值的全部或部分在 $f(u)$ 的定义域内，则 $y$ 通过 $u$ 成为 $x$ 的函数，这种函数称为由函数 $u=g(x)$ 和函数 $y=f(u)$ 构成的复合函数，记作 $y=f[g(x)]$ ，其中 $u$ 叫作中间变量。\n1.4 多元函数设有两个独立的变量 $x$ 与 $y$ 在其给定的变域 $ D$ 中，任取一组数值时，第 $3$ 个变量 $ z$ 就以某一确定的法则有唯一确定的值与其对应，那么变量 $z$ 称为 $x$ 和 $y$ 的二元函数，记作 $z=f(x,y)$ ，其中 $x$ 与 $y$ 称为自变量，函数 $z$ 也叫作因变量，自变量 $x$ 与 $y$ 的变域 $D$ 称为函数的定义域。\n1.5 函数极限的性质数列极限的定义：给定数列 $ {x_n}$ ，实常数 $a$ ，若对任意给定的 $\\epsilon&gt;0$ ，可以找到正整数 $N$ ，使得当 $n&gt;N$ 时，$ |x_n-a|&lt;\\epsilon$ 成立，则称数列 $ {x_n}$收敛于 $a$ （或称 $a$ 是数列 $ {x_n}$ 的极限）。\n自变量趋向于无穷大时函数极限的定义： 类似数列极限的定义。\n自变量趋向有限值时函数极限的定义：设函数 $f(x)$ 在某点 $x_0$ 的某个去心邻域内有定义，后面类似数列极限的定义。\n一些懒得敲公式的知识概念：数列极限的夹逼定理、函数极限的夹逼定理、函数极限的运算法则（加减乘除幂）、无穷大量和无穷小量。\n两个公式：\n\n\\lim\\limits_{x\\rightarrow0}\\frac{\\text{sin} x}{x}=1\\\\\n\\lim\\limits_{x\\rightarrow0}(1+x)^{\\frac{1}{x}}=e1.6 洛必达法则适用于 $\\large\\frac{0}{0}$ 和 $\\large\\frac{\\infty}{\\infty}$ 及其变式。\n1.7 函数的连续性一些懒得敲公式的知识概念：函数连续性定义、最值定理、介值定理、零点存在定理（勘根定理）\n1.8 拉格朗日乘数法拉格朗日乘数法可以解决约束优化问题。其基本思想是通过引入拉格朗日乘子来将含有 $n$ 个变量和 $k$ 个约束条件的约束优化问题转化成含有 $n+k$ 个变量的无约束优化问题。是解决带等式约束优化问题的常用方法。\n详见：\n好久不见的拉格朗日乘数法：https://zhuanlan.zhihu.com/p/149104728\n如何理解拉格朗日乘子法？：https://www.zhihu.com/question/38586401\n1.9 函数间断点即不满足函数连续定义的条件。分为：\n\n第一类间断点：左右极限有定义\n跳跃间断点：左右极限不相等\n可去间断点：左右极限相等\n\n\n第二类间断点：左右极限不全有定义\n\n习题利用 $SymPy^{[1]}$ 库实现 $\\lim\\limits{a\\rightarrow 0}\\frac{sin(a)}{a}$ 和 $\\lim\\limits{n\\rightarrow\\infty}(\\frac{n+3}{n+2})^n$ 的求解。\n#coding:utf-8import sympysympy.init_printing()from sympy import oo# 1.求sin(a) / a在a=0处的极限a = sympy.Symbol(&#x27;a&#x27;)b = sympy.sin(a) / aresult = sympy.limit(b,a,0)print(&#x27;sin(a) / a在a趋近于0处的极限:&#x27;,result)#2.求[(n+3)/(n+2)]^n ,n趋紧无穷大时的极值n = sympy.Symbol(&#x27;n&#x27;)y = ((n+3)/(n+2))**nprint ( &#x27;[(n+3)/(n+2)]^n ,n趋紧无穷大的极值：&#x27;,sympy.limit(y, n, sympy.oo) )\n\n1. SymPy是一个符号计算的Python库。 它的目标是成为一个全功能的计算机代数系统，同时保持代码简洁、易于理解和扩展。 它完全由Python写成，不依赖于外部库。 SymPy支持符号计算、高精度计算、模式匹配、绘图、解方程、微积分、组合数学、离散数学、几何学、概率与统计、物理学等方面的功能。 &#8617;\n\n2 导数2.1 导数的概念有定义，有增量，极限存在。是一种变化率\n2.2 偏导数、全导数偏导数相对于多元函数，全导数相对于复合函数。\n2.3 高阶导数多次连续求导。\n2.4 函数的基础求导法则一些懒得敲公式的知识概念：常用的求导公式、和与差的求导法则、复合函数的求导规则、反函数的求导法则、隐函数的求导法则等各种求导法则。\n2.5 链式法则及复杂函数的求导假设 $z = f(u, v)$的每一个自变量都是二元函数，也就是说，$u=h(x, y)$ ，$v = g(x, y)$ ，且这些函数都是可微的。那么，$z$ 的偏导数为：\n\n\\frac {\\delta z}{dx}  =  \\frac {\\delta z}{du}  \\frac {du}{dx}  +  \\frac {\\delta z}{dv} \\frac {\\delta v}{dx} \\\\\\\n \\frac {\\delta z}{dy}= \\frac {\\delta z}{du} \\frac {\\delta u}{dy}+\\frac {\\delta z}{dv} \\frac {\\delta v}{dy}如果我们考虑 $ \\overrightarrow {r} =(u,v)$，为一个向量函数，我们可以用向量的表示法把以上的公式写成 $f$ 的梯度与 $\\overrightarrow {r}$ 的偏导数的数量积:\n\n\\frac {\\delta f}{dx}  =  \\overrightarrow {V}  f  \\cdot   \\frac {\\theta r}{dx}更一般地,对于从向量到向量的函数,求导法则为:\n\n\\frac {\\delta (z_ {1},\\cdots ,z_ {m})}{o(x_ {1},\\cdots ,x_ {p})} =  \\frac {d(z_ {1},\\cdots ,z_ {m})}{d(y_ {1},\\cdots ,y_ {n})}  \\frac {o(y_ {1},\\cdots ,y_ {n})}{o(x_ {1},\\cdots ,x_ {p})}2.6 导数的应用一些懒得敲公式的知识概念：极值与最值、曲线的凸凹、拐点（凹弧的分界点）、泰勒公式和泰勒展开式、泰勒定理、中值定理（拉格朗日、罗尔和柯西中值定理）\n3 方向导数和梯度3.1 向量一些懒得敲公式的知识概念：向量的模、数量积（内积、点积、标量积 $ |a||b|cos\\theta$）、向量积（外积、叉积，它的模是 $|a||b|sin\\theta$）\n3.2 方向导数、梯度带有方向（极值方向，一般是取最大值的方向）的导数。\n3.3 雅可比矩阵与局部最优问题详见：雅可比矩阵和雅可比行列式：https://zhuanlan.zhihu.com/p/39762178\n3.4 黑塞矩阵二阶偏导数矩阵就是黑塞矩阵（Hessian Matrix）。它是一个自变量为向量的实值函数的二阶偏导数组成的方阵。\n\n如果H(M)是 正定矩阵，则临界点M处是一个局部的极小值。\n如果H(M)是 负定矩阵，则临界点M处是一个局部的极大值。\n如果H(M)是 不定矩阵，则临界点M处不是极值。\n\n习题调用 SymPy的库函数求解出雅可比矩阵的形式，调用Theano的库函数计算雅可比矩阵和黑塞矩阵的值。\n#实例01：用SymPy库函数求解出的雅克比矩阵的形式。import sympym,n,i,j = sympy.symbols(&quot;m n i j&quot;)m = i**4-2*j**3-1 #设置变量（符号）n = j-i*j**2+5funcs = sympy.Matrix([m,n])#矩阵的维度m,nargs = sympy.Matrix([i,j])res = funcs.jacobian(args)#调用jacobian函数求解print(res)#实例02：Theano库实现雅克比矩阵的计算。import theanofrom theano import function, config, shared, sandboximport theano.tensor as T# 计算雅克比矩阵x=T.dvector(&#x27;x&#x27;)y=x**3+x**4# 调用scan构建循环GraphJ,updates=theano.scan(lambda i,y,x:T.grad(y[i],x),sequences=T.arange(y.shape[0]),non_sequences=[y,x])f1=function([x],J,updates=updates)print(&quot;f1=&quot;,f1([5, 5]))#实例03：Theano库实现海森矩阵的计算。import theanofrom theano import function, config, shared, sandboximport theano.tensor as Tx=T.dvector(&#x27;x&#x27;)# 计算hessian矩阵y=x**3+x**4cost=y.sum()gy=T.grad(cost,x) #求梯度# 调用scan构建循环GraphH,updates=theano.scan(lambda i,gy,x:T.grad(gy[i],x),sequences=T.arange(gy.shape[0]),non_sequences=[gy,x])f2=function([x],H,updates=updates)print(&quot;f2=&quot;,f2([5, 5]))\n4 积分4.1 不定积分一些懒得敲公式的知识概念：原函数（定义、存在定理、原函数族 $F(x)+C$）\n4.2 求不定积分的方法一些懒得敲公式的知识概念：积分基本公式、换元法、分部积分法\n4.3 定积分一些懒得敲公式的知识概念：定积分定义、性质（和差、与常数乘积、单调性、极值）、积分中值定理\n","categories":["机器学习数学基础"]},{"title":"机器学习数学基础：线性代数","url":"/2022/01/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/","content":"本章介绍线性代数中向量、内积、范数、矩阵、线性变换、二次型、矩阵分解等基本知识。\n1 行列式线性代数中，行列式是一个函数，和矩阵有一定的关系。几何上，行列式可以看作有向面积或体积的概念在一般的欧几里得空间中的推广。\n1.1 二阶与三阶行列式可用来解决线性方程求解的问题，同时注意对角线法则只适用于二阶和三阶的行列式。\n表达式\n\nD=a_{11}a_{22}-a_{12}a_{21}=\\left[\n \\begin{matrix}\n   a_{11} & a_{12}\\\\\n   a_{21} & a_{22} \n  \\end{matrix}\n  \\right]称为数表的二阶行列式，记作 $D=det(a_{ij})$ .\n三阶行列式定义为\n\n\\left[\n \\begin{matrix}\n   a_{11} & a_{12} & a_{13} \\\\\n   a_{21} & a_{22} & a_{23} \\\\\n   a_{31} & a_{32} & a_{33}\n  \\end{matrix}\n  \\right]=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}-a_{12}a_{21}a_{33}-a_{11}a_{23}a_{32}1.2 全排列和对换全排列的定义：由 $1$ 到 $n$ 个数组成的一个有序数组称为一个n级全排列，通常用 $ P_n$ 表示。\n\nP_n=n\\times (n-1)\\times \\dots\\times3\\times2\\times1=n!逆序数的定义：在一个排列中，如果两个数（称为数对）的前后位置与大小顺序相反，即前面的数大于后面的数，那么称它们构成一个逆序（反序）。一个排列中所有逆序的总数称为此排列的逆序数。分别计算出排在 $1,2,\\dots,n-1,n$ 前面比它大的数码之和。\n\nt=t_1+t_2+\\dots+t_n=\\sum_{i=1}^nt_i此外，还有奇排列和偶排列的概念，即奇排列为逆序数为奇数的排列。\n对换的定义：在排列中，将任意两个元素对调，其余元素不动，就得到另一个排列，这样的一个变换叫作对换。将相邻的两个元素对换，叫作相邻对换。其中，一个排列中任意两个元素对换，排列改变奇偶性。\n1.3 n阶行列式\n\\left[\n \\begin{matrix}\n   a_{11} & a_{12} & a_{13} \\\\\n   a_{21} & a_{22} & a_{23} \\\\\n   a_{31} & a_{32} & a_{33}\n  \\end{matrix}\n  \\right]=\\sum(-1)^ta_{11}a_{22}a_{33}可推广至n阶，其中 $t$ 为列标排列的逆序数。\n1.4 几种特殊行列式的值上、下三角行列式，对角行列式，特殊行列式（类似反的下三角行列式） \n如果是计算的是主对角线的值，则直接连乘；如果是副对角线，则需加一个系数 $(-1)^{\\frac{n(n-1)}{2}}$.\n一些其他的见知乎：https://zhuanlan.zhihu.com/p/34685081。\n1.5 n阶行列式的性质\n性质1：行列式与它的转置行列式相等。\n性质2：互换行列式的两列，行列式变号。（推论：两行或两列相同，此行列式等于零）。\n性质3：行列式的某一行（列）中所有元素都乘同一数 $k$，等于用数 $k$ 乘此行列式。\n性质4：行列式中如果有两行（列）元素成比例，则此行列式等于零。\n性质5：若行列式的某一列（行）的元素都是两数 之和，则该行列式等于两个行列式之和。\n性质6：把行列式的某一列（行）的各个元素乘同一个数，然后加到另一列（行）对应的元素上去，行列式的值不变。\n\n2 用向量描述空间2.1 向量及其运算$n$ 维空间用 $\\pmb{R}^n$ 表示，上标 $n$ 表示空间的维度。\n2.2 向量组的线性组合若干个同纬度的列向量（或同纬度的行向量）所组成的集合叫作向量组。\n给定向量组 $\\pmb{A}:\\pmb{a}_1,\\pmb{a}_2,\\dots,\\pmb{a}_m$ 和一组实数 $\\lambda_1,\\lambda_2,\\dots,\\lambda_m$，那么表达式 $\\lambda_1\\pmb{a}_1+\\lambda_2\\pmb{a}_2,\\dots,\\lambda_m\\pmb{a}_m$称为向量组 $\\pmb{A} $ 的一个线性组合。\n给定向量组 $\\pmb{A}:\\pmb{a}_1,\\pmb{a}_2,\\dots,\\pmb{a}_m$ 和向量 $\\pmb{b}$，如果存在一组数 $\\lambda_1,\\lambda_2,\\dots,\\lambda_m$，使得 $\\pmb{b}=\\lambda_1\\pmb{a}_1+\\lambda_2\\pmb{a}_2,\\dots,\\lambda_m\\pmb{a}_m$，则向量 $\\pmb{b}$ 是向量组 $\\pmb{A}$ 的线性组合，这时称向量 $\\pmb{b}$ 能由向量组 $\\pmb{A}$ 线性表示，也就是对应的方程组有解。\n2.3 向量组的线性相关性向量组的线性相关性的定义：给定向量组 $\\pmb{A}:\\pmb{a}_1,\\pmb{a}_2,\\dots,\\pmb{a}_m$ ，如果存在不全为0的数 $\\lambda_1,\\lambda_2,\\dots,\\lambda_m$，使得$\\lambda_1\\pmb{a}_1+\\lambda_2\\pmb{a}_2,\\dots,\\lambda_m\\pmb{a}_m=0$，则称向量组 $\\pmb{A}$ 是线性相关的。\n线性相关的充要条件是：向量组中至少有一个向量可以由其他所有向量线性表示。\n3 内积、正交向量组和范数3.1 内积向量之间的乘法可分为内积和外积，一般用内积比较多。内积的直接描述为某一向量在另一个向量方向上的投影长度。\n内积的定义：设有 $n$ 维向量 $\\pmb{x}=\\left(\\begin{matrix}   x_1 \\  x_2\\ \\vdots  \\ x_n  \\end{matrix}\\right)$，$\\pmb{y}=\\left(\\begin{matrix}   y_1 \\  y_2\\ \\vdots  \\ y_n  \\end{matrix}\\right)$，令$（\\pmb{x},\\pmb{y}）=x_1y_1+x_2y_2+\\dots+x_ny_n$，$（\\pmb{x},\\pmb{y}）$称为向量 $\\pmb{x}$ 和向量 $\\pmb{y}$ 的内积，也可以表示为 $\\pmb{x·}\\pmb{y}$.\n3.2 正交向量组和施密特正交化正交向量的定义：如果两向量的内积为零，则称它们正交。\n正交向量组的定义：如果向量组中任意两个向量都正交且不含零向量，则称为正交向量组，并且正交向量组是线性无关的。\n标准正交向量：向量组内向量彼此之间的点积为 $0$，与自身的点积为 $1$。\n施密特正交化详见：\n如何理解施密特（Schmidt）正交化：https://zhuanlan.zhihu.com/p/136627868。\n3.3 范数wiki——矩阵范数\n向量范数的定义：如果向量 $\\pmb{a} \\in \\pmb{R}^n$ 的某个实值函数 $f(\\pmb{a})=||\\pmb{a}|| $ 满足\n\n非负性：$||\\pmb{a}||\\geq 0$ ，且$||\\pmb{a}||=0$ 当且仅当 $\\pmb{a}=0$\n齐次性：对于任意实数 $\\lambda$，都有 $||\\lambda\\pmb{a}||=|\\lambda|||\\pmb{a}||$\n三角不等式：对于任意 $\\pmb{a,b}\\in\\pmb{R}^n$，都有 $||\\pmb{a+b}||\\leq||\\pmb{a}||+||\\pmb{b}||$\n\n常用的向量范数有 $ 1$ -范数、$2$ -范数和 $\\infty$ -范数。\n\n$ 1$ -范数：也称为曼哈顿距离\n\n||\\pmb{a}||_1=\\sum_i^n|\\pmb{a}_i|\n$2$ -范数：也称为欧几里得范数\n\n\n\n||\\pmb{a}||_2=\\sqrt{\\sum_{i=1}^n\\pmb{a}_i^2}\n$\\infty$ -范数：所有向量元素中的最大值\n||\\pmb{a}||_\\infty=\\max_i|a_i|\n\n矩阵范数的定义：如果矩阵 $\\pmb{A} \\in \\pmb{R}^{n\\times n}$ ，若按某一确定的法则对应于一个非负实数 $||\\pmb{A}|| $ 满足\n\n非负性：$||\\pmb{A}||\\geq 0$ ，且$||\\pmb{A}||=0$ 当且仅当 $\\pmb{A}=0$\n齐次性：对于任意实数 $\\lambda$，都有 $||\\lambda\\pmb{A}||=|\\lambda|||\\pmb{A}||$\n三角不等式：对于任意 $\\pmb{A，B}\\in\\pmb{R}^{n\\times n}$，都有 $||\\pmb{A+B}||\\leq||\\pmb{A}||+||\\pmb{B}||$\n相容性：对于任意的 $\\pmb{A，B}\\in\\pmb{R}^{n\\times n}$，都有 $||\\pmb{AB}||\\leq||\\pmb{A}||||\\pmb{B}||$\n\n常用的矩阵范数有 $ 1$ -范数、$2$ -范数、 $\\infty$ -范数和 $F$ -范数。\n\n$ 1$ -范数：又称为列和范数。顾名思义，即矩阵列向量中绝对值之和的最大值。\n\n||\\pmb{A}||_1=\\max_j\\sum_i^m|a_{ij}|\n$2$ -范数：又称为谱范数，计算方法为 $\\pmb{A^TA}$ 矩阵的最大特征值开平方。\n\n\n\n||\\pmb{A}||_2=\\sqrt{\\lambda_1}\n$\\infty$ -范数：又称为行和范数。顾名思义，即矩阵行向量中绝对值之和的最大值。\n\n||\\pmb{A}||_\\infty=\\max_i\\sum_j^n|a_{ij}|\n$F$ -范数：又称为Frobenius范数，计算方法为矩阵元素的绝对值的平方和再开方。\n\n||\\pmb{A}||_F=\\sqrt{\\sum_{i=1}^m\\sum_{j=1}^n|a_{ij}|}\n\n习题：Python编程实现求向量范数和矩阵范数。# -*- coding: UTF-8 -*-import numpy as npa=np.array([2,4,5,8,-3])print(&#x27;向量a：&#x27;,a)print (&#x27;向量a的1-范数:&#x27;)print(np.linalg.norm(a,ord=1))print (&#x27;向量a的2-范数:&#x27;)print(np.linalg.norm(a,ord=2))print (&#x27;向量a的∞-范数:&#x27;)print(np.linalg.norm(a,ord=np.inf))# -*- coding: UTF-8 -*-import numpy as npA=np.arange(3,15).reshape(3,4)print(&#x27;矩阵A：&#x27;,A)print(&#x27;矩阵A的1-范数:&#x27;)print(np.linalg.norm(A,ord=1))print(&#x27;矩阵A的2-范数:&#x27;)print(np.linalg.norm(A,ord=2))print(&#x27;矩阵的∞-范数:&#x27;)print(np.linalg.norm(A,ord=np.inf))print(&#x27;矩阵A的F-范数:&#x27;)print(np.linalg.norm(A,ord=&#x27;fro&#x27;))print(&#x27;矩阵A列向量的2-范数:&#x27;)print(np.linalg.norm(A,ord=2,axis=0))print(&#x27;矩阵A行向量的2-范数:&#x27;)print(np.linalg.norm(A,ord=2,axis=1))\n4 矩阵和线性变换4.1 矩阵及其运算一些不想敲的概念：\n\n矩阵的定义。\n特殊的矩阵：行矩阵（也叫行向量）、列矩阵（也叫列向量）、方阵（$\\pmb{A}_n$），三角矩阵、单位矩阵、对称矩阵、对角矩阵、实对称矩阵、零矩阵、正交矩阵。\n矩阵的运算：加法、数乘、乘法、转置、行列式（$|\\pmb{A}|$ 或 $det(\\pmb{A})$).\n\n4.2 逆矩阵对于 $n$ 阶矩阵 $\\pmb{A}$ ，如果有一个 $n$ 阶矩阵 $\\pmb{B}$ ，使得 $\\pmb{AB=BA=E}$，则称矩阵 $\\pmb{A}$ 是可逆的，并把矩阵 $\\pmb{B}$ 称为 $\\pmb{A}$ 的逆矩阵，记作 $\\pmb{A}^{-1}$ .\n\n若矩阵 $\\pmb{A}$ 可逆，则 $|\\pmb{A}| \\neq 0$ .\n若  $|\\pmb{A}| \\neq 0$，则矩阵 $\\pmb{A}$ 可逆，且 $\\pmb{A}^{-1}=\\frac{1}{|\\pmb{A}|}\\pmb{A}^$，其中 $\\pmb{A}^$ 为矩阵 $\\pmb{A}$ 的伴随矩阵。\n\n4.3 矩阵的初等变换\n对调两行\n数乘\n数乘再相加\n\n4.4 标量对向量的导数、最小二乘法\n详见：\n系列：机器学习中的矩阵向量求导\n单列一个：机器学习中的矩阵向量求导(二) 矩阵向量求导之定义法 \n最小二乘法及python代码：最小二乘法（least sqaure method）\n4.5 线性变换线性变换的定义：设 $\\pmb{V}$ 为一个线性空间，映射 $f:\\pmb{V}\\rightarrow\\pmb{V}$ 为 $\\pmb{V}$ 的一个变换，若 $f$ 保持 $\\pmb{V}$ 的加法与数乘运算，则称 $f:\\pmb{V}\\rightarrow\\pmb{V}$ 为 $\\pmb{V}$ 的一个线性变换。\n线性变换把一个向量空间里的向量映射到了另一个向量空间里的另一个向量，因此，我们可以把线性变换理解成输入一个向量，然后输出一个向量的特殊函数。\n它可以看成一个动态的过程，比如旋转、伸缩或者投影之类的升降维的操作。\n常见的线性变换还有：恒等变换（单位变换）、求微商（线性空间 $P[x]$ 内）、求定积分。\n详见：线性变换（一）【直观理解线性变换】\n4.6 矩阵的秩一些概念：矩阵的秩、行秩、列秩和极大无关向量组。\n定理：设 $n$ 元线性方程组 $\\pmb{Ax=b}$ ，$R(\\pmb{A})$ 表示系数矩阵 $\\pmb{A}$ 的秩，$R(\\pmb{A,b})$ 表示增广矩阵 $\\pmb{B=(A,b)}$ 的秩，则该线性方程组\n\n无解的充要条件是 : $R(\\pmb{A})&lt;R(\\pmb{A,b})$；\n有唯一解的充要条件是： $R(\\pmb{A})=R(\\pmb{A,b})=n$；\n有无限多解的充要条件是： $R(\\pmb{A})=R(\\pmb{A,b})&lt;n$ .\n\n一些概念：线性方程组的解、方阵的特征值和特征向量\n习题：Python编程实现求逆矩阵、行列式的值、秩import numpy as np# a=np.array([[1,4,7],[2,5,8],[3,6,9]])# a=np.array([[1,2,3],[4,5,6]])A=np.array([[1,4,9],[2,5,8],[3,6,9]])print(&quot;A矩阵为：&quot;)print(A)print(&#x27;*&#x27;*40)F = np.linalg.inv(A)print(&quot;A矩阵的逆矩阵为：&quot;)print(F)print(&#x27;*&#x27;*40)print(&quot;A矩阵与其逆矩阵乘积为：&quot;)print(np.dot(A,np.linalg.inv(A)))print(np.dot(A,np.linalg.inv(A)).astype(int))print(&#x27;*&#x27;*40)print(&quot;A矩阵的行列式的值为：&quot;)print(np.linalg.det(A))print(&#x27;*&#x27;*40)print(&quot;A矩阵的秩为：&quot;)print(np.linalg.matrix_rank(A))\n5 二次型5.1 二次型的定义即二次齐次函数（$x^2,x_1x_2,\\dots$，类似平方项展开加上系数）。二次型的标准型可理解为只含平方项的二次型。\n若 $\\pmb{A}$ 为对称矩阵， $\\pmb{x}=\\left(\\begin{matrix}   x_1 \\  x_2\\ \\vdots  \\ x_n  \\end{matrix}\\right)$，则 $f=\\pmb{x^TAx}$ 也是二次型的一种形式，对成矩阵 $\\pmb{A}$ 叫作二次型 $f$ 的矩阵，也把 $f$ 叫作对称矩阵 $\\pmb{A}$ 的二次型。对称矩阵 $\\pmb{A}$ 的秩就叫做二次型 $f$ 的秩。\n5.2 用正交变换化二次型为标准型\n将二次型 $f=\\sum\\limits{i=1}^n\\sum\\limits{j=1}^na_{ij}x_ix_j$，写成矩阵形式 $f=\\pmb{x^TAx}$ .\n由 $|\\pmb{A-\\lambda E}|=0$，求出 $\\pmb{A}$ 的全部特征值。\n\n由 $(\\pmb{A-\\lambda E})\\pmb{x}=0$，求出 $\\pmb{A}$ 的特征向量。\n\n\n对于求出的不同特征值所对应的特征向量已正交，只需要单位化；对于 $k$ 冲特征值 $\\lambda$ 所对应的 $k$ 个线性无关的特征向量，用施密特正交化方法把它们化成 $k$ 个两两正交的单位向量。\n\n把求出的n个两两正交的单位向量拼成正交矩阵 $\\pmb{P}$，作正交变换 $\\pmb{x=Py}$.\n\n用 $\\pmb{x=Py}$ ，把 $f$ 化成标准型 $f=\\lambda1y_1^2+\\lambda_2y_2^2+\\dots+\\lambda_ny_n^2$，其中 $\\lambda_1,\\lambda_2,\\dots,\\lambda_n$是矩阵 $\\pmb{A}=(a{ij})$ 的特征值。\n\n\n5.3 二次型的正定型正定二次型、惯性定理、和赫尔维茨定理（后两个是判定定理）。\n麻省理工线性代数笔记（二十四）-正定矩阵\n6 矩阵分解矩阵分解有很多种，如 $LU$ 分解、$QR$ 分解、特征值分解（$EVD$ ）和奇异值分解（$SVD$ ）。$LU$ 分解是将满秩矩阵分解为两个倒扣的三角形，即分解成下三角矩阵和上三角矩阵的乘积，它的意义在于求解大型方程组。QR分解经常用来解线性最小二乘法问题，它和机器学习的相关算法密切。特征值分解的方法也非常有效，但有一些局限性，即要求矩阵必须是方阵且能够被对角化。奇异值分解可以对任意形状的矩阵进行分解，实用性更广。\n麻省理工线性代数笔记（二十六）-奇异值分解\n【学长小课堂】什么是奇异值分解SVD—SVD如何分解时空矩阵\n\n\n李航统计学习之奇异值分解（SVD）\n习题：实现矩阵的QR分解import numpy as npM= np.array([[12,9,-45],[7,4,15],[6,-3,21],[6,18,5]],dtype=float)Q=np.zeros((4, 3))j = 0for a in M.T:    b = np.copy(a)    for i in range(0, j):        b = b - np.dot(np.dot(Q[:, i].T, a), Q[:, i])    e = b / np.linalg.norm(b)    Q[:, j] = e    j += 1R = np.dot(Q.T, M)np.set_printoptions(precision=3,suppress=True)print(&#x27;Gram-schmidt正交化变换结果&#x27;)print(&#x27;Q矩阵：&#x27;)print(Q)print(&#x27;R矩阵：&#x27;)print(R)print(&#x27;矩阵的乘积：&#x27;)print(np.dot(Q,R))#方法二import numpy as npnp.set_printoptions(precision=4, suppress=True)M= np.array([[12,9,-45],[7,4,15],[6,-3,21],[6,18,5]],dtype=float)s = 4t = 3Q = np.identity(s)R = np.copy(M)for j in range(s - 1):    x = R[j:, j]    E = np.zeros((4, 3))    E = np.zeros_like(x)    b = x - E    d = b / np.linalg.norm(b)    Q_j = np.identity(s)    Q_j[j:, j:] -= 2.0 * np.outer(d, d)    R = np.dot(Q_j, R)    Q = np.dot(Q, Q_j)np.set_printoptions(precision=3,suppress=True)print(&#x27;Householder变换结果&#x27;)print(&#x27;Q矩阵：&#x27;)print(Q)print(&#x27;R矩阵：&#x27;)print(R)print(&#x27;矩阵的乘积：&#x27;)print(np.dot(Q,R))\nQ：奇异值分解的应用场景有哪些?$SVD$ 的应用有很多，可以说，$SVD$ 是矩阵分解、降维、压缩、特征学习的一个基础工具，所以 $SVD$ 在机器学习领域相当的重要。\n$SVD$ 在降维中作用如何呢？通过 $SVD$ 的公式可以看出，原来矩阵 $ \\pmb{A}$ 的特征有 $n$ 维，经过 $SVD$ 后，可以用前 $r$ 个非零奇异值对应的奇异向量表示矩阵$\\pmb{A}$ 的主要特征，这样就把矩阵 $ \\pmb{A}$ 进行了降维。\n$SVD$ 在压缩中作用如何呢？机器学习最基本和最有趣的特征之一是数据压缩概念的相关性。如果我们能提取有用的数据，我们就能用更少的比特位来表达数据。从信息论的观点来看，数据之间存在相关性，则有可压缩性。通过 $SVD$ 的公式可以看出，矩阵 $ \\pmb{A}$ 经过 $SVD$ 后，要表示原来的大矩阵 $ \\pmb{A}$ ，我们只需要存储 $\\pmb{U,\\Delta,V}$ 三个较小的矩阵即可。而这 $3$ 个较小规模的矩阵占用内存上也是远远小于原有矩阵 $ \\pmb{A}$ 的，这样 $SVD$ 就起到了压缩的作用。\n$SVD$ 和主成分分析有什么关系呢？$PCA$ 即主成分分析方法，是一种使用最广泛的数据降维算法。$PCA$ 的主要思想是将n维特征映射到 $ m$ 维上，这 $m$ 维是全新的正交特征，也被称为主成分，是在原有 $n$ 维特征的基础上重新构造出来的 $m$ 维特征。$PCA$ 的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。$PCA$ 算法有两种实现方法，即基于特征值分解协方差矩阵实现 $PCA$ 算法和基于 $SVD$ 协方差矩阵实现 $PCA$ 算法。所以，$SVD$ 是 $PCA$ 算法的一种实现方法。\n我们再来看一下潜在语义索引，它是一种简单实用的主题模型。潜在语义索引是一种利用 $SVD$ 方法获得在文本中术语和概念之间关系的索引和获取方法。该方法的主要依据是在相同文章中的词语一般有类似的含义，可以从一篇文章中提取术语关系，从而建立起主要概念内容。潜在语义索引不同于 $PCA$ ，至少不是实现了 $SVD$ 就可以直接用的，但它也是一个高度依赖 $SVD$ 的算法。\n","categories":["机器学习数学基础"]},{"title":"爬虫：电子阅读权限保存电子书","url":"/2022/02/09/%E7%88%AC%E8%99%AB-%E7%94%B5%E5%AD%90%E9%98%85%E8%AF%BB%E6%9D%83%E9%99%90%E4%BF%9D%E5%AD%98%E7%94%B5%E5%AD%90%E4%B9%A6/","content":"在文泉书局上买了本《统计学习必学的十个问题》，但是只有网上的阅读权限，不是很方便做笔记。电子阅读后发现，每一页其实都是一张图片，因此可以把图片保存下来，然后再合成PDF。再HTML进行解析获得左边目录对应的书签，对PDF进行添加书签，然后一本带书签的PDF文件就下载完毕了。\n本文代码参考：\n\n文泉书局的电子书下载方法-c++\n\n文泉学堂PDF(带书签)下载原理详细讲解_python脚本实现\n\n\n问题所有的电子书内容都是以图片进行传输和显示的，那么只要能获取图片的url或者直接获取图片内容就可以把图片保存下来。但是，在图片的url请求地址中包含有一个key，不要这个key就只能获取到一张非常模糊的图片，只有key值对上了才能请求到清晰的图片，而且这个请求地址好像还是有时间限制的，估计是在key里面有个时间戳$^{[1]}$。（只有留在那一页才会有清晰的图片，否则只会有缩略图或者模糊版本，而且图片采用的是base64加密。）\n\n\n解决方案登陆因为是登陆后买的书籍，所以需要登陆（懒得写验证码等等，直接分开运行好了）。\nBEGIN_PAGE = 1url = &quot;https://wqbook.wqxuetang.com/read/pdf?bid=3224478&quot;# 图片路径，mac路径_image_path = &quot;./img/&quot;start = time.time()  # 开始时间browser = webdriver.Chrome() #这里也可以加路径，通过chrome://settings/help了解版本号，下载对应的web driver即可browser.get(url=url)  # 访问\n然后手动登陆。\n下载高清图片可以在页码框中依次输入页码1，3…（也可以改成1，2，3…），每次输入完之后保存图片再进行下一次输入。采用python的selenium模块进行模拟，然后用webdriver.Chrome()对象的page_source来获取当前页面的html文档，之后再用BeautifulSoup获取加密的图片数据，然后base64解密，获取图片的分辨率，如果$&gt;500$（判断是不是缩略图）就保存图片。\nmain-codedataQueue = queue.Queue(155)  # 初始化队列try :    time.sleep(4)    pages = 155    print(&quot;总页数为%d&quot; % pages)  # 获取书本总页数    for i in range(BEGIN_PAGE, pages, 2) :        list = input_action(browser, i, False)        download_image(list[0], i, dataQueue)        download_image(list[1], i + 1, dataQueue)        print(&quot;下载进度: %f%%   \\r&quot; % ((i / pages) * 100))    if pages % 2 == 1 :  # 如果是奇数的话，最后一页没有下载        text = input_action(browser, pages, True)        download_image(text, pages, dataQueue)    while not dataQueue.empty() :        index = dataQueue.get(False)        b_text = input_action(browser, index, True)        download_image(b_text, index, dataQueue)    end = time.time()    print(&quot;-------------下载图片完毕-------------&quot;)\ndef-code函数：get_image_size，delay，input_action，download_image .\ndef get_image_size(content) :    &quot;&quot;&quot;    :param content: 图片的二进制数据    :return: 图片的宽高    &quot;&quot;&quot;    tmpIm = BytesIO(content)    im = Image.open(tmpIm)    w = im.size[0]  # 宽    h = im.size[1]  # 高    return w, h  def delay(browser, page, is_last) :    &quot;&quot;&quot;    此函数非常关键，它等待两个页面全部加载完毕    原理是根据此页面的图片是否高清    输入页码之后，一次加载的是2页，比如输入3，加载的是3,4两页的图片    :param browser:webdriver.Chrome()    :param page:页数    :param is_last: 是否是最后一页    :return:图片字节流    &quot;&quot;&quot;    start_tm = datetime.datetime.now()    while True :        try :            # 如果长时间没反应就刷新当前页面，并且把当前下载页面输入进去            end_tm = datetime.datetime.now()            if (end_tm - start_tm).seconds &gt; 8 :                start_tm = datetime.datetime.now()                browser.refresh()                time.sleep(1)                input_ = browser.find_element_by_id(&#x27;input&#x27;)                input_.clear()                input_.send_keys(&#x27;1&#x27;)                time.sleep(0.5)                input_.clear()                input_.send_keys(str(page))                input_.send_keys(Keys.ENTER)            html = browser.page_source            soup = BeautifulSoup(html, &quot;html.parser&quot;)            help_contents = soup.find_all(&#x27;div&#x27;, class_=&quot;page-img-box&quot;)            text1 = help_contents[page].find(&#x27;img&#x27;)[&#x27;src&#x27;]            index1 = text1.find(&quot;,&quot;)            img_byte1 = base64.b64decode(text1[index1 + 1 :].encode(&quot;utf-8&quot;))            num1, tmp = get_image_size(img_byte1)            if not is_last :  # 如果不是最后一个                text2 = help_contents[page + 1].find(&#x27;img&#x27;)[&#x27;src&#x27;]                index2 = text2.find(&quot;,&quot;)                img_byte2 = base64.b64decode(text2[index2 + 1 :].encode(&quot;utf-8&quot;))                num2, tmp = get_image_size(img_byte2)                if num1 &gt; 500 and num2 &gt; 500 :                    return [img_byte1, img_byte2]                time.sleep(0.5)            else :  # 如果是最后一个                if num1 &gt; 500 :                    return img_byte1        except :            passdef input_action(browser, page_num, is_last) :    &quot;&quot;&quot;    开始模拟自动输入页码，加载    &quot;&quot;&quot;    input = browser.find_element_by_id(&#x27;input&#x27;)    input.clear()    input.send_keys(str(page_num))    input.send_keys(Keys.ENTER)    # 延迟，等待页面加载完毕    _list = delay(browser, page_num, is_last)    return _listdef download_image(text, page, dataQueue) :    &quot;&quot;&quot;    :param text: 图片字节流    :param page: 页码，防止保存失败的时候，加入到dataQueue中方便最后重新下载    :param dataQueue:    :return:    &quot;&quot;&quot;    try :        byte_text = text        img_name = str(page) + &quot;.jpeg&quot;        with open(_image_path + img_name,&#x27;wb&#x27;) as f:            f.write(byte_text)    except :        print(&quot;第%d页下载失败&quot; % page)        print(_image_path+img_name)        dataQueue.put(page)\n下载书签main-codebookmark = download_bookmark(bookmark_action(browser))  # 打开目录，准备下载书签print(&quot;下载书签完毕&quot;)\ndef-code函数：bookmark_action，download_bookmark .\ndef bookmark_action(browser) :    &quot;&quot;&quot;    开始模拟打开所有目录(经过测试，只有打开所有目录之后，子书签才会出现在html文档中)    :param browser:    :return:返回包含子书签的html文档，以便后期用BeautifulSoup获取保存书签    &quot;&quot;&quot;    mulu = browser.find_element_by_css_selector(&quot;[class=&#x27;iconfont2 icon-wq-catalog&#x27;]&quot;)    mulu.click()    # 打开所有的三角形    sjxs = browser.find_elements_by_css_selector(&quot;[class=&#x27;el-tree-node__expand-icon el-icon-caret-right&#x27;]&quot;)    for sjx in sjxs :        sjx.click()        time.sleep(0.5)    time.sleep(2)  # 等待2秒，开始    html = browser.page_source.encode(&quot;utf-8&quot;)    return html  def download_bookmark(html) :    &quot;&quot;&quot;    爬取书签保存为列表,格式如下    无子书签： [name, num, 0]    有子书签: [name, num, [[child_name1, child_num1],[child_name2, child_num2]...... ]]    :param html: html字符串    :return:返回书签列表    &quot;&quot;&quot;    soup = BeautifulSoup(html, &quot;html.parser&quot;)    help_contents = soup.find(&#x27;div&#x27;, role=&quot;tree&quot;)    xs = help_contents.contents    parent_tag_list = []    for i in range(len(xs)) :  # 筛选出父节点        try :            if xs[i][&#x27;role&#x27;] == &quot;treeitem&quot; :                parent_tag_list.append(xs[i])        except :            pass    bookmark = []    for i in range(len(parent_tag_list)) :        temp_bookmark = [0, 0, 0]  # 名字，序号，子节点        temp_bookmark[0] = parent_tag_list[i].find(&quot;div&quot;, class_=&quot;el-tree-node__content&quot;).find(&quot;span&quot;,class_=&quot;BookCatTree-node-left&quot;).string.replace(&quot;\\n&quot;, &quot;&quot;)        temp_bookmark[1] = parent_tag_list[i].find(&quot;div&quot;, class_=&quot;el-tree-node__content&quot;).find(&quot;span&quot;,class_=&quot;BookCatTree-node-pagenum&quot;).string.replace(&quot;\\n&quot;, &quot;&quot;).replace(&quot; &quot;, &quot;&quot;)        a = parent_tag_list[i].find(&quot;div&quot;, class_=&quot;el-tree-node__children&quot;)        if a :  # 如果存在子节点            tag_children = a.find_all(&#x27;div&#x27;, role=&quot;treeitem&quot;)            children_bookmark = []            for i in range(len(tag_children)) :                child_bookmark = [0, 0]  # 名字，序号                child_bookmark[0] = tag_children[i].find(&quot;div&quot;, class_=&quot;el-tree-node__content&quot;).find(&quot;span&quot;,class_=&quot;BookCatTree-node-left&quot;).string.replace(&quot;\\n&quot;, &quot;&quot;)                child_bookmark[1] = tag_children[i].find(&quot;div&quot;, class_=&quot;el-tree-node__content&quot;).find(&quot;span&quot;,class_=&quot;BookCatTree-node-pagenum&quot;).string.replace(&quot;\\n&quot;, &quot;&quot;).replace(&quot; &quot;, &quot;&quot;)                children_bookmark.append(child_bookmark)            temp_bookmark[2] = children_bookmark        else :            temp_bookmark[2] = 0        bookmark.append(temp_bookmark)    return bookmark\n合成pdfmain-codepdf_name = &quot;统计学习必学的十个问题&quot;img_path = _image_pathpdf_path = pdf_name + &quot;.pdf&quot;convert_images_to_pdf(img_path, pdf_path, get_size(), pages)\ndef-codedef get_size() :    &quot;&quot;&quot;    获取图片的宽高，来计算比例，用来合成PDF    :return:PDF的尺寸    &quot;&quot;&quot;    with open(_image_path + &quot;1.jpeg&quot;, &quot;rb&quot;) as f :        A4_MY = [0, 297 * mm]        f.seek(16)        w = bytes_to_long(f.read(4))        h = bytes_to_long(f.read(4))    bili = h / w    A4_MY[0] = (297 // bili) * mm    return A4_MYdef convert_images_to_pdf(img_path, pdf_path, size, pdf_count) :    &quot;&quot;&quot;    将图片合成为PDF    :param img_path:图片文件夹路径    :param pdf_path:PDF文件路径(包含PDF文件名)    :param size: PDF的尺寸    :param pdf_count: PDF的总页数    :return:    &quot;&quot;&quot;    pages = 0    (w, h) = size    c = canvas.Canvas(pdf_path, pagesize=portrait((w, h)))    l = os.listdir(img_path)    l.sort(key=lambda x : int(x[:-5]))      # 因为后缀都是.jpeg，所以是-5，对图片进行排序    for i in l :        f = img_path + os.sep + str(i)        c.drawImage(f, 0, 0, w, h)        c.showPage()        pages = pages + 1        print(&quot;添加进度: %f%%   \\r&quot; % ((pages / pdf_count) * 100))    c.save()\n添加书签main-codeadd_bookmark(bookmark, pdf_path)print(&quot;下载完成，时间:%.2f秒&quot; % (end - start))\ndef-codedef add_bookmark(bookmark, pdf_path) :    &quot;&quot;&quot;    书签的列表格式    无子书签： [name, num, 0]    有子书签: [name, num, [[child_name1, child_num1],[child_name2, child_num2]...... ]]    根据bookmark列表来对PDF文件添加书签    :param bookmark:    :return:    &quot;&quot;&quot;    # 读取PDF文件，创建PdfFileReader对象    book = PdfFileReader(pdf_path)    # 创建PdfFileWriter对象，并用拷贝reader对象进行初始化    pdf = PdfFileWriter()    pdf.cloneDocumentFromReader(book)    # 添加书签    # 注意：页数是从0开始的，中文要用unicode字符串，否则会出现乱码    # 如果这里的页码超过文档的最大页数，会报IndexError异常    for i in range(len(bookmark)) :        parent = pdf.addBookmark(bookmark[i][0], int(bookmark[i][1]) - 1)        if bookmark[i][2] != 0 :  # 如果有子书签            for j in range(len(bookmark[i][2])) :                pdf.addBookmark(bookmark[i][2][j][0], int(bookmark[i][2][j][1]) - 1, parent=parent)    # 保存修改后的PDF文件内容到文件中    # 注意：这里必须用二进制的&#x27;wb&#x27;模式来写文件，否则写到文件中的内容都为乱码    with open(pdf_path, &#x27;wb&#x27;) as fout :        pdf.write(fout)\n运行登陆模块后的main代码dataQueue = queue.Queue(155)  # 初始化队列try :    time.sleep(4)    pages = 155    print(&quot;总页数为%d&quot; % pages)  # 获取书本总页数    for i in range(BEGIN_PAGE, pages, 2) :        list = input_action(browser, i, False)        download_image(list[0], i, dataQueue)        download_image(list[1], i + 1, dataQueue)        print(&quot;下载进度: %f%%   \\r&quot; % ((i / pages) * 100))    if pages % 2 == 1 :  # 如果是奇数的话，最后一页没有下载        text = input_action(browser, pages, True)        download_image(text, pages, dataQueue)    while not dataQueue.empty() :        index = dataQueue.get(False)        b_text = input_action(browser, index, True)        download_image(b_text, index, dataQueue)    end = time.time()    print(&quot;-------------下载图片完毕-------------&quot;)    # 开始获取尺寸    bookmark = download_bookmark(bookmark_action(browser))  # 打开目录，准备下载书签    print(&quot;下载书签完毕&quot;)    pdf_name = &quot;统计学习必学的十个问题&quot;    img_path = _image_path    pdf_path = pdf_name + &quot;.pdf&quot;    convert_images_to_pdf(img_path, pdf_path, get_size(), pages)    # 添加书签    add_bookmark(bookmark, pdf_path)    print(&quot;下载完成，时间:%.2f秒&quot; % (end - start))finally :    browser.close()\n","categories":["爬虫"]},{"title":"如何做文献阅读汇报？——改自知乎答案","url":"/2022/02/14/%E7%A0%94%E7%A9%B6%E5%88%9D%E6%88%90%E9%95%BF-%E5%A6%82%E4%BD%95%E5%81%9A%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%B1%87%E6%8A%A5%EF%BC%9F/","content":"青藤学术来源：知乎答案\n一篇文章又分成研究型文献和综述性文献。\n研究型文献针对一篇研究型文献，那么我们需要向读者讲清这篇文献的故事：\n\n这篇文献讲了个什么问题， 他的假设是什么？\n\n作者为什么要提出这个假设 （研究背景，他不可能凭空提出来做这个）？\n\n他证明了这个假设吗（大概率是的）？\n\n他怎么证明的（数据来源是什么，做了几组实验，什么被试，使用了什么器材，什么试剂，为什么要做几组实验，分别证明了什么）？\n\n作者的结论是什么？\n\n他还对研究提出了展望吗？他有没有声明研究的一些缺憾？\n\n最重要的： 你的看法（这就是 critical thinking了）， 具体可以包括：\n\n\n你同意作者提出的这个假设吗，有其他的文献支持或者反对作者的假设吗，为什么？\n作者的实验（数据）证明了假设吗，为什么？\n你同意作者的结论吗？为什么？\n你觉得可以在作者的基础上，有拓展空间吗？作者提出的展望值得我们研究吗？\n\n\n\n综述性文献综述文献和研究型文献有相似的地方，也有不同的地方。 相同的地方，文献都是讲一个故事，不同的地方是，综述里不涉及作者新的证明（实验），但是他会用已有的文献找到一个好的角度，这个角度是最重要的。\n我们对综述文献汇报：\n\n作者为什么要写这篇综述？\n\n这篇综述提出了一个什么问题或者几个问题？\n\n针对每个问题，作者是用哪些文献来佐证他的观点的？有没有反对的文献？\n\n接下来，你的想法：\n\n\n读完这些综述，你学习到了一些什么领域的知识？\n作者列出的这么多文献里，你觉得那些值得继续阅读？为什么？\n你觉得读完这篇综述，从作者提出的这些问题，你有启发吗？\n作者所列举的问题，你觉得值得拓展研究吗？\n\n\n\n综述一般是用来学习的，所以从汇报者的角度来说，如果能 Get 到能学习哪些东西，能继续阅读哪些文献，有没有启发，有没有一些 idea， 那就实现了目的。 \n多篇文献第一种方法，如果文献不是很多，比如1-3篇，可以逐篇汇报，没有毛病，不扣分。当然控制时间，不要一个人讲了一个小时，其他人听得要睡觉。 \n第二种方法，文献数量比较多，那么就要串一下，类似做个小综述，这就需要点本事。你得找到一个串的东西，我可以举一些例子，你大概会明白怎么串：\n​    1） 这几个文献用了同一个研究方法，比如问卷，你发现他们其实挺难有因果关系；\n​    2） 他们都用了相同的仪器，然后你觉得这个仪器你们也可以用，可以怎么在你们的实验中发挥重大的作用；\n​    3） 他们都讨论了同一个问题，要么是时间先后顺序，要么是一个问题的不同角度，得出了不同的结论，这很有趣，我们也可以Follow。\n第二种方法需要的时间比较多一些，抽象一些，如果时间有，可以多试试看，毕竟你都读了好几篇。 \n以上说的都是内容，汇报不仅包括内容，还包括内容的形式，以及报告本身（你的表演）。我稍微点一下，仁者见仁，这个需要本事。 \n关于PPT写 PPT 的套路就很多了，但有一些guideline 和你share 一下：\n\n能用图就不要用文字；\n文字要少，不要和有些老师上课用的 PPT 那样，全部是字 … \n列要点，说要点，不要遗漏即可。要点简要到你自己能想起来你要说啥就可以。\n风格不要洗剪吹，简洁，清爽，切记，看的人舒服。\n求你们英文不要用宋体，不要全角，不要用中文标点。反之一样，中文不要用英文标点。\n不要有拼写错误（现在 PPT 都有拼写检查），领域里的专业名词和缩小多检查几遍，不要出错。\n尽量不要有语法错误，找个英语好的同学帮忙 Check一下。\n不要最后一个晚上赶工，提前做准备。\n多检查几遍，会发现很多错误。\n\n\n这篇文章的目的是什么？研究的核心是什么？核心的结论有哪些？\n\n把文章的结论进行一个简要的翻译，起到最终收尾的作用\n\n\n车前草来源：知乎答案\n每个session一般有4-5篇必读文献，3篇左右选读文献。一般课上5-6人，一个人当leader，其他人都是discussant。leader是每周轮流换的。leader需要读全部的论文，discussant读必读的就可以。我们一般是三小时的课，每篇文献大概能讨论半小时到五十分钟。\nLeader通常会做PPT。内容不多，七八页左右就可以了。每页内容也很简单，大致包含以下标题：\n\nResearch Question(s). 本文研究了什么？解答了什么样的问题？\nTheoretical argument. 本文的主要argument是什么？具体说来，可以包括以下几个小的方面：a. 使用了什么理论作为依托。也就是说本文的论点是建立在什么理论上的。b. 对哪个理论/领域做出了贡献（因为使用的理论不一定要和做出贡献的理论一致。）c. 理论的推导是如何完成的。对于实证论文来说，就是自变量到因变量的逻辑是什么样的。d. argument的边界条件是什么。本文所提出的观点（即hypothesis）在什么样的条件下是立得住脚的。\nEmpirics. 本文使用了什么数据？数据是从哪里来的？取得数据的行业大致是什么样的情况？为什么这个数据有用？在方法上，为什么采用这种回归模型？为什么用零膨胀负二项回归（ZINB）而不是泊松回归（Poisson）？\nComments. 即你作为评论人对于这篇文章是怎么看的。好在哪里？差在哪里？\n\n我们老师在领域里有一定地位，提问题特别辛辣。他一般不喜欢学生在1和3方面集中太多精力。而是喜欢在2和4里挑战（challenge）学生。在讨论问题2时，他不喜欢学生把PPT里的东西念出来，而是希望你回答 “为什么这篇文章要用A理论不用B理论？”“这篇文章跟我们以前读过X文章有什么联系？”“这篇文章是如何和讨论C理论的Y、Z文章联系起来的？” 之类的问题。在回答问题4时，他希望你有建设性的批评（constructive critique）而不是光批评不提解决办法（criticizing）。总之，上文献讨论课会很辛苦。尤其对于没有参与过西方式课堂讨论的中国学生来说，第一年是一个大挑战。\n\n接下来说说为啥你觉得没什么好说的。这种事情非常正常。刚开始接触文献的时候会自然而然地把接收到的信息当做真理，大牛都已经做过了，好像都挺对，没啥意见…… 其实看多了文章之后就会知道，其实每篇文章都有或多或少的毛病。区别只在于毛病是否有损文章在领域内的贡献罢了。为了解决这个问题，还是要多看、多写。看完文章一定要写summary，research question写一段就够了，重点是constructive critique(s)。文章看多了，每篇文章在整个领域内的位置就清晰了。\n有一些速成问题可以思考的：\n理论方面：\n\n理论的假设（assumption）是什么？适用范围如何？对于本文的行业背景到底合适吗？为何本文讨论的事情不用A理论非要用B理论？\n理论的边界条件（boundary condition）是啥？假设是不是所有情况下一定成立的？\n\n实证方面：\n\n有没有遗漏变量、互为因果、测量误差等内生性问题？\n模型设定对吗？估计方法合适吗？\n假设有没有其他解释（alternative explanation）？排除了其他逻辑的干扰了吗？\n结果稳健吗？有没有P-hacking？\n\n\n最后说说汇报时的小技巧。我自己认为，写在slides上的东西其实并不重要，关键是观众听进去了多少。我一般会避免在slides上堆太多文字，一页五行字，一些关键词就差不多了。千万不要念PPT。听众期待的是脑力上的激荡，而不是一台复读机。为了激起观众的兴趣，在讲述过程中可以适当抛几个问题。（并不一定要点人回答，只要看到听众在想问题就可以了。）尽量跟听众有眼神接触，同时辅之以少量手势。不独中国人，东亚学生在做presentation上都不如北美学生熟练自信。掌握了这些讲述技巧，听众也容易弄懂你的内容。\nPPT 目录结构\nThe Summary of Wesley D. Sine &amp; Brandon H. Lee 2009.3 Tilting at Windmills? The Environmental Movement and the Emergence of the U.S. Wind Energy Sector Administrative Science Quarterly, 54(2009): 123-155\n\nExecutive Summary\n\nAbstract\n\n\nResearch Question\n\nGeneral Research Question\n\nHow …..?\n\n\nSpecific Research Question\n\nHow can …. ?\n\n\n\n\nTheory\n\nKey Theoretical Arguments\n\n\nEmpirics\n\nResearch Context : U.S. Wind energy sector\nSampling : 1978-1992, state-level data on entrepreneurial activity, environmental social movement organizations, and the regulatory environment in the U.S. wind energy sector.\n\nDependent Variable: \nExplanatory Variables: \nControl Variables: \n\n\nMethod:\n\nEvent history methods …. \n\n\nFindings:\n\n\nComments\n\nImportant Citations\nCitations that have direct theoretical impact to the focal paper\n\n\n\n","categories":["研究初成长"]},{"title":"流程图与伪代码","url":"/2022/02/11/%E7%BC%96%E7%A8%8B%E6%80%9D%E7%BB%B4-%E6%B5%81%E7%A8%8B%E5%9B%BE%E4%B8%8E%E4%BC%AA%E4%BB%A3%E7%A0%81/","content":"流程图 FlowchartWhat is flowchart?A flowchart is a schematic representation of an algorithm or process that tell how to complete a task.\n程序流程图又称程序框图，是用统一规定的标准符号描述程序运行具体步骤的图形表示。\nHow to use flowchart?——symbol\n\n\n\nHow to use flowchart?——structure1）顺序结构\n这种结构最简单，各个步骤是按先后顺序执行的。如图，A、B、C是三个连续的步骤，它们是按顺序执行的，即完成上一个框中指定的操作才能再执行下一个动作。\n\n\n2） 选择结构\n选择结构又称分支结构，用于判断给定的条件，根据判断的结果判断某些条件，根据判断的结果来控制程序的流程。\n\n\n3）循环结构\n循环结构又称为重复结构，指在程序中需要反复执行某个功能而设置的一种程序结构。它由循环体中的条件，判断继续执行某个功能还是退出循环。\n根据判断条件，循环结构又可细分为以下两种形式：先判断后执行的循环结构（当型结构），和先执行后判断的循环结构（直到型结构）。\n\n\nHow to use flowchart?——path/arrow除了符号规划、结构规划，绘制流程图过程中还要注意一些约定俗成的路径规划，比如\n1）绘制流程图时，为了提高流程图的逻辑性，应遵循从左到右、从上到下的顺序排列。\n2）一个流程从开始符开始，以结束符结束。开始符号只能出现一次，而结束符号可出现多次。若流程足够清晰，可省略开始、结束符号。\n3）同一流程图内，符号大小需要保持一致，同时连接线不能交叉，连接线不能无故弯曲。\n4）流程处理关系为并行关系的，需要将流程放在同一高度。\n5）处理流程须以单一入口和单一出口绘制，同一路径的指示箭头应只有一个。\n\n\nAnnotation\n\nParallel\n\nExample\n\n制作软件\n\nvisio\nprocesson\n\n伪代码 PseudocodeWhat is Pseudocode?Pseudocode is a simpler version of a programming code in plain English before it is implemented in a specific programming language. Pseudocode is often referred to as a syntactical representation of a program and it doesn’t have a strict syntax since it only represents the way we’re thinking.\n\n伪代码是在以特定编程语言实现之前的简单英语编程代码的更简单版本。 伪代码通常被称为程序的语法表示，它没有严格的语法，因为它只表示我们的思维方式。 \n\nWhy use pseudocode?Pseudocode abstracts away syntax to let you focus on solving the problem in front of you. So instead of getting bogged down in the exact syntax of a language, pseudocode allows you to work almost impure programming logic. This way you don’t actually have to know what exact built-in functions a programming language provides, you can simply write down in plain English what it is you’re trying to do.\n\n伪代码抽象出语法，让您专注于解决您面前的问题。 因此，伪代码可以让您使用几乎不纯的编程逻辑，而不是陷入语言的确切语法中。 这样，您实际上不必知道编程语言提供了哪些确切的内置函数。 您可以简单地用日常的英语写下您想要做什么。\n\nIt essentially helps us break down large problems into smaller manageable pieces through this. Running pseudocode allows you to think through a problem with some foresight and lets you anticipate important questions before they arise. It is actually saving some time and headaches along the way and breaking things down with pure programming logic, and then googling or researching what you need to look for will essentially help you in becoming a better programmer.\n\n它本质上帮助我们把大问题分解成更小的可管理的部分。 运行伪代码可以让您有远见地思考问题，并在重要问题出现之前预测它们。 它实际上节省了一些时间和痛苦，并用纯编程逻辑分解事情，然后谷歌搜索或研究你需要寻找的内容将帮助你成为一个更好的程序员。\n\nHow to write pseudocode?\nCapitalize key commands (IF numbers is &gt; 10 THEN) $\\rightarrow$​ create code blocks and actually spots some of the logic\nWrite one statement per line\nUse indentation keep $\\rightarrow$ different components of your pseudocode isolated\nBe specific\nKeep it simple $\\rightarrow$ even for a layman or a client\n\nExampleWrite a program that prints the number from 1 to 20 .For multiples of three print “Fizz” instead of the number.For the multples of five print “Buzz” instead of the number.For numbers which are multiples of both three and five print “FizzBuzz”.For numbers not divisible by 3, or 5, or both, print the number as is.\nFOR LOOP:Set counter to 1Break when counter reaches 20Increment counter by 1\tIF number MOD 15  == 0\t\tprint &#x27;FizzBuzz&#x27;\tELSE IF number MOD 3 == 0\t\tprint &#x27;Fizz&#x27;\tELSE IF number MOD 5 == 0\t\tprint&#x27;Buzz&#x27;\tELSE\t\tprint number\nAnother version规则：\n\n在伪代码中，每一条指令占一行(else if 例外)，指令后不跟任何符号；\n“缩进”表示程序中的分支程序结构（同一模块的语句有相同的缩进量，次一级模块的语句相对与其父级模块的语句缩进）；\n通常每个算法开始时都要描述它的输入和输出，而且算法中的每一行都给编上行号，在解释算法的过程中会经常使用算法步骤中的行号来指代算法的步骤；\n每一行可以加上编号（也可不加）。\n\n1.变量的声明\n算法中出现的数组、变量可以是以下类型：整数、实数、字符、字符串或指针。定义变量的语句不用写出来，但必须在注释中给出。\n2.指令的表示\n在算法中的某些指令或子任务可以用文字来叙述，例如，”设x是A中的最大项”，这里A是一个数组；或者”将x插入L中”，这里L是一个链表。这样做的目的是为了避免因那些与主要问题无关的细节使算法本身杂乱无章。\n3.表达式\n算术表达式可以使用通常的算术运算符（+，-，*，/，以及表示幂的^）。逻辑表达式可以使用关系运算符 = 、≠、&lt;、&gt;、≤ 和 ≥，以及逻辑运算符与(and)、或（or）、非（not）。\n4.赋值语句\n赋值语句是如下形式的语句：a←b。这里a是变量、数组项，b是算术表达式、逻辑表达式或指针表达式。语句的含义是将b的值赋给a。\n变量交换：若a和b都是变量、数组项，那么记号a&lt;-&gt;b 表示a和b的内容进行交换。\n5.goto语句\ngoto语句具有形式：\ngoto label（goto标号）\n它将导致转向具有指定标号的语句。\n6.分支结构\n条件语句：\nif i=10    then xxxx    else xxxx //else 和 then 要对齐       //或者if i=10    then xxxx //if 后面必定跟上then，else后面不用跟then    else if i=9 //elseif 要连在一起写        then xxxx        yyyy    else  xxxx //else 跟在 elseif 的 then 对齐\n7.循环结构\n有两种循环指令：while和for。\nwhile语句的形式是：\nwhile time&lt;10    do  xxxxx //while后面必定要紧跟缩进的do    xxxxx    end\nfor语句的形式是：\nfor var init to limit by incr \tdo send\n这里var是变量，init、limit和incr都是算术表达式，而s是由一个或多个语句组成的语句串。初始时，var被赋予init的值。假若incr≥0，则只要var≤limit，就执行s并且将incr加到var上。（假若incr&lt;0，则只要var≥limit，就执行s并且将incr加到var上）。incr的符号不能由s来该改变。\n8.程序的结束\nexit语句可以在通常的结束条件满足之前，被用来结束while循环或者for循环的执行。exit导致转向到紧接在包含exit的（最内层）while或者for循环后面的一个语句。\nreturn用来指出一个算法执行的终点；如果算法在最后一条指令之后结束，它通常是被省略的；它被用得最多的场合是检测到不合需要的条件时。return的后面可以紧接被括在引号的信息。\n9.注释风格\n算法中的注释被括在 / / 之中。\n10.函数的编写\n函数的伪代码格式例子为：search（A，name）， 参数类型可以不给出，但必须在注释中说明。\nExample\n//伪代码x ← 0y ← 0z ← 0while x &lt; Ndo x ← x + 1y ← x + yfor t ← 0 to 10do z ← ( z + x * y ) / 100repeaty ← y + 1z ← z - yuntil z &lt; 0z ← x * yy ← y / 2  //代码x = y = z = 0;while( z &lt; N )&#123;   x ++;　　y += x;　　for( t = 0; t &lt; 10; t++ )　　&#123;　　     z = ( z + x * y ) / 100;　　     do 　　     &#123;\t\t\ty ++;\t\t\tz -= y;　　     &#125; while( z &gt;= 0 );    &#125;　　z = x * y;&#125;y /= 2;\n参考自\nYoutube：Introduction to Creating Flowcharts\n\n专栏：知乎专栏：流程图绘制\n\nYouTube: What is pseudocode and how do you use it?\n\n博客：学会写伪代码\n\n博客：科研基础3-伪代码规范-latex\n\n\n","categories":["编程思维"]},{"url":"/2022/07/28/%E8%BF%90%E8%A1%8C%E5%AE%89%E8%A3%85%E2%80%94%E2%80%94%E4%B8%80%E4%BA%9B%E6%8A%A5%E9%94%99%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","content":"AttributeError: module ‘pymc3’ has no attribute ‘diagnostics’pip install pymc==3.7\nAttributeError: ‘int’ object has no attribute ‘cuda’PyTorch报CUDA error: no kernel image is available for execution on the device 错误安装合适的pytorch版本，推荐its版本，详见: Linux服务器安装pytorch &amp; 踩坑记录\nERROR: Failed building wheel for box2d-pypip3 install cmakesudo apt-get install swig\nERROR: Failed building wheel for mpi4pysudo apt-get updatesudo apt install libopenmpi-devpip install mpi4py==3.0\n","categories":["运行安装"]},{"url":"/2022/07/28/%E8%BF%90%E8%A1%8C%E5%AE%89%E8%A3%85%E2%80%94%E2%80%94%E8%BF%90%E8%A1%8Cshell/","content":"macos# 进入目录./name.sh run\nwindows安装git：Git - Downloading Package。(具体见 Git安装配置教程\n添加环境变量：路径下的bin文件夹。(有sh运行文件\nsh xxx.sh\n来运行Shell脚本。\n","categories":["运行安装"]},{"url":"/2022/07/28/%E8%BF%90%E8%A1%8C%E5%AE%89%E8%A3%85%E2%80%94%E2%80%94%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83py27%E5%8F%8A%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","content":"基于anaconda安装。\npython2.7环境安装jupyter notebook\n尝试一：\npython2.7环境安装jupyter notebook ，官方给的代码安装报错Command “python setup.py egg_info” failed with error code 1 in /tmp/pip-install-pjB8H9/ipython/\n尝试二：\npip install jupyter notebook, 安装成功，但启动jupyter notebook时报错UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe5 in position 4: ordinal not in range(128)\n\n多方搜罗信息，以上安装报错是因为新版的ipython只支持python3.4以上版本，python2.7的jupyter需配置如下版本。\n#=====新建python2.7环境conda create -n my_name python=2.7  # 创建conda环境source activate my_name  # 激活conda环境#=====python2.7环境安装jupyter notebook which jupyter  # 默认jupyter路径，通常是主conda环境下的# 新版的ipython只支持python3.4以上版本，python2.7的jupyter需配置如下版本pip install ipython==5.5.0pip install ipykernel==4.8.2which jupyter  # my_name环境下的jupyter#=====保证my_name环境下的package在python可以引用到import syssys.path  # 如果没有my_name下site-packages路径，则手动加入sys.path.append(&#x27;/root/anaconda3/envs/my_name/lib/python2.7/site-packages&#x27;)#=====启动jupyter#nohup jupyter notebook --no-browser --port=3434 --ip=192.168.1.230 --allow-root --notebook-dir=&#x27;/&#x27;\nJupyter notebook 增加Python2.7因为是为了Jupyter强大的编辑可读特性，所以不在乎性能内存问题，怎么方便怎么来，直接装装装。安装完成后运行：\nconda create -n py27 python=2.7 ipykernelconda create -n py36 python=3.6 ipykernel\n这里有可能会出现 Command conda not found的情况，是因为没有设置环境变量，设置一下就好了：\nexport PATH=&quot;~/anaconda2/bin:$PATH&quot;\n但是我在完成这些工作之后打开Jupyter notebook仍然只有python3，最后再加上以下命令重新安装一下需要的核就好了：\npython2.7 -m pip install ipykernelpython2.7 -m ipykernel install --user\n在PyCharm环境中使用Jupyter Notebook的两种方法（需注意上述版本问题）\nPython 2.7中安装pip的方法及步骤1､安装setup-tools\n下载地址：https://pypi.python.org/pypi/setuptools\n下载安装包，可以使用wget命令下载。下载及安装命令如下，\nwget https://pypi.python.org/packages/45/29/8814bf414e7cd1031e1a3c8a4169218376e284ea2553cc0822a6ea1c2d78/setuptools-36.6.0.zip#md5=74663b15117d9a2cc5295d76011e6fd1unzip setuptools-36.6.0.zip cd setuptools-36.6.0python setup.py install\n2､Python2安装pip\n下载地址：https://pypi.python.org/pypi/pip\n可以使用wget命令下载。下载及安装命令如下，\nwget https://pypi.python.org/packages/11/b6/abcb525026a4be042b486df43905d6893fb04f05aac21c32c638e939e447/pip-9.0.1.tar.gz#md5=35f01da33009719497f01a4ba69d63c9tar -zxvf pip-9.0.1.tar.gzcd pip-9.0.1python setup.py install\n3､创建pip软链接\n如果上面执行完成，可以正常使用pip命令，也可以不执行下面命令创建软链接。\n进入到/usr/local/python27/bin目录，如果上面安装没报错的话，就可以看到easy_install和pip命令，创建命令如下，\nrm -rf /usr/bin/easy_install* /usr/bin/pipln -s /usr/local/python27/bin/pip2.7 /usr/bin/pipln -s /usr/local/python27/bin/pip2.7 /usr/bin/pip27ln -s /usr/local/python27/bin/pip2.7 /usr/bin/pip2.7ln -s /usr/local/python27/bin/easy_install /usr/bin/easy_installln -s /usr/local/python27/bin/easy_install /usr/bin/easy_install27ln -s /usr/local/python27/bin/easy_install /usr/bin/easy_install2.7# 验证操作是否成功pip --versionpip 9.0.1 from /usr/local/python27/lib/python2.7/site-packages/pip-9.0.1-py2.7.egg (python 2.7)easy_install --versionsetuptools 36.5.0 from /usr/local/python27/lib/python2.7/site-packages/setuptools-36.5.0-py2.7.egg (Python 2.7)\nSyntax error whenever i try to use sys.stderrYou think you’re using Python 3.x, but it’s actually Python 2.x. On most systems python executable means Python 2.x.\n虚拟环境：ERROR: virtualenv is not compatible with this system or executable!! 虚拟环境的路径中不能有中文，否则也会出现此问题\n","categories":["运行安装"]},{"title":"第2章 感性的语言","url":"/2022/02/14/%E6%9C%89%E6%AF%92%E7%9A%84%E9%80%BB%E8%BE%91-%E7%AC%AC2%E7%AB%A0%E6%84%9F%E6%80%A7%E7%9A%84%E8%AF%AD%E8%A8%80/","content":"\n&nbsp;&nbsp;越是坏事，越能说出好的道理来。\n我们都有情感的需要：爱的需要、被爱的需要、被接受的需要、体验成就感的需要、体验自我价值的需要、体会自身重要性的需要、感到被人需要的需要、能够保护自己的需要、获取自己眼中的相应地位和其他人眼中的相应地位的需要，以及安全感的需要。这些需要依次隐含了其他情感：爱、恨、恐惧、嫉妒、愤怒、愧疚、贪婪、希望和忠诚。情感既脆弱又敏感。它们很容易被入侵，也很容易被操纵。只要有人懂得如何诉诸我们的情感，就能欺骗我们，操纵我们，并且让我们把谬误当作真理来接受。\n以下就是为达到欺骗理性的目的而蹂躏情感的几种手段。如果我们能够辨认出它们，就有可能避免受到欺骗和操纵。\n诉诸怜悯有人求助于我们的同情心、慈悲和友爱，而不是拿出有案可查的严密推理、证据或事实。他们给我们展示一张瘦骨嶙峋的儿童的照片、一位营养不良的受害者，使得我们不得不拿出一大笔钱捐献给为养活饥饿儿童而建立的基金会。如今这样的诉求并没有什么本质上的错误。但是我们不应该太天真，居然相信我们的捐赠都能实实在在地用来养活饥饿的儿童。那些捐赠有多少会用于行政管理，有多少会用于其他广告宣传，又有多少会用于支付基金高管们的高薪？此种诉诸怜悯的关键问题在于，它并没有告诉我们捐赠会被如何使用，甚至都不能保证我们的捐赠能被用于当初所诉求的目的。\n诉诸罪恶感让我们回顾前一个例子。有人给我们展示了一张饥饿儿童的图片，然后又给我们展示了另一张温馨的家庭聚餐图片。“你们衣食无忧，”宣传语这样写着，“你们什么都不缺。比起全世界上百万的挨饿人群，你们心安理得。”我们因为生活舒适而被动地产生罪恶感。此外，它还暗示甚至可以说是明示：如果不捐款，我们的罪恶感更强。以后享用晚餐时，我们会不自觉地想起饥饿的儿童。这样的情景会一直萦绕在心中，直到我们捐款为止。\n对于这样的诉诸罪恶感有三点需要说明。\n第一，任何人无权践踏我们的情感天平。\n第二，除非能给出充足的理由支持我们应该有罪恶感的推测，否则此种推测无足轻重。\n第三，就算我们有罪恶感，也没有任何理由去做宣传所鼓吹的事情，因为仍然无法保证我们的捐赠会带来任何明显的好处。\n诉诸恐惧则试图恐吓我们，让我们做出特定的行为或者接受特定的信念。“如果你不照做X事情，就会发生Y事情。”当然，Y事情的后果非常可怕。“如果你不先杀死敌人，敌人就会先干掉你。”但是，为证明此命题正确有效，表述者有义务证明X事件和Y事件之间的确切因果关系。有时候诉诸恐惧是私下形成的。\n诉诸希望“如果你做了X事情，Y事情就有可能发生；如果你想要Y事情发生，就去做X事情吧。”但是，说这话的人既无法保证Y事情一定会发生，也没有任何好的理由证明X事情对Y事情有显著影响。州彩票活动就是利用这一手段。人人都想赢得十万美金，一旦听说有人中了奖，尤其是当配合使用“这也能发生在你身上”的营销说辞，这种希望所带来的感觉会令人忘记真正买到中奖彩票的机会是多么渺茫。\n诉诸恭维如果有人恭维我们，我们就会容易混淆对于恭维者的好感和恭维者真正表达的意思。贝丝恭维乔治，乔治享受贝丝的恭维，因此乔治对贝丝的一切都有正面倾向，也更加容易顺从贝丝的立场。但是，请注意，贝丝并没有提供任何正当的理由让乔治接受她的观点。\n诉诸地位有些人非常在意社会地位。他们炫耀这个是Gucci牌的，那个是Pucci牌的，他们开国外名车，他们无论买什么东西，名牌商标总是最重要的。这些人觉得，财富外露会让他们显得更加重要、更加与众不同、更加优雅或更加精明老练。这些人更容易受到声称能提高社会地位的诉求蛊惑：“充分彰显你独一无二的品位”是为一支150美元的圆珠笔打广告。毋庸置疑，一个人独特地位的取得是靠他的行为而不是靠他使用的产品。但这些人对烤鸡配米饭不屑一顾，却对arrozconpollo（西班牙语：西班牙米烧鸡）趋之若鹜。\n诉诸潮流此种诉求有些类似于诉诸地位，但不同之处在于，它诉诸我们内心渴望的归属感、不落人后。我们被鼓励到国外旅游，因为一切有品位的人都这么做。我们被鼓励到乡村买宅邸，因为我们的邻居有乡间别墅。日常生活中，“如果有一千万的家庭主妇使用Sparkle产品，你不是也应该使用Sparkle产品吗！”又是如此，像所有的感性诉求一样，我们去国外、买乡间别墅或者使用Sparkle的产品并没有合理的理由。唯一的理由就是不甘人后。\n诉诸信任有人说你不赞同他就意味着你不爱他或者不信任他。“要么与我同一阵线，要么与我为敌！”“如果你以前真正地信任我，你早该和我在一起了。”这种手段并不公平。你赞成一个人与否，与你对此人的喜爱毫无瓜葛。不与某人为伍并不意味着你不爱某人或者不信任某人。在接受任何思想路线或者同意任何行动方案之前，你应该首先确定接受或者同意的理由。否则你的行为可能是不负责任的。此类感性诉求有个共同的名称——诉诸友情。我们应该记得，真正的友情有时候需要我们投反对票。\n诉诸自豪或忠诚“如果你真的为自己的国家感到自豪，真的想看到它繁荣富强，那就买储蓄债券吧。”“你什么意思——你一张教会抽奖券都不买吗？你想干什么，反对教会吗？”“你再也不带我出去吃饭了，你就是嫌弃我，你再也不爱我了。”诉诸自豪或忠诚通常是一种令人眩目的过度简单化。不购买储蓄债券并不意味着对国家不忠诚。不购买抽奖券并不意味着对教会不忠诚。不带妻子出去吃饭并不意味着你不再以她为荣，或者不再爱她。诉诸真诚。此种诉求非常有效，尤其适用于演技派。这种人采用认真、诚挚、不出风头且绝对谦卑的语气，看上去此人说话绝对发自肺腑，他频频止声，仿佛很难找到合适的词语表达他的下一想法。他的感情是如此深沉，以至于一般的语言无法表达。他不断地重复字词以示强调。加强语气的动词形式（以语气助词does和do的形式）和副词——真实地（really）、千真万确地（genuinely）、实实在在地（truly）、绝对无疑地（absolutely）、实事求是地（actually）——都是用来加强真诚感。\n诉诸群众最后一个感性诉求可能涵盖了许多本章提到的其他诉求，也许是大部分诉求。这是对群众的诉求，对暴民的诉求，对议会旁听者的诉求。一概而论、陈腔滥调、标语口号、老生常谈、道貌岸然地哗众取宠以及歌颂大众等泛滥成灾。\n最后，有一点必须反复申明，感性诉求并非本身有错。有时候，这样的诉求只是反映了内心深处的感受或者信念。那位妻子也许真的被她心里认为“丈夫冷落自己”给伤害了，在她对自尊进行感性诉求时，情感自然流露出来。那位机修工有可能真的认为你的传动装置即将导致严重问题，他可能把诉诸恐惧作为一种便捷的手段，让你做出他认为必要的防备。而道奇欧米尼也许就是一部绝世好车。重要的是，你要认识到感性诉求可能反映了某些未言明的感受或信念，还留有一条底线未被明确地表达，或者还有一条隐藏的事项未被确认。一方面，也许那位试图说服你购买教会抽奖券的人是真的很想帮助教会；另一方面，也许他只是想完成抽奖券配额任务。永远记得找出什么是底线，或者探明隐藏的事项。感觉很重要，但人不能只依赖感觉行事。行动必须要有理性。正是由于理性的含糊不清，感性诉求才变得危险。\n","categories":["有毒的逻辑"]},{"title":"Rstudio配置镜像","url":"/2022/07/20/R/R-Rstudio%E9%85%8D%E7%BD%AE%E9%95%9C%E5%83%8F/","content":"方法\n安装package时指定镜像\n修改默认镜像\n\n安装package时指定镜像&gt; install.packages(&quot;ggpot2&quot;,repos=&quot;http://mirror.bjtu.edu.cn/ &quot;)\n\n常用镜像\n清华大学：http://mirrors.tuna.tsinghua.edu.cn/\n中国科学技术大学：http://mirror.bjtu.edu.cn/\n兰州大学：http://mirror.lzu.edu.cn/\n\n修改默认镜像\nTools—Global options\n\n点击右边菜单packages\n\n点击change，更改选择中国区域内的镜像\n点击apply\n\n","categories":["R"]},{"title":"install package","url":"/2022/07/20/R/R-install%20package/","content":"方法\ninstall.packages(‘包名’)\ninstall.packages(&#x27;colorspace&#x27;, depend=TRUE)\n\n如果所要下载的R包不在R语言官网上，那它极有可能在Bioconductor或者Github上，可以先登录Bioconductor官网（http://www.bioconductor.org/）搜索相关R包，比如edgeR这个包，搜索到后先查看其相关用途，再进行如下安装：\ninstall.packages(&#x27;BiocManager&#x27;)library(BiocManager)install(&#x27;edgeR&#x27;)\n这里需要注意的是，下载Bioconductor的R包需使用BiocManager包里的install函数。\n\n接下来便是安装源自Github（https://github.com/）的R包了，它的步骤和安装源自Bioconductor的R包类似，需要先安装devtools包，然后用devtools包里的install_github函数来进行安装，具体代码如下：\ninstall.packages(&#x27;devtools&#x27;)library(devtools)install_github(&#x27;gertvv/gemtc&#x27;)\n这里需要注意的是，github中的R包需要在其前面加上该包所在的库名，否则无法进行下载安装。\n\nRstudio内置\n右下角packages -&gt; install packages -&gt; 输入包名下载\n\n\n报错\n不存在叫 ‘xxxx’ 这个名字的程序包\n\n\n换一种方式下载。\n\n\n\n\n显示“退出狀態的值不是0”\n\n\n# 一个原因是R的版本太低\ninstall.packages(&quot;installr&quot;)\nlibrary(installr)\nupdateR()\n\n# 包重复，删除错误提示路径中的文件，重新安装\n# 依赖包未安装好\n\n\n\n\n参考\nR语言入门之R包的安装\n\n","categories":["R"]},{"title":"merge与subset的使用","url":"/2022/08/23/R/R-merge%E4%B8%8Esubset%20%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"原文：https://blog.csdn.net/qq_34941023/article/details/51606805\nmerge函数对数据框的操作，从两个数据框中选择出条件相等的行组合成一个新的数据框\ndf1=data.frame(name=c(&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;),age=c(20,29,30),sex=c(&quot;f&quot;,&quot;m&quot;,&quot;f&quot;))df2=data.frame(name=c(&quot;dd&quot;,&quot;bb&quot;,&quot;cc&quot;),age=c(40,35,36),sex=c(&quot;f&quot;,&quot;m&quot;,&quot;f&quot;))mergedf=merge(df1,df2,by=&quot;name&quot;)mergedfname age.x sex.x age.y sex.y1   bb    29     m    35     m2   cc    30     f    36     f\nsubset函数，从某一个数据框中选择出符合某条件的数据或是相关的列\n（1）单条件查询\n&gt; selectresult=subset(df1,name==&quot;aa&quot;)&gt; selectresult  name age sex1   aa  20   f\n（2）指定显示列\n&gt; selectresult=subset(df1,name==&quot;aa&quot;,select=c(age,sex))&gt; selectresult  age sex1  20   f\n（3）多条件查询\n&gt; selectresult=subset(df1,name==&quot;aa&quot; &amp; sex==&quot;f&quot;,select=c(age,sex))&gt; selectresult  age sex1  20   f\n","categories":["R"]},{"title":"R语言标准化（归一化）之scale()函数、sweep()函数","url":"/2022/08/23/R/R-%E5%BD%92%E4%B8%80%E5%8C%96%E6%A0%87%E5%87%86%E5%8C%96/","content":"R中数据的标准化0-1标准化数据标准化，是将数据按比例缩放，使之落入到特定区间，一般我们使用0-1标准化；\n\nx=\\frac{x-min}{max-min}&gt;data &lt;- read.csv(&#x27;1.csv&#x27;, fileEncoding=&#x27;utf-8&#x27;)&gt; data   class   name score1   一班 朱志斌   1202   一班   朱凤   1223   一班 郑丽萍   1404   一班 郭杰明   1315   一班   许杰   1226   二班   郑芬   1197   二班   林龙    968   二班 林良坤   1359   二班 黄志红   10510  三班 方小明   11411  三班 陈丽灵   11512  三班 方伟君   13613  三班 庄艺家   119&gt; data.scale &lt;- (data$score-min(data$score))/(max(data$score)-min(data$score))&gt; newData &lt;- data.frame(data, data.scale)&gt; newData   class   name score data.scale1   一班 朱志斌   120  0.54545452   一班   朱凤   122  0.59090913   一班 郑丽萍   140  1.00000004   一班 郭杰明   131  0.79545455   一班   许杰   122  0.59090916   二班   郑芬   119  0.52272737   二班   林龙    96  0.00000008   二班 林良坤   135  0.88636369   二班 黄志红   105  0.204545510  三班 方小明   114  0.409090911  三班 陈丽灵   115  0.431818212  三班 方伟君   136  0.909090913  三班 庄艺家   119  0.5227273\n注意scale( )标准化函数跟0-1标准化的区别。标准化的方法很多，根据实际数据分析需求进行选择。\n&gt; scale&lt;-scale(data$score)  &gt; scale              [,1]   [1,] -0.0865256   [2,]  0.0741648   [3,]  1.5203783   [4,]  0.7972716   [5,]  0.0741648   [6,] -0.1668708   [7,] -2.0148103   [8,]  1.1186523   [9,] -1.2917035  [10,] -0.5685968  [11,] -0.4882516  [12,]  1.1989975  [13,] -0.1668708  attr(,&quot;scaled:center&quot;)  [1] 121.0769  attr(,&quot;scaled:scale&quot;)  [1] 12.44629  &gt; View(data)  &gt; mean(data$score)  [1] 121.0769  &gt; sd(data$score)  [1] 12.44629  \n【机器学习】R语言标准化（归一化）之scale()函数、sweep()函数#数据集x&lt;-cbind(c(1,2,3,4),c(5,5,10,20),c(3,6,9,12)) #自己写标准化x_min_temp&lt;-apply(x,2,min) x_min&lt;-matrix(rep(x_min_temp,4),byrow=TRUE,ncol=3)        #需要输入行数和列数abs(x-x_min)                                              #当前值减去均值x_extreme_temp&lt;-apply(x,2,max)-apply(x,2,min)x_extreme&lt;-matrix(rep(x_extreme_temp,4),byrow=TRUE,ncol=3)#需要输入行数和列数abs(x-x_min)/x_extreme #sweep函数center &lt;- sweep(x, 2, apply(x, 2, min),&#x27;-&#x27;)     #在列的方向上减去最小值，不加‘-’也行R &lt;- apply(x, 2, max) - apply(x,2,min)          #算出极差，即列上的最大值-最小值x_star&lt;- sweep(center, 2, R, &quot;/&quot;)               #把减去均值后的矩阵在列的方向上除以极差向量#sweep函数更简洁、易懂，且不需要输入行数和列数，二者性能也差不多 #sweep再举一个例子m&lt;-matrix(c(1:9),byrow=TRUE,nrow=3)#第一行都加1，第二行都加4，第三行都加7sweep(m, 1, c(1,4,7), &quot;+&quot;)   #scale函数，这个比较简单，不多说scale(x, center = TRUE, scale = TRUE)\n","categories":["R"]},{"title":"安装R语言运行环境及RStudio-mac","url":"/2022/07/20/R/R-%E5%AE%89%E8%A3%85R%E8%AF%AD%E8%A8%80%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E5%8F%8ARStudio/","content":"步骤\nR 语言环境\nR studio软件\n\nR 语言环境\n安装包下载地址：https://cran.r-project.org\n\n下载.pkg文件，打开后一直按下一步即可\n\n\nR studio软件\n安装包下载地址：https://rstudio.com/products/rstudio/download/\n\n下载免费版.dmg文件，打开拖到application中即可\n\n\n","categories":["R"]},{"title":"R语言做正态分布检验","url":"/2022/08/23/R/R-%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%E6%A3%80%E9%AA%8C/","content":"ANOVA单因素方差分析与R实现在进行方差分析前先对输入数据做正态性检验。 对数据的正态性，利用Shapiro-Wilk正态检验方法(W检验)，它通常用于样本容量n≤50时，检验样本是否符合正态分布。\nR中，函数shapiro.test()提供了W统计量和相应P值，所以可以直接使用P值作为判断标准(P值大于0.05说明数据正态)，其调用格式为shapiro.test(x)，参数x即所要检验的数据集，它是长度在3到5000之间的向量。\n更多正态性检验见：R语言做正态分布检验 其中，D检验(Kolmogorov - Smirnov)是比较精确的正态检验法。\n\nSPSS 规定:当样本含量3 ≤n ≤5000 时,结果以Shapiro - Wilk (W 检验) 为准,当样本含量n &gt; 5000 结果以Kolmogorov - Smirnov 为准。\nSAS 规定:当样本含量n ≤2000 时,结果以Shapiro - Wilk (W 检验) 为准,当样本含量n &gt;2000 时,结果以Kolmogorov - Smirnov (D 检验) 为准。\n\nR语言做正态分布检验1、ks.test() 例如零假设为N（15,0.2），则ks.test(x,”pnorm”,15,0.2)。如果不是正态分布，还可以选”pexp”, “pgamma”等。\n2、shapiro.test(data) 可以进行关于正态分布的Shapiro-Wilk检验。\n3、nortest包 lillie.test()可以实行更精确的Kolmogorov-Smirnov检验。 ad.test()进行Anderson-Darling正态性检验。 cvm.test()进行Cramer-von Mises正态性检验。 pearson.test()进行Pearson卡方正态性检验。 sf.test()进行Shapiro-Francia正态性检验。\n4、fBasics包 normalTest()进行Kolmogorov-Smirnov正态性检验。 ksnormTest()进行Kolmogorov-Smirnov正态性检验。 shapiroTest()进行Shapiro-Wilk’s正态检验。 jarqueberaTest()进行jarque-Bera正态性检验。 dagoTest进行D’Agostino正态性检验。 gofnorm采用13种方法进行检验，并输出结果。\nR语言—正态性检验总结一下：判断分布的步骤：\n（1）首先应该看直方图和密度曲线。\n（2）然后结合QQ图和峰度、偏度的信息在做判断。可以通过moments包直接计算skewness和kurtosis来判断是否符合正态分布，也可以通过fitdistrplus包对数据的分布进行探索。\n（3）最后，以上这些都只是描述性的，事实是否如我们所见的那样呢？需要通过进一步的统计检验来确证，这就如同诊断肿瘤得靠病理切片，得到病因得靠高质量的RCT或队列研究。\n4.统计学检验（拟合优度检验）\n常用的4种方法：\n（1）Shapiro-Wilks检验：\nshapiro.test(a);shapiro.test(b);shapiro.test(c);shapiro.test(d)\n（2）Kolmogorov-Smirnov(K-S检验)\nks.test(x, y, ..., alternative=c(&quot;two.sided&quot;,&quot;less&quot;,&quot;greater&quot;), exact=NULL)\nx:要检验的向量或具体数据\ny:可以和x的类型一样，为待检验的数据，也可以是累积分布函数的名称或具体的累积分布函数。当然，只有连续型的累积密度函数（CDF）才是有效的。\n…：由y所指定的分布所对应的参数。\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nq：在x轴上某个点取值为q，计算q左侧概率密度曲线下的面积。q取0时，面积应为0.5。\n下面进行KS检验：两种写法结果相近，更多的用第一种。\nks.test(a,&quot;pnorm&quot;,mean=mean(a),sd=sqrt(var(a)))ks.test(a,pnorm(q=0,mean = mean(a),sd=sqrt(var(a))))\n（3）Cramer-Von Mises检验（cvm.test）\nlibrary(nortest)cvm.test(a)cvm.test(b)cvm.test(c)cvm.test(d)\n（4）Anderson Darling检验\nlibrary(nortest)ad.test(a)ad.test(b)ad.test(c)ad.test(d)\n具体见原链接。\n","categories":["R"]},{"title":"R系统教程","url":"/2022/08/23/R/R-%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B/","content":"李东风开设《统计软件》等课程的讲义\n前言\nI 介绍\n1 R语言介绍\n2 R语言入门运行样例\nII 数据类型与相应的运算\n3 常量与变量\n4 数值型向量及其运算\n5 逻辑型向量及其运算\n6 字符型数据及其处理\n7 R向量下标和子集\n8 R数据类型的性质\n9 R日期时间\n10 R因子类型\n11 列表类型\n12 R矩阵和数组\n13 数据框\n14 工作空间和变量赋值\nIII 编程\n15 R输入输出\n16 程序控制结构\n17 函数\n18 R程序效率\n19 函数进阶\nIV 制作研究报告和图书\n20 用R制作研究报告\n21 Markdown格式\n22 R Markdown文件格式\n23 用bookdown制作图书\n24 用R Markdown制作简易网站\n25 制作幻灯片\nV 数据整理与汇总\n26 数据整理\n27 数据汇总\nVI 绘图\n28 基本R绘图\n29 ggplot作图入门\n30 ggplot的各种图形\nVII 统计模型\n31 R初等统计分析\n32 R相关与回归\n33 R多元回归\n34 R非参数回归\n35 R方差分析\n36 线性混合模型\n37 广义线性模型\n38 统计学习介绍\n39 R时间序列分析\n40 随机模拟\nVIII 特殊应用\n41 R语言的文本处理\nIX 用Rcpp访问C++代码\n42 Rcpp介绍\n43 R与C++的类型转换\n44 Rcpp 属性\n45 Rcpp提供的C++数据类型\n46 Rcpp糖\n47 用Rcpp帮助制作R扩展包\nX 其它\n48 R编程例子\n49 使用经验\nReferences\n\nwiki R教程\nR - 概述\nR - 环境设置( Environment Setup)\nR - Basic 语法\nR - 数据类型\nR - 变量\nR - 运算符\nR - 决策( Decision Making)\nR - 循环\nR - 功能( Functions)\nR - Strings\nR - Vectors\nR - Lists\nR - 矩阵( Matrices)\nR - Arrays\nR - Factors\nR - 数据帧( Data Frames)\nR - 包裹( Packages)\nR - 数据重塑( Data Reshaping)\nR - CSV 文件\nR - Excel 文件\nR - Binary 文件\nR - XML 文件\nR - JSON 文件\nR - Web Data\nR - 数据库( Database)\nR - Pie Charts\nR - Bar Charts\nR - Boxplots( Boxplots)\nR - 直方图( Histograms)\nR - 线图( Line Graphs)\nR - 散点图( Scatterplots)\nR - 均值，中位数和模式( Mean, Median &amp; Mode)\nR - 线性回归( Linear Regression)\nR - 多元回归( Multiple Regression)\nR - Logistic回归( Logistic Regression)\nR - 正态分布( Normal Distribution)\nR - 二项分布( Binomial Distribution)\nR - 泊松回归( Poisson Regression)\nR - 协方差分析( Analysis of Covariance)\n具有分类变量和预测变量之间相互作用的模型\n模型没有分类变量和预测变量之间的相互作用\n比较两种模型\n\n\nR - 协方差分析( Analysis of Covariance)\nR - 非线性最小二乘法( Nonlinear Least Square)\nR - 决策树( Decision Tree)\nR - 随机森林( Random Forest)\nR - 生存分析( Survival Analysis)\nR - Chi广场测试( Chi Square Tests)\nR - 面试问题( Interview Questions)\nR - 快速指南\nR - 有用的资源\nR - 讨论\n\n","categories":["R"]},{"title":"保存文件","url":"/2022/10/04/python%E5%B0%8F%E8%AE%B0/python%E5%B0%8F%E8%AE%B0-%20%E4%BF%9D%E5%AD%98%E6%96%87%E4%BB%B6/","content":"合并到多个sheet工作表import pandas as pdwriter = pd.ExcelWriter(&#x27;test.xlsx&#x27;)data1 = pd.read_csv(&quot;2019-04-01.csv&quot;, encoding=&quot;gbk&quot;)data2 = pd.read_csv(&quot;2019-04-02.csv&quot;, encoding=&quot;gbk&quot;)data1.to_excel(writer,sheet_name=&#x27;2019-04-01&#x27;)data2.to_excel(writer,sheet_name=&#x27;2019-04-02&#x27;)writer.save()\n简单来说就是\ndata.to_excel(file, sheet_name = name, encoding=&#x27;gbk&#x27;, index_col=0)\n\ndata：需要保存的dataframe格式的数据\nfile：pd.ExcelWriter(&#39;name.xlsx&#39;)格式的写入文件\nsheet_name：sheet name\nencoding：编码格式\nindex_col：是否保留序号，0为不保留\n\n把所有要处理的csv文件放在一个文件夹中。让python自动读取这些csv文件，并创建一个Excel文件，以及自动将文件名作为sheet导入到Excel文件中。\nimport pandas as pdimport os newdir = &#x27;G:\\编程代码\\python代码\\表格\\\\new&#x27;list = os.listdir(newdir)  # 列出文件夹下所有的目录与文件 writer = pd.ExcelWriter(&#x27;步数.xlsx&#x27;) for i in range(0,len(list)):    data = pd.read_csv(list[i],encoding=&quot;gbk&quot;, index_col=0)    data.to_excel(writer, sheet_name=list[i]) writer.save()\nlist元素转存为CSV文件import pandas as pd list=[[1,2,3],[4,5,6],[7,9,9]]name=[&#x27;one&#x27;,&#x27;two&#x27;,&#x27;three&#x27;]name2=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;]test=pd.DataFrame(columns=name,index=name2,data=list) # columns 列名，index 行名test.to_csv(&#x27;e:/testcsv.csv&#x27;,encoding=&#x27;gbk&#x27;)\n","categories":["python小记"]},{"title":"论文动词查找","url":"/2022/10/04/python%E5%B0%8F%E8%AE%B0/python%E5%B0%8F%E8%AE%B0-%20%E8%AE%BA%E6%96%87%E5%8A%A8%E8%AF%8D%E6%9F%A5%E6%89%BE/","content":"老师有个作业，需要找出四篇论文中常见的动词，但是我对她给的领域不是很感兴趣，但是又想自己整理一些论文的动词，所以就想写个程序解决这个问题\n先在网上找到了一个nltk的demo：使用Python+NLTK实现英文单词词频统计，但是经过几次测试后，准确率不是很高，感觉仅仅只是分词后对每个单词进行一个常见词性分析，并没有纳入句子信息（可能没发现其他方法）。之后寻找类似库，发现了stanford-corenlp，github见https://github.com/Lynten/stanford-corenlp。这个库是用java写的，但是可以用python的api。\ndemo# Simple usagefrom stanfordcorenlp import StanfordCoreNLPnlp = StanfordCoreNLP(r&#x27;G:\\JavaLibraries\\stanford-corenlp-full-2018-02-27&#x27;)sentence = &#x27;Guangdong University of Foreign Studies is located in Guangzhou.&#x27;print &#x27;Tokenize:&#x27;, nlp.word_tokenize(sentence)print &#x27;Part of Speech:&#x27;, nlp.pos_tag(sentence)print &#x27;Named Entities:&#x27;, nlp.ner(sentence)print &#x27;Constituency Parsing:&#x27;, nlp.parse(sentence)print &#x27;Dependency Parsing:&#x27;, nlp.dependency_parse(sentence)nlp.close() # Do not forget to close! The backend server will consume a lot memery.\n注意：您必须下载一个附加模型文件并将其放在.../stanford-corenlp-full-2018-02-27文件夹中。例如，stanford-chinese-corenlp-2018-02-27-models.jar如果要处理中文，则应下载该文件。(可以对别的语言进行分词，但是需要下载对应的jar文件)\n# _*_coding:utf-8_*_# Other human languages support, e.g. Chinesesentence = &#x27;清华大学位于北京。&#x27;with StanfordCoreNLP(r&#x27;G:\\JavaLibraries\\stanford-corenlp-full-2018-02-27&#x27;, lang=&#x27;zh&#x27;) as nlp:    print(nlp.word_tokenize(sentence))    print(nlp.pos_tag(sentence))    print(nlp.ner(sentence))    print(nlp.parse(sentence))    print(nlp.dependency_parse(sentence))\n其他用法详见github。\n词性标记\nCC Coordinating conjunctionCD Cardinal numberDT DeterminerEX Existential thereFW Foreign wordIN Preposition or subordinating conjunctionJJ AdjectiveJJR Adjective, comparativeJJS Adjective, superlativeLS List item markerMD ModalNN Noun, singular or massNNS Noun, pluralNNP Proper noun, singularNNPS Proper noun, pluralPDT PredeterminerPOS Possessive endingPRP Personal pronounPRP$ Possessive pronounRB AdverbRBR Adverb, comparativeRBS Adverb, superlativeRP ParticleSYM SymbolTO toUH InterjectionVB Verb, base formVBD Verb, past tense\nVBG Verb, gerund or present participleVBN Verb, past participleVBP Verb, non­3rd person singular presentVBZ Verb, 3rd person singular presentWDT Wh­determinerWP Wh­pronounWP$ Possessive wh­pronounWRB Wh­adverb\n\ncodefrom stanfordcorenlp import StanfordCoreNLPimport pprintnlp = StanfordCoreNLP(r&#x27;stanford-corenlp-4.5.1&#x27;)sen = &#x27;&#x27;&#x27;I am happy. You are sad.&#x27;&#x27;&#x27;# files = [&quot;2008-introduction.txt&quot;, &quot;2008-Theoretical.txt&quot;, &quot;2008-Applications.txt&quot;, &quot;2008-Conculsion.txt&quot;,#          &quot;2012-Conculsion.txt&quot;, &quot;2012-Course-design.txt&quot;, &quot;2012-Discussion.txt&quot;,&quot;2012-introduction.txt&quot;, &quot;2012-Literature.txt&quot;,&quot;2012-Results.txt&quot;]# files=[&quot;2009-Conceptual-framework.txt&quot;, &quot;2009-Design.txt&quot;, &quot;2009-Discussion.txt&quot;,#        &quot;2009-Evaluation-results.txt&quot;, &quot;2009-Introduction.txt&quot;]# files = [&quot;2016-Background.txt&quot;,&quot;2016-Discussion.txt&quot;,&quot;2016-Introduction.txt&quot;,&quot;2016-Methods.txt&quot;,&quot;2016-Results.txt&quot;]files = [&quot;introduction.txt&quot;,&quot;Discussion.txt&quot;,&quot;Methods.txt&quot;,&quot;Results.txt&quot;,&quot;Literature.txt&quot;]final = &#123;&#125;final_spe = &#123;&#125;for file in files:    doc = &#x27;&#x27;    with open(file) as f:        for line in f:            doc = doc + line.strip()    props = &#123;&#x27;annotators&#x27;: &#x27;pos,lemma&#x27;&#125;    s = nlp.annotate(doc, properties=props)    import json    new_dict = json.loads(s)    res = &#123;&#125;    # 每个句子    for sentence in new_dict[&quot;sentences&quot;]:      for word in sentence[&quot;tokens&quot;]:        if &#x27;V&#x27; in word[&quot;pos&quot;]:          lemma = word[&quot;lemma&quot;].lower()          pos = word[&quot;pos&quot;]          if lemma in res:            if pos in res[lemma]:              res[lemma][pos] += 1            else:              res[lemma][pos] = 1            else:              res[lemma] = &#123;pos: 1&#125;    final_spe[file[:-4]] = res    final[file[:-4]] = sorted([[key, sum(value.values())] for key, value in res.items()], key=lambda x: x[1],reverse=True)# pprint.pprint(final)import pandas as pdwriter = pd.ExcelWriter(&#x27;final.xlsx&#x27;)for key, value in final.items():    value = pd.DataFrame(columns=[&quot;word&quot;, &quot;count&quot;], data=value)    value.to_excel(writer, sheet_name=key)writer.save()# pprint.pprint(new_dict)# print(new_dict[&quot;sentences&quot;][1][&quot;tokens&quot;][0][&quot;lemma&quot;])\n整理后\n\n","categories":["python小记"]},{"title":"python中无穷大与无穷小表示","url":"/2022/07/31/python%E5%B0%8F%E8%AE%B0/python%E5%B0%8F%E8%AE%B0-python%E4%B8%AD%E6%97%A0%E7%A9%B7%E5%A4%A7%E4%B8%8E%E6%97%A0%E7%A9%B7%E5%B0%8F%E8%A1%A8%E7%A4%BA/","content":"python中整型不用担心溢出，因为python理论上可以表示无限大的整数，直到把内存挤爆。而无穷大在编程中常常需要的。比如，从一组数字中筛选出最小的数字。一般使用一个临时变量用于存储最后结果，变量去逐个比较和不断地更新。而这临时变量一般要初始无穷大或者去第一个元素的值。\n正无穷大与负无穷大python中并没有特殊的语法来表示这些值，但是可以通过 float() 来创建它们：\n&gt;&gt;&gt; a = float(&quot;inf&quot;)&gt;&gt;&gt; b = float(&quot;-inf&quot;)&gt;&gt;&gt; ainf&gt;&gt;&gt; b-inf\n为了测试这些值的存在，使用 math.isinf() 进行判断：\n&gt;&gt;&gt; import math&gt;&gt;&gt; math.isinf(a)True&gt;&gt;&gt; math.isinf(b)True\n无穷大数在执行数学计算的时候会传播这个就类似于数学中讲述的，无穷大加上一个常数还是无穷大，无穷大与无穷大相等：\n&gt;&gt;&gt; a = float(&#x27;inf&#x27;)&gt;&gt;&gt; a + 45inf&gt;&gt;&gt; a * 10inf&gt;&gt;&gt; 10 / a0.0&gt;&gt;&gt; float(&quot;inf&quot;) == float(&quot;inf&quot;)True\n无穷大在比较中比任何一个数都要大。\n无穷大在比较中比任何一个数都要大。\n正无穷与负无穷相加的结果是什么有些操作时未定义的并会返回一个 NaN 结果:\n&gt;&gt;&gt; a = float(&#x27;inf&#x27;)&gt;&gt;&gt; a/anan&gt;&gt;&gt; b = float(&#x27;-inf&#x27;)&gt;&gt;&gt; a + bnan\n表示非数字的 NaNnan 值在所有操作中也会传播，并且不会产生异常：\n&gt;&gt;&gt; c = float(&#x27;nan&#x27;)&gt;&gt;&gt; c + 23nan&gt;&gt;&gt; c / 2nan&gt;&gt;&gt; c * 2nan&gt;&gt;&gt; math.sqrt(c)nan\n使用 math.isnan() 可以判断值是否是 NaN：\n&gt;&gt;&gt; math.isnan(c)True\nnan 值的任何比较操作都是返回 False ：\n&gt;&gt;&gt; float(&quot;nan&quot;) == float(&quot;nan&quot;)False&gt;&gt;&gt; c &gt; 3False\n由于无穷的存在，因此字符串装浮点数就存在的一些例外，并且这个转换过程不会抛出异常。如果程序员们想改变 python 的默认行为，可以使用 fpectl 模块，但是它在标准的Python 构建中并没有被启用，它是平台相关的，并且针对的是专家级程序员。这里提供一个比较简单的转换，就是加一个 isdigit() 判断:\ndef str2float(ss):    if not ss.isdigit():        raise ValueError    return float(ss) sss = &quot;inf&quot;a = str2float(sss)\n原链接https://blog.csdn.net/hellojoy/article/details/81077019\n","categories":["python小记"]},{"title":"python列表的深浅层复制及效率问题","url":"/2022/09/07/python%E5%B0%8F%E8%AE%B0/python%E5%B0%8F%E8%AE%B0-python%E5%88%97%E8%A1%A8%E7%9A%84%E6%B7%B1%E6%B5%85%E5%B1%82%E5%A4%8D%E5%88%B6/","content":"浅复制a = [1, 2, 3, 4]# 第一种 使用copy()函数 或导入copy包b = a.copy()import copyb = copy.copy(a)# 第二种 b = a[:]# 第三种b = list(a)\n特点：b有独立的不用于a的地址，修改元素不会相互影响但 若a中的元素包含列表等特殊类型时，上述方法则无效，修改时会同时影响a和b。因为浅复制只是改变了第一层的引用地址，a中存储的列表的第二层引用没有改变，是单独分配的空间。此时需要用到列表的深层复制。\n即：二维列表等使用上述方法复制仍会改变原列表。\n深复制import copya = [1, 2, [4, 5, 6], 4]b = copy.deepcopy(a)\n深拷贝效率问题不过在使用深拷贝的时候需要慎重，阅读过源码的都知道，深拷贝需要维护一个 memo 用于记录已经拷贝的对象，这是它比较慢的原因。在绝大多数情况下，程序里都不存在相互引用。但作为通用模块，Python 深拷贝必须为了这 1% 情形，牺牲 99% 情形下的性能。所以可以另外一种复制方式以提高效率。\n#生成一个列表li = [i for i in xrange(10000000)] #方式1：直接复制st1 = time.clock()temp1 = [i for i in li]# 二维列表# copy = [[graph[i][j] for j in range(len(graph[0]))] for i in range(len(graph))]end1 = time.clock() #方式2：深拷贝start2 = time.clock()temp2 = copy.deepcopy(li)end2 = time.clock() print &quot;方式1耗时：%s&quot; % (end1 - st1)print &quot;方式2耗时：%s&quot; % (end2 - start2)# 方式1耗时：0.996229993256# 方式2耗时：18.3346241035\n推荐使用列表推导式复制一维：[i for i in ls]二维：[[graph[i][j] for j in range(len(graph[0]))] for i in range(len(graph))]\n参考\nPython List 列表的深浅层复制\n\nPython 深拷贝效率问题与改进\n\n\n","categories":["python小记"]},{"title":"python中的一些复杂度","url":"/2022/09/13/python%E5%B0%8F%E8%AE%B0/python%E5%B0%8F%E8%AE%B0-python%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%8D%E6%9D%82%E5%BA%A6/","content":"listpython的列表内部实现是数组（数组是一种线性表结构，其用一块连续的内存空间，来存储一组具有相同类型的数据。具体实现要看解析器, CPython的实现 ），因此就有数组的特点。超过容量会增加更多的容量，set, get 是O(1)，但del, insert, in的性能是O(n)。具体的看下表，’n’是容器中当前的元素数， ‘k’需要操作的元素个数\n\n\n\n\nOperation\nAverage Case\nAmortized Worst Case\n\n\n\n\nCopy\nO(n)\nO(n)\n\n\nAppend[1]\nO(1)\nO(1)\n\n\nInsert\nO(n)\nO(n)\n\n\nPop\nO(n)(best O(1))\nO(n)\n\n\nGet Item\nO(1)\nO(1)\n\n\nSet Item\nO(1)\nO(1)\n\n\nDelete Item\nO(n)\nO(n)\n\n\nIteration\nO(n)\nO(n)\n\n\nGet Slice\nO(k)\nO(k)\n\n\nDel Slice\nO(n)\nO(n)\n\n\nSet Slice\nO(k+n)\nO(k+n)\n\n\nExtend[1]\nO(k)\nO(k)\n\n\nSort\nO(n log n)\nO(n log n)\n\n\nMultiply\nO(nk)\nO(nk)\n\n\nx in s\nO(n)\n\n\n\nmin(s), max(s)\nO(n)\n\n\n\nGet Length\nO(1)\nO(1)\n\n\n\n\n列表的实现\ntypedef struct &#123;    PyObject_VAR_HEAD    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */    PyObject **ob_item;    /* ob_item contains space for &#x27;allocated&#x27; elements.  The number     * currently in use is ob_size.     * Invariants:     *     0 &lt;= ob_size &lt;= allocated     *     len(list) == ob_size     *     ob_item == NULL implies ob_size == allocated == 0     * list.sort() temporarily sets allocated to -1 to detect mutations.     *     * Items must normally not be NULL, except during construction when     * the list is not yet visible outside the function that builds it.     */    Py_ssize_t allocated;&#125; PyListObject;list_resize(PyListObject *self, Py_ssize_t newsize)&#123;    PyObject **items;    size_t new_allocated;    Py_ssize_t allocated = self-&gt;allocated;    /* Bypass realloc() when a previous overallocation is large enough       to accommodate the newsize.  If the newsize falls lower than half       the allocated size, then proceed with the realloc() to shrink the list.    */    if (allocated &gt;= newsize &amp;&amp; newsize &gt;= (allocated &gt;&gt; 1)) &#123;        assert(self-&gt;ob_item != NULL || newsize == 0);        Py_SIZE(self) = newsize;        return 0;    &#125;    /* This over-allocates proportional to the list size, making room     * for additional growth.  The over-allocation is mild, but is     * enough to give linear-time amortized behavior over a long     * sequence of appends() in the presence of a poorly-performing     * system realloc().     * The growth pattern is:  0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ...     */    new_allocated = (newsize &gt;&gt; 3) + (newsize &lt; 9 ? 3 : 6);...&#125;# 截取部分代码\n可以看到，python list本质上是一个over-allocate的数组，啥叫over-allocate呢？就是当底层数组容量满了而需要扩充的时候，python依据规则会扩充多个位置出来。比如初始化列表array=[1, 2, 3, 4]，向其中添加元素23，此时array对应的底层数组，扩充后的容量不是5，而是8。这就是over-allocate的意义，即扩充容量的时候会多分配一些存储空间。这样做的优点当然是提高了执行效率，否则每次添加元素，都要对底层数组进行扩充，效率是很低下的。另外，当列表存储的元素在变少时，python也会及时收缩底层的数组，避免造成内存浪费。这里可以通过对列表的实践，验证扩充与收缩的过程（通过 __sizeof__()或sys.getsizeof()查看内存变化，并推算容量值）。\npython列表存储的是对象引用，而非对象本身。不管是python文档的说明还是我们开发过程中的实践，均能感知到列表存储的是对象的引用。\nappend向列表尾部添加元素，最好情况即列表容量足够，添加元素一步完成，对应的时间复杂度为O(1)；最坏情况即列表需要扩容，这时需要对现有元素一一遍历移动位置，此时时间复杂度为O(n)。因此最终的时间复杂度为O(1)。\nstatic intapp1(PyListObject *self, PyObject *v)&#123;    Py_ssize_t n = PyList_GET_SIZE(self);    assert (v != NULL);    if (n == PY_SSIZE_T_MAX) &#123;        PyErr_SetString(PyExc_OverflowError,            &quot;cannot add more objects to list&quot;);        return -1;    &#125;    if (list_resize(self, n+1) == -1)        return -1;    Py_INCREF(v);    PyList_SET_ITEM(self, n, v);    return 0;&#125;intPyList_Append(PyObject *op, PyObject *newitem)&#123;    if (PyList_Check(op) &amp;&amp; (newitem != NULL))        return app1((PyListObject *)op, newitem);    PyErr_BadInternalCall();    return -1;&#125;\n只有两步：重置 List 空间；新增元素设置到尾部。非常直接，常数时间内即可完成。\npoppop 元素根据弹出元素是否是尾部元素区别处理，如果是尾部元素，那么只是进行 resize 操作，弹出元素会被直接截断抛弃。实际上在这个过程中有可能根本不会发生真正的 resize，只是调整了 ob_size 的大小。此时时间复杂度为O(1)。\n较为复杂的情况是弹出元素在数组中间或者头部，当目标元素弹出后，会在 List 中出现一个 “空缺”，其后的元素会被 “前移”，这与插入过程恰好相反，时间复杂度为O(n)。\n//Objects/listobject.c:1025static PyObject *list_pop_impl(PyListObject *self, Py_ssize_t index)/*[clinic end generated code: output=6bd69dcb3f17eca8 input=b83675976f329e6f]*/&#123;    PyObject *v;    int status;    // 忽略一些无关代码    v = self-&gt;ob_item[index];    // 如果要弹出结尾元素，快捷处理    if (index == Py_SIZE(self) - 1) &#123;        status = list_resize(self, Py_SIZE(self) - 1);        if (status &gt;= 0)            return v; /* and v now owns the reference the list had */        else            return NULL;    &#125;    // 如果位于 List 中间，则删除元素    Py_INCREF(v);    status = list_ass_slice(self, index, index+1, (PyObject *)NULL);    if (status &lt; 0) &#123;        Py_DECREF(v);        return NULL;    &#125;    return v;&#125;\nindex or index assignment列表的索引操作，比如data[i]或data[i]=new_item，对应的时间复杂度为O(1)，即常量阶。对于一块连续的存储空间，获取某个位置的地址通过一步计算即可算出。通常的公式是i_addr = start_addr + i  unit_byte，比如一个int类型的数组，想要计算数组下标为2的元素的内存地址，此时就等于数组的起始内存地址（数组下标为0的地址）+ 2  unit_byte，unit_byte为不同类型对应的字节数，比如int类型占用2个节点，float类型占用4个字节。因为数组存储的是一组具有相同类型的数据，unit_byte固定，因此i位置内存地址较容易算出。python列表可以存储任意类型，为啥也能通过这个公式算计算呢，本质上还是因为列表实际存储的是对象的引用，每个对象的引用占用的字节数固定，因此i_addr = start_addr + i * unit_byte公式可用。\ninsert insert(index, object)：在指定索引前插入元素object。在第一个位置插入元素时，列表中原有的所有数据均要向后移动，在最后一个位置插入元素时，这时就不需要移动元素的操作。同时insert伴随着容量扩容的潜在操作，因此最终的时间复杂度为O(n)。\ndel del通常的操作是del data[i] 或者 del data[i:j]，删除元素后，涉及到i或j之后元素的向前移动的操作，同时还会伴随着数组收缩的潜在操作，因此对应的时间复杂度为O(n)。\ncontains or in 判断一个元素是否在列表内，需要从头遍历列表，最好情况是第一个元素就是，最坏情况是最后一个元素才是，或者元素根本就不在列表中，这两种情况均会完整遍历列表元素。\nstatic intlist_contains(PyListObject *a, PyObject *el)&#123;    Py_ssize_t i;    int cmp;    for (i = 0, cmp = 0 ; cmp == 0 &amp;&amp; i &lt; Py_SIZE(a); ++i)        cmp = PyObject_RichCompareBool(el, PyList_GET_ITEM(a, i),                                           Py_EQ);    return cmp;&#125;\niteration iteration需要遍历列表内的所有元素，因此时间复杂度为O(n)。\nremove remove(value)：删除列表中第一次出现的value。首先要从头开始遍历列表，判断是否找到value值，若找到value，则紧接着将其删除，然后后续的元素向前移动。如果要删除的元素在列表的第一个位置，删除之后，后续所有的元素均向前移动；若要删除的元素在最后一个位置，删除之后，不存在要移动位置的元素，但需要遍历到最后一个元素。这些操作同时伴随着底层数组收缩的潜在操作，因此最终的时间复杂度为O(n)。\nstatic PyObject *listremove(PyListObject *self, PyObject *v)&#123;    Py_ssize_t i;    for (i = 0; i &lt; Py_SIZE(self); i++) &#123;        int cmp = PyObject_RichCompareBool(self-&gt;ob_item[i], v, Py_EQ);        if (cmp &gt; 0) &#123;            if (list_ass_slice(self, i, i+1,                               (PyObject *)NULL) == 0)                Py_RETURN_NONE;            return NULL;        &#125;        else if (cmp &lt; 0)            return NULL;    &#125;    PyErr_SetString(PyExc_ValueError, &quot;list.remove(x): x not in list&quot;);    return NULL;&#125;\ncountcount(value)：获取列表中value出现的次数，这个仅需要完整遍历列表元素即可，不涉及元素移动位置的操作，因此时间复杂度为O(n)。\nstatic PyObject *listcount(PyListObject *self, PyObject *v)&#123;    Py_ssize_t count = 0;    Py_ssize_t i;    for (i = 0; i &lt; Py_SIZE(self); i++) &#123;        int cmp = PyObject_RichCompareBool(self-&gt;ob_item[i], v, Py_EQ);        if (cmp &gt; 0)            count++;        else if (cmp &lt; 0)            return NULL;    &#125;    return PyInt_FromSsize_t(count);&#125;\nlen len(list)：获取列表内元素的个数，因为在列表实现中，其内部维护了一个Py_ssize_t类型的变量表示列表内元素的个数，因此时间复杂度为O(1)。\nreverse reverse()：反转列表内的所有元素，至少需要遍历一半的元素，因此时间复杂度为O(n)。\nsort sort(cmp=None, key=None, reverse=False)：对列表内的元素，依据某种策略进行排序。其时间复杂度为O(nlogn)，这里咱们先记住这个复杂度，等到后续的排序文章中再给大家做具体的分析。\n通过分析可以发现，列表不太适合做元素的查找、删除、插入等操作，对应的时间复杂度为O(n)；访问某个索引的元素、尾部添加元素或删除元素这些操作比较适合做，对应的时间复杂度为O(1)。比如我们要在业务开发中，判断一个value是否在一个数据集中，如果数据集用list存储，那此时的判断操作就很耗时，如果我们用hash table（set or dict，后续也会专门写文章分析）来存储，此时的判断就能在O(1)的复杂度下完成，这样我们的程序就会有一定的提高。当然，在开发中如何有效的评估数据量也是非常重要的。\ndict关于字典需要了解的是hash函数和哈希桶。一个好的hash函数使到哈希桶中的值只有一个，若多个key hash到了同一个哈希桶中，称之为哈希冲突。查找值时，会先定位到哈希桶中，再遍历hash桶。更详细的信息请点这里。在hash基本没有冲突的情况下get, set, delete, in方面都是O(1)。自己的操作不会超过O(n)\n\n\n\n\nOperation\nAverage Case\nAmortized Worst Case\n\n\n\n\nCopy[2]\nO(n)\nO(n)\n\n\nGet Item\nO(1)\nO(n)\n\n\nSet Item[1]\nO(1)\nO(n)\n\n\nDelete Item\nO(1)\nO(n)\n\n\nx in s\nO(1)\nO(n)\n\n\nIteration[2]\nO(n)\nO(n)\n\n\n\n\nset内部实现是dict的。在in操作上是O(1), 这一点比list要强。也有list不存在的差运算\n\n\n\n\nOperation\nAverage case\nWorst Case\n\n\n\n\nx in s\nO(1)\nO(n)\n\n\nUnion s\\\nt\nO(len(s)+len(t))\n\n\n\nIntersection s&amp;t\nO(min(len(s), len(t))\nO(len(s) * len(t))\n\n\nMultiple intersection s1&amp;s2&amp;…&amp;sn\n\n(n-1)*O(l) where l is max(len(s1),…,len(sn))\n\n\nDifference s-t\nO(len(s))\n\n\n\ns.difference_update(t)\nO(len(t))\n\n\n\nSymmetric Difference s^t\nO(len(s))\nO(len(s) * len(t))\n\n\ns.symmetric_difference_update(t)\nO(len(t))\nO(len(t) * len(s))\n\n\n\n\n原网址\n深入Python(17)-List 的 append 和 pop\n\n【python】list，dict，set的时间复杂度\n\npython list 之时间复杂度分析\n\n\n","categories":["python小记"]},{"title":"python提速","url":"/2022/09/25/python%E5%B0%8F%E8%AE%B0/python%E5%B0%8F%E8%AE%B0-python%E6%8F%90%E9%80%9F/","content":"Input() → sys.stdin.readline()优化接收参数过程，将input() 接收参数，改为 sys.stdin.readline()\n由于input 默认将每行最后的换行符删除掉，需要额外的运算。\n在python3环境下，读取数据量很大时，sys.stdin.readline()会提高不少运算速度\nimport sysprint(&#x27;Plase input your name: &#x27;)name = sys.stdin.readline()print(name)\n列表推导式 → for 循环a = []for i  in range(10):    a.append(i)print(a)b = [a for a in range(10)]# b = [a for a in range(10) if a % 2 ==0]print(b)\n","categories":["python小记"]},{"title":"python小记-shell","url":"/2022/11/06/python%E5%B0%8F%E8%AE%B0/python%E5%B0%8F%E8%AE%B0-shell/","content":"运行命令行命令os.system仅仅在一个子终端运行系统命令，而不能获取命令执行后的返回信息。如果在命令行下执行，结果直接打印出来。\nos.system(&#x27;ls&#x27;)# 04101419778.CHM   bash      document    media      py-django   video# 11.wmv            books     downloads   Pictures  python# all-20061022      Desktop   Examples    project    tools\nos.popen该方法不但执行命令还返回执行后的信息对象，好处在于：将返回的结果赋给一变量，便于程序处理。\nimport ostmp = os.popen(&#x27;ls *.py&#x27;).readlines()# [&#x27;dump_db_pickle.py &#x27;,# &#x27;dump_db_pickle_recs.py &#x27;,# &#x27;dump_db_shelve.py &#x27;,# &#x27;initdata.py &#x27;,# &#x27;__init__.py &#x27;,# &#x27;make_db_pickle.py &#x27;,# &#x27;make_db_pickle_recs.py &#x27;,# &#x27;make_db_shelve.py &#x27;,# &#x27;peopleinteract_query.py &#x27;,# &#x27;reader.py &#x27;,# &#x27;testargv.py &#x27;,# &#x27;teststreams.py &#x27;,# &#x27;update_db_pickle.py &#x27;,# &#x27;writer.py &#x27;]\nsubprocessimport subprocesssubprocess.call ([&quot;cmd&quot;, &quot;arg1&quot;, &quot;arg2&quot;],shell=True)# [&#x27;python3&#x27;,&#x27;-m&#x27;,&#x27;nbconvert&#x27;,&#x27;assessing.ipynb&#x27;]# python3 -m nbconvert assessing.ipynb \n获取返回和输出:\nimport subprocessp = subprocess.Popen(&#x27;ls&#x27;, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)for line in p.stdout.readlines():    print(line)retval = p.wait()\nsubprocess.call如果命令执行错误不会抛出异常(经测试在linux系统上执行错误命令也会有异常错误，在pycharm上面则不抛出异常)，而subprocess.check_call如果命令执行错误则抛出异常，如果命令执行正确则两者效果一样.\nFileNotFoundError: [Errno 2] No such file or directory -&gt; shell= True\n设置工作目录subprocess模块中的其他方法都有一个cwd参数。此参数确定要执行进程的工作目录。\nret = subprocess.run(&#x27;ls&#x27;, shell=True, cwd=&#x27;/xxx/xxx&#x27;)\npython3 不支持commands。\npython执行shell命令时报错： -/bin/sh: 命令:not found的解决办法1 IDE原因用pycharm一直报错，结果发现用命令行运行python xx.py即可。\n2 没有加入环境变量参考链接\n\n在python中运行命令行命令的四种方案\nhttp://30daydo.com/article/519\n\n","categories":["python小记"]},{"title":"Bonuses","url":"/2022/02/18/%E9%A2%98%E8%A7%A3/%E9%A2%98%E8%A7%A3-Bonuses/","content":"BonusesJohn wants to give a total bonus of $851 to his three employees taking fairly as possible into account their number of days of absence during the period under consideration. Employee A was absent 18 days, B 15 days, and C 12 days.\nThe more absences, the lower the bonus …\nHow much should each employee receive? John thinks A should receive $230, B $276, C $345 since 230 * 18 = 276 * 15 = 345 * 12 and 230 + 276 + 345 = 851.\nTaskGiven an array arr (numbers of days of absence for each employee) and a number s (total bonus) the function bonus(arr, s) will follow John’s way and return an array of the fair bonuses of all employees in the same order as their numbers of days of absences.\ns and all elements of arr are positive integers.\nExamplesbonus([18, 15, 12], 851) -&gt; [230, 276, 345]bonus([30, 27, 8, 14, 7], 34067) -&gt; [2772, 3080, 10395, 5940, 11880]\nNotes\nSee Example Test Cases for more examples.\nPlease ask before translating.\nIn some tests the number of elements of arr can be big.\n\nAnswerdef bonus(arr, s):    # your code    s=s/(sum(1/n for n in arr))    return [round(s/n) for n in arr]  def bonus2(arr, s):    ls=[]    d=sum([1/x for x in arr])    for i in arr:        ls.append(s/i/d)    return list(map(round, ls))\nTimeerror or Overflowerror\ndef bonus(arr, s):    mul = reduce(lambda x, y:x * y, arr)    denominator = 0    for i in arr:        denominator = denominator + mul//i     ls=[]    for i in arr:        ls.append(mul*s/i/denominator)    return list(map(round,ls))\nSomething else- inverse variationIn Maths, inverse variation is the relationships between variables that are represented in the form of y = k/x, where x and y are two variables and k is the constant value. It states if the value of one quantity increases, then the value of the other quantity decreases.\n\n\n- Overflowerror：integer division result too large for a floatIn Python 3, number / 10 will try to return a float. However, floating point values can’t be of arbitrarily large size in Python and if number is large an OverflowError will be raised.\nYou can find the maximum that Python floating point values can take on your system using the sys module:\n&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.float_info.max1.7976931348623157e+308\nTo get around this limitation, instead use // to get an integer back from the division of the two integers:\nnumber // 10\nThis will return the int floor value of number / 10 (it does not produce a float). Unlike floats, int values can be as large as you need them to be in Python 3 (within memory limits).\n- cyc（Cyclic Sum）\n\\sum _{cyc}f(x_1,x_2,\\cdots,x_n)=f(x_1,x_2,\\cdots,x_{n-1},x_n)+f(x_2,x_3,\\cdots,x_{n},x_1)+\\cdots +f(x_n,x_1,\\cdots,x_{n-2},x_{n-1})\\\\\n\\sum_{cyc}{f(a,b,c)}=f(a,b,c)+f(b,c,a)+f(c,a,b)- sym（Symmetric Sum）\n\\sum _{sym}f(x_1,x_2,\\cdots,x_n)=\\sum _{\\sigma}f(x_{\\sigma{(1)}},x_{\\sigma{(2)}},\\cdots,x_{\\sigma{(n)}})\n\\\\ \\sum\\limits_{sym}{f(a,b,c)}=f(a,b,c)+f(b,c,a)+f(c,a,b)+f(b,a,c)+f(c,b,a)+f(a,c,b)where $\\sigma$ is the permutation of $(1,2,…,n)$.\n","categories":["题解"]},{"title":"Look and say numbers","url":"/2022/02/19/%E9%A2%98%E8%A7%A3/%E9%A2%98%E8%A7%A3-Look-and-say-numbers/","content":"Look and say numbersThere exists a sequence of numbers that follows the pattern\n     1    11    21   1211  111221  312211 131122211113213211     .     .     .\nStarting with “1” the following lines are produced by “saying what you see”, so that line two is “one one”, line three is “two one(s)”, line four is “one two one one”.\nWrite a function that given a starting value as a string, returns the appropriate sequence as a list. The starting value can have any number of digits. The termination condition is a defined by the maximum number of iterations, also supplied as an argument.\nExampleexpected = [&#x27;11&#x27;, &#x27;21&#x27;, &#x27;1211&#x27;, &#x27;111221&#x27;, &#x27;312211&#x27;, &#x27;13112221&#x27;, &#x27;1113213211&#x27;, &#x27;31131211131221&#x27;, &#x27;13211311123113112211&#x27;, &#x27;11131221133112132113212221&#x27;]result = look_and_say(&#x27;1&#x27;, 10)test.assert_equals(result, expected)expected = [&#x27;111312&#x27;, &#x27;31131112&#x27;, &#x27;1321133112&#x27;, &#x27;11131221232112&#x27;, &#x27;31131122111213122112&#x27;, &#x27;13211321223112111311222112&#x27;, &#x27;1113122113121122132112311321322112&#x27;, &#x27;311311222113111221221113122112132113121113222112&#x27;]result = look_and_say(&#x27;132&#x27;, 8)test.assert_equals(result, expected)\nAnswer 1 - groupbyfrom itertools import groupbydef look_and_say(data=&#x27;1&#x27;, maxlen=5):    L = []    for i in range(maxlen):        data = &quot;&quot;.join(str(len(list(g)))+str(n) for n, g in groupby(data))        L.append(data)    return L\nfrom itertools import groupbydef look_and_say(data=&#x27;1&#x27;, maxlen=5):    data = &quot;&quot;.join(str(len(list(n))) + c for c, n in groupby(data))    return [data] + look_and_say(data, maxlen - 1) if maxlen else []\nAnswer 2 - two pointersdef say(string):  current, count, res = string[0], 0, &#x27;&#x27;  for char in string:    if char == current: count += 1    else:      res += str(count) + current      current, count = char, 1  res += str(count) + current  return resdef look_and_say(data=&#x27;1&#x27;, maxlen=5):  res = list()  for x in range(maxlen):    if x == 0: res.append(say(data))    else: res.append(say(res[x - 1]))  return res\ndef look_and_say(data=&#x27;1&#x27;, maxlen=5):    last = data[0]    cnt = 1    say = &quot;&quot;    for idx in range(1, len(data)):        if last != data[idx]:            say += str(cnt) + last            last = data[idx]            cnt = 1        else:            cnt += 1    say += str(cnt) + last    if maxlen == 1:        return [say]        return [say] + look_and_say(say, maxlen - 1)\ndef look_and_say(data=&#x27;1&#x27;, maxlen=5):    ls = list(data)    result = []    while maxlen:        num = ls[0]        s = &#x27;&#x27;        cnt = 0        while ls:            if ls[0] == num:                cnt += 1            else:                if cnt !=0:                    s = s + str(cnt) + num                num = ls[0]                cnt = 1            ls.pop(0)        if cnt !=0:            s = s + str(cnt) + num        maxlen -= 1        ls = list(s)        result.append(s)    return result #def look_and_say(data=&#x27;1&#x27;, maxlen=5):#     data_list = list(data)#     result = []#     for i in range(maxlen):#         if len(result) != 0:#             data_list = list(result[-1])#         str1 = 0#         ans = &#x27;&#x27;#         while str1 &lt; len(data_list):#             cnt = 1#             str2 = str1 + 1#             while str2 &lt; len(data_list):#                 if data_list[str1] == data_list[str2]:#                     cnt = cnt + 1#                     str2 = str2 + 1#                 else:#                     break#             ans = ans + str(cnt) + data_list[str1]#             str1 = str2#         result.append(ans)                    #     return result\nAnswer 3 - refrom re import subdef look_and_say(data=&#x27;1&#x27;, maxlen=5):    result = []    for _ in range(maxlen):        data = sub(r&#x27;(.)\\1*&#x27;, lambda m: str(len(m.group(0))) + m.group(1), data)        result.append(data)    return result\nimport redef look_and_say(data=&#x27;1&#x27;, maxlen=5):  l = [data]  for i in range(maxlen):    numstrs = re.findall(&#x27;1+|2+|3+|4+|5+|6+|7+|8+|9+&#x27;, l[i])    s = &#x27;&#x27;    for numstr in numstrs:      s += str(len(numstr)) + numstr[0]    l.append(s)  return l[1:]\nSomething else- itertools.groupby() in PythonPrerequisites: Python Itertools\nPython’s Itertool is a module that provides various functions that work on iterators to produce complex iterators. This module works as a fast, memory-efficient tool that is used either by themselves or in combination to form iterator algebra.\nItertools.groupby()This method calculates the keys for each element present in iterable. It returns key and iterable of grouped items.\nSyntax: itertools.groupby(iterable, key_func)Parameters:iterable: Iterable can be of any kind (list, tuple, dictionary).key: A function that calculates keys for each element present in iterable.Return type: It returns consecutive keys and groups from the iterable. If the key function is not specified or is None, key defaults to an identity function and returns the element unchanged.\nExample 1# Python code to demonstrate# itertools.groupby() methodimport itertoolsL = [(&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;b&quot;, 3), (&quot;b&quot;, 4)]# guess L = [(&quot;a&quot;, 1),(&quot;b&quot;, 3),(&quot;b&quot;, 4),(&quot;a&quot;, 2)] , sort first？# Key functionkey_func = lambda x: x[0]for key, group in itertools.groupby(L, key_func):    print(key + &quot; :&quot;, list(group))\nOutputa : [(&#x27;a&#x27;, 1), (&#x27;a&#x27;, 2)]b : [(&#x27;b&#x27;, 3), (&#x27;b&#x27;, 4)]\nExample 2# Python code to demonstrate# itertools.groupby() methodimport itertoolsa_list = [(&quot;Animal&quot;, &quot;cat&quot;),          (&quot;Animal&quot;, &quot;dog&quot;),          (&quot;Bird&quot;, &quot;peacock&quot;),          (&quot;Bird&quot;, &quot;pigeon&quot;)]an_iterator = itertools.groupby(a_list, lambda x : x[0])for key, group in an_iterator:    key_and_group = &#123;key : list(group)&#125;    print(key_and_group)\nOutput&#123;&#x27;Animal&#x27;: [(&#x27;Animal&#x27;, &#x27;cat&#x27;), (&#x27;Animal&#x27;, &#x27;dog&#x27;)]&#125;&#123;&#x27;Bird&#x27;: [(&#x27;Bird&#x27;, &#x27;peacock&#x27;), (&#x27;Bird&#x27;, &#x27;pigeon&#x27;)]&#125;\n- RE (Regular Expression)YouTube：python基础 36 RegEx 正则表达式 (教学教程tutorial)\n- two pointersBlog：双指针算法模板和一些题目 \n","categories":["题解"]},{"title":"Magnitude","url":"/2022/02/19/%E9%A2%98%E8%A7%A3/%E9%A2%98%E8%A7%A3-Magnitude/","content":"MagnitudeWe will represent complex numbers in either cartesian or polar form as an array/list, using the first element as a type tag; the normal tags are &quot;cart&quot; (for cartesian) and &quot;polar&quot;. The forms of the tags depend on the language.\n\n[‘cart’, 3, 4]represents the complex number3+4i\n[‘polar’, 2, 3]represents the complex number with modulus (or magnitude)2and angle3\n\nIn the same way:\n\n[&#39;cart&#39;, 1, 2, 3, 4, 5, 6]includes the three complex numbers 1+2i, 3+4i, 5+6i\n[&#39;polar&#39;, 2, 1, 2, 2, 2, 3] represents the complex numbers (2, 1), (2, 2), (2, 3) in polar form where the magnitudes are 2, 2, 2 and the angles 1, 2, 3\n\nNote:\nThe polar form of a complex number z = a+bi is z = (r, θ) = r(cosθ+isinθ), where r = |z| = the (non-negative) square-root of a^2+b^2 is the modulus.\nIn the arrays/lists beginning by a tag all terms after the tag must be integers (no floats, no strings).\n\nTaskGiven a sequence of complex numbers z in one of the previous forms we first calculate the sum s of the squared modulus of all complex elements of z if z is in correct form.\nOur function sqr-modulus returns an array/list of three elements; the form of the list will depend on the language:\n\nthe first element is a boolean:\n\n#t or True or true if z is in correct form as defined previously (correct type of numbers, correct tag)\n#f or False or false if z is not in correct form.\n\nthe second element is the sum s of the squared modulus of all complex numbers in z if the returned boolean is true, -1 if it is false.\nthe third element is the greatest number got by rearranging the digits of s. We will admit that the greatest number got from-1 is 1.\n\nExamples (in general form):See the exact form of the return for your language in “RUN SAMPLE TESTS”\nsqr_modulus([&#x27;cart&#x27;, 3, 4]) -&gt; (True , 25, 52)sqr_modulus([&#x27;cart&#x27;, 3, 4, 3, 4]) -&gt; (True , 50, 50)sqr_modulus([&#x27;polar&#x27;, 2531, 3261]) -&gt; (True , 6405961, 9665410)sqr_modulus([&#x27;polar&#x27;, 2, 3, 2, 4]) -&gt; (True , 8, 8)sqr_modulus([&#x27;polar&#x27;, &quot;2&quot;, 3]) -&gt; (False , -1, 1)sqr_modulus([&#x27;polara&#x27;, 2, 3]) -&gt; (False , -1, 1)sqr_modulus([&#x27;cart&#x27;, 3, 4.1]) -&gt; (False , -1, 1)\nNotes:\nRacket: in Racket (integer? 2.0) returns #t\nPascal: The given input is a string; the first substring is the tag; other substrings represent integers or floats or strings. The first element of return is -1 for ‘false’ or 1 for true.\nShell: in this kata an object will be an integer if it contains only digits.\nPerl see “Template Solution”.\n\nAnswer 1def sqr_modulus(z):    # your code    for i in z[1:]:        if isinstance(i, int)!= True:            return (False , -1, 1)        isright = True    sz = 0    if z[0] == &#x27;cart&#x27;:        for i in z[1:]:            sz += i**2        return (True, sz,int(&#x27;&#x27;.join(sorted(str(sz)),reverse=True)))      # return (True, sz,int(&#x27;&#x27;.join(sorted(str(sz)[::-1]))))    elif z[0] == &#x27;polar&#x27;:        for i in z[1::2]:            sz += i**2        return (True, sz,int(&#x27;&#x27;.join(sorted(str(sz)),reverse=True)))    else:        return (False , -1, 1)\nAnswer 2def sqr_modulus(*args):    arr = list(args)[0]    if arr[0] not in (&#x27;cart&#x27;, &#x27;polar&#x27;) or any(type(x) != int for x in arr[1:]):        return False, -1, 1        if arr[0] == &#x27;cart&#x27;:        s = sum(x**2 for x in arr[1:])    elif arr[0] == &#x27;polar&#x27;:        s = sum(x**2 for x in arr[1::2])            return True, s, int(&#x27;&#x27;.join(sorted(str(s))[::-1]))\n","categories":["题解"]},{"title":"Sums of Parts","url":"/2022/02/19/%E9%A2%98%E8%A7%A3/%E9%A2%98%E8%A7%A3-Sums-of-Parts/","content":"Sums of PartsLet us consider this example (array written in general format):\nls = [0, 1, 3, 6, 10]\nIts following parts:\nls = [0, 1, 3, 6, 10]ls = [1, 3, 6, 10]ls = [3, 6, 10]ls = [6, 10]ls = [10]ls = []\nThe corresponding sums are (put together in a list): [20, 20, 19, 16, 10, 0]\nThe function parts_sums (or its variants in other languages) will take as parameter a list ls and return a list of the sums of its parts as defined above.\nOther Examplesls = [1, 2, 3, 4, 5, 6] parts_sums(ls) -&gt; [21, 20, 18, 15, 11, 6, 0]ls = [744125, 935, 407, 454, 430, 90, 144, 6710213, 889, 810, 2579358]parts_sums(ls) -&gt; [10037855, 9293730, 9292795, 9292388, 9291934, 9291504, 9291414, 9291270, 2581057, 2580168, 2579358, 0]\nNotes\nTake a look at performance: some lists have thousands of elements.\nPlease ask before translating.\n\nAnswer 1def parts_sums(ls):    # your code    sum = 0    new_ls = [0]    for i in ls[::-1]:        sum += i         new_ls.append(sum)    return new_ls[::-1]\nAnswer 2def parts_sums(ls):    result = [sum(ls)]    for item in ls:        result.append(result[-1]-item)    return result\nAnswer 3from itertools import accumulatedef parts_sums(ls):    return [0, *accumulate(reversed(ls))][::-1]\nfrom itertools import accumulatedef parts_sums(ls):    return list(accumulate(ls[::-1]))[::-1]+[0]\nSomething else- Python – Itertools.accumulate()Python itertools module is a collection of tools for handling iterators.\nAccording to the official documentation:\n\n“Module [that] implements a number of iterator building blocks inspired by constructs from APL, Haskell, and SML… Together, they form an ‘iterator algebra’ making it possible to construct specialized tools succinctly and efficiently in pure Python.” this basically means that the functions in itertools “operate” on iterators to produce more complex iterators.\n\nSimply put, iterators are data types that can be used in a for loop. The most common iterator in Python is the list.\nLet’s create a list of strings and named it colors. We can use a for loop to iterate the list like:\ncolors = [&#x27;red&#x27;, &#x27;orange&#x27;, &#x27;yellow&#x27;, &#x27;green&#x27;]# Iterating Listfor each in colors:    print(each)\nOutputredorangeyellowgreen\nThere are many different kinds of iterables but for now, we will be using lists and sets.\nRequirements to work with itertoolsMust import the itertools module before using. We have to also import the operator module because we want to work with operators.\nimport itertoolsimport operator ## only needed if want to play with operators\nItertools module is a collection of functions. We are going to explore one of these accumulate() function.\nNote: For more information, refer to Python Itertools\naccumulate()This iterator takes two arguments, iterable target and the function which would be followed at each iteration of value in target. If no function is passed, addition takes place by default. If the input iterable is empty, the output iterable will also be empty.\n\nSyntaxitertools.accumulate(iterable[, func]) –&gt; accumulate object\n\nThis function makes an iterator that returns the results of a function.\nParametersiterable &amp; function\nNow its enough of the theory portion lets play with the code\nCode 1# import the itertool module# to work with itimport itertools# import operator to work# with operatorimport operator# creating a list GFGGFG = [1, 2, 3, 4, 5]# using the itertools.accumulate()result = itertools.accumulate(GFG, operator.mul)# printing each item from listfor each in result:    print(each)\nOutput12624120\nExplanation The operator.mul takes two numbers and multiplies them.\noperator.mul(1, 2)2operator.mul(2, 3)6operator.mul(6, 4)24operator.mul(24, 5)120\nNow in the next example, we will use the max function as it takes a function as a parameter also.\nCode 2# import the itertool module# to work with itimport itertools# import operator to work with# operatorimport operator# creating a list GFGGFG = [5, 3, 6, 2, 1, 9, 1]# using the itertools.accumulate()# Now here no need to import operator# as we are not using any operator# Try after removing it gives same resultresult = itertools.accumulate(GFG, max)# printing each item from listfor each in result:    print(each)\nOutput5566699\nExplanation5max(5, 3)5max(5, 6)6max(6, 2)6max(6, 1)6max(6, 9)9max(9, 1)9\nNote: The passing function is optional as if you will not pass any function items will be summed i.e. added by default.\nitertools.accumulate(set.difference)This return accumulate of items of difference between sets.\nCode to explain\n# import the itertool module to# work with itimport itertools# creating a set GFG1 and GFG2GFG1 = &#123; 5, 3, 6, 2, 1, 9 &#125;GFG2 =&#123; 4, 2, 6, 0, 7 &#125;# using the itertools.accumulate()# Now this will first give difference# and the give result by adding all# the element in result as by default# if no function passed it will add alwaysresult = itertools.accumulate(GFG2.difference(GFG1))# printing each item from listfor each in result:    print(each)\nOutput:\n0411\nsomething else前缀函数与 KMP 算法\n前缀和 &amp; 差分\n","categories":["题解"]},{"title":"Ugly Number","url":"/2022/02/18/%E9%A2%98%E8%A7%A3/%E9%A2%98%E8%A7%A3-Ugly-Number/","content":"Definition\n$ugly$ $number$：把只包含质因子 $2$，$3$ 和 $5$ 的数称作丑数（$Ugly$  $Number$）。例如 $6$、 $8$ 都是丑数，但 $7$ 、$14$ 不是，因为它们包含质因子 $7$ 。 习惯上我们把 $1$ 当做是第一个丑数。\n\n$humble$ $number$：对于一给定的素数集合 $S = {p_1, p_2, …, p_K}$ , 考虑一个正整数集合，该集合中任一元素的质因数全部属于 $S$ 。这个正整数集合包括，$p_1、p_1p_2、p_1p_1、p_1p_2p_3…$ 。该集合被称为 $S$ 集合的 “丑数集合” 。\n\n\nLeetCode 263. Ugly NumberAn ugly number is a positive integer whose prime factors are limited to 2, 3, and 5.\nGiven an integer n, return true if n is an ugly number.\nExample 1Input: n = 6Output: trueExplanation: 6 = 2 × 3\nExample 2Input: n = 1Output: trueExplanation: 1 has no prime factors, therefore all of its prime factors are limited to 2, 3, and 5.\nExample 3Input: n = 14Output: falseExplanation: 14 is not ugly since it includes the prime factor 7. \nConstraints\n$-2^{31} &lt;= n &lt;= 2^{31} - 1$\n\nAnswer 1来源：LeetCode-Solution\n根据丑数的定义，0 和负整数一定不是丑数。\n当 $n&gt;0$ 时，若 $n$ 是丑数，则 $n$ 可以写成 $n = 2^a \\times 3^b \\times 5^c $ 的形式，其中 $a,b,c$ 都是非负整数。特别地，当 $a,b,c$ 都是 $0$ 时，$n=1$。\n为判断 n 是否满足上述形式，可以对 n 反复除以 2,3,5 ，直到 nn 不再包含质因数 2,3,5。若剩下的数等于 1，则说明 nn 不包含其他质因数，是丑数；否则，说明 n 包含其他质因数，不是丑数。\nclass Solution:    def isUgly(self, n: int) -&gt; bool:        if n &lt;= 0:            return False        factors = [2, 3, 5]        for factor in factors:            while n % factor == 0:                n //= factor                return n == 1\n复杂度分析时间复杂度：$O(\\log n)$。时间复杂度取决于对 $n$ 除以 $2,3,5$ 的次数，由于每次至少将 $n$ 除以 $2$，因此除法运算的次数不会超过 $O(\\log n)$。\n空间复杂度：$O(1)$。\nAnswer 2class Solution:    def isUgly(self, n: int) -&gt; bool:        if n &lt;= 0:            return False        while n % 2 == 0:            n //= 2        while n % 3 == 0:            n //= 3        while n % 5 == 0:            n //= 5        return n == 1\nLeetCode 264. Ugly Number II / 剑指 Offer 49. 丑数 LCOFAn ugly number is a positive integer whose prime factors are limited to 2, 3, and 5.\nGiven an integer n, return the nth ugly number.\nExample 1Input: n = 10Output: 12Explanation: [1, 2, 3, 4, 5, 6, 8, 9, 10, 12] is the sequence of the first 10 ugly numbers.\nExample 2Input: n = 1Output: 1Explanation: 1 has no prime factors, therefore all of its prime factors are limited to 2, 3, and 5.\nConstraints\n$1 &lt;= n &lt;= 1690$\n\nAnswer来源：剑指 Offer 49. 丑数（动态规划，清晰图解）\nclass Solution:    def nthUglyNumber(self, n: int) -&gt; int:        dp, a, b, c = [1] * n, 0, 0, 0        for i in range(1, n):            n2, n3, n5 = dp[a] * 2, dp[b] * 3, dp[c] * 5            dp[i] = min(n2, n3, n5)            if dp[i] == n2: a += 1            if dp[i] == n3: b += 1            if dp[i] == n5: c += 1        return dp[-1]\n","categories":["题解"]},{"title":"pyschools while 9-13","url":"/2022/01/24/%E9%A2%98%E8%A7%A3/%E9%A2%98%E8%A7%A3-pyschools%E4%B8%ADwhile-9-13/","content":"09 Square Root Approximationlink：http://www.pyschools.com/quiz/view_question/s5-q9\nCreate a function that takes in a positive number and return 2 integers such that the number is between the squares of the 2 integers. It returns the same integer twice if the number is a square of an integer.\nExamples&gt;&gt;&gt; sqApprox(2)(1, 2)&gt;&gt;&gt; sqApprox(4)(2, 2)&gt;&gt;&gt; sqApprox(5.1)(2, 3)\nAnswer 1import mathdef sqApprox(num):    return (math.floor(num**0.5),math.ceil(num**0.5))\nAnswer 2def sqApprox(num):    i = 0    minsq = 0             # set lower bound    maxsq = int(num) + 1  # set upper bound    while i &lt; maxsq:        if i * i &lt;= num and i &gt;= minsq:  # complete inequality condition            minsq = i        if i * i &gt;= num and i &lt;= maxsq:  # complete inequality condition            maxsq = i        i += 1                           # update i so that &#x27;while&#x27; will terminate    return (minsq, maxsq)\n10 Pi Approximationlink:http://www.pyschools.com/quiz/view_question/s5-q10\nCreate a function that computes the approximation of pi, based on the number of iterations specified.\npi can be computed by 4*(1-1/3+1/5-1/7+1/9- ...).\nExamples&gt;&gt;&gt; piApprox(1)4.0&gt;&gt;&gt; piApprox(10)3.04183961893&gt;&gt;&gt; piApprox(300)3.13825932952\nAnswerdef piApprox(num):    i = 1    pi = 0    while i &lt;= num:        pi += 4 * (-1) ** (i + 1) * (1.0 / (2 * i - 1))        i += 1    return pi\n11 Estimate Pilink:http://www.pyschools.com/quiz/view_question/s5-q11\nWrite a function estimatePi() to estimate and return the value of pi based on the formula found by an Indian Mathematician Srinivasa Ramanujan. It should use a while loop to compute the terms of the summation until the last item is smaller than 1e -15. The formula for calculating distance is given below:\n\n\nExamples&gt;&gt;&gt; estimatePi()3.14159265359\nAnswerdef estimatePi():    import math    def factorial(n):        if n == 0:            return 1        else:            return n * factorial(n - 1)    item = 1103    k = 0    pi = 0    while item &gt; 1e-15:        item = (            (factorial(4 * k) * (1103.0 + 26390 * k))            / factorial(k) ** 4            / 396 ** (4 * k))        pi += 2 * 2 ** (0.5) / 9801 * item        k += 1    return 1 / pi\n12 Prime Factorizationlink:http://www.pyschools.com/quiz/view_question/s5-q12\nGiven a positive integer, write a function that computes the prime factors that can be multplied together to get back the same integer.\nExamples&gt;&gt;&gt; primeFactorization(60)[2, 2, 3, 5]&gt;&gt;&gt; primeFactorization(1050)[2, 3, 5, 5, 7]&gt;&gt;&gt; primeFactorization(1)[]\nAnswer 1def primeFactorization4(num):    def prime(num):        if num &lt; 2:            return 0        for i in range(2, int(num ** 0.5) + 1):            if num % i == 0:                return 0        else:            return 1    ls = []    start = 2    while prime(num) == 0 and num &gt; 1:        for i in range(start, int(num ** 0.5) + 1):          #if num % i == 0 and prime(i):            if num % i == 0:                ls.append(i)                num = num // i                start = i                break    if prime(num):        ls.append(num)    return ls\nAnswer 2 **def primeFactorization(num):    possible = [2] + list(range(3, int(num ** 0.5) + 1, 2))    for p in possible:        if num % p == 0:            return [p] + primeFactorization(num / p)    if num &lt; 2:        return []    return [num]\nAnswer 3def primeFactorization(num):     factor=[]    while num&gt;1:        for i in range(2,num+1):            if num%i==0:                factor.append(i)                num = num//i                break    return factor\nTime complexity comparisonimport timedef primeFactorization1(num):    def prime(num):        if num &lt; 2:            return 0        for i in range(2, int(num ** 0.5) + 1):            if num % i == 0:                return 0        else:            return 1    ls = []    start = 2    while prime(num) == 0 and num &gt; 1:        for i in range(start, int(num ** 0.5) + 1):            if num % i == 0 and prime(i):                ls.append(i)                num = num // i                start = i                break    if prime(num):        ls.append(num)    return lsdef primeFactorization2(num):    possible = [2] + list(range(3, int(num ** 0.5) + 1, 2))    for p in possible:        if num % p == 0:            return [p] + primeFactorization2(num / p)    if num &lt; 2:        return []    return [num]for c in [100, 1e6, 1e10, 1e14]:    print(c)    t0 = time.time()    primeFactorization1(c)    t1 = time.time()    print(&#x27;%.10f&#x27; % (t1 - t0))    primeFactorization2(c)    t2 = time.time()    print(&#x27;%.10f&#x27; % (t2 - t1))# 100# 0.0000226498# 0.0000531673# 1e6# 0.0000638962# 0.0000970364# 1e10# 0.0000720024# 0.0017700195# 1e14# 0.0000679493# 0.2322161198\n\nleetcode:[剑指 Offer 49. 丑数]\n\nleetcode:[263. Ugly Number]\n\nleetcode:[264. Ugly Number II]\n\nleetcode:[1201. Ugly Number III]\n\nleetcode:[507. Perfect Number]\n\nleetcode:[1390. Four Divisors]\n\n\n13 Lowest Common Multiplelink:http://www.pyschools.com/quiz/view_question/s5-q13\nThe smallest common multiple of two or more numbers is called the lowest common multiple (LCM). Given a list of integers, find the lowest common multiple.\nExamples&gt;&gt;&gt; LCM([2, 3, 4])12&gt;&gt;&gt; LCM([3, 6, 9])18&gt;&gt;&gt; LCM([3, 3])3\nAnswer 1def LCM(nums):     def lcm(a, b):        c = a        while True:            if c % a == 0 and c % b == 0:                return c                break            c += 1    l=lcm(nums[0],nums[1])    for i in nums[2:]:        l=lcm(l,i)    return l\nAnswer 2from math import gcd # Python versions 3.5 and above#from fractions import gcd # Python versions below 3.5from functools import reduce # Python version 3.xdef LCM(denominators):    return reduce(lambda a,b: a*b // gcd(a,b), denominators)\n\nfunctools.``reduce(function, iterable[, initializer])\n\nApply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5). The left argument, x, is the accumulated value and the right argument, y, is the update value from the iterable. If the optional initializer is present, it is placed before the items of the iterable in the calculation, and serves as a default when the iterable is empty. If initializer is not given and iterable contains only one item, the first item is returned.Roughly equivalent to:\ndef reduce(function, iterable, initializer=None):    it = iter(iterable)    if initializer is None:        value = next(it)    else:        value = initializer    for element in it:        value = function(value, element)    return value\nSee itertools.accumulate() for an iterator that yields all intermediate values.\n\nAs of Python 3.9 lcm() function has been added in the math library. It can be called with the following signature:\n\nmath.lcm(*integers)\n\nReturn the least common multiple of the specified integer arguments. If all arguments are nonzero, then the returned value is the smallest positive integer that is a multiple of all arguments. If any of the arguments is zero, then the returned value is 0. lcm() without arguments returns 1.\n\nAnswer 3def LCM(nums):  def gcd(n, m):      if m == 0:          return n      return gcd(m, n % m)  lcm = 1  for i in nums:      lcm = lcm * i // gcd(lcm, i)  return lcm\nAnswer 4import numpy as npnp.lcm.reduce([40, 12, 20])\n","categories":["题解"]},{"title":"linux安装miniconda和python","url":"/2022/10/22/linux/linux%20%E5%AE%89%E8%A3%85miniconda%E5%92%8Cpython/","content":"安装minicondastep 1: 获取安装shell脚本文件wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.shwget https://repo.continuum.io/archive/Anaconda3-2020.07-Linux-x86_64.shwget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2020.07-Linux-x86_64.sh wget https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh\nstep 2: 执行脚本文件安装bash Miniconda3-latest-Linux-x86_64.sh\nstep 3: 激活source ~/.bashrc\nconda create -n jjenv python=3.6 -c https://mirrors.tuna.tsinghua.edu.cn/[anaconda](https://so.csdn.net/so/search?q=anaconda&amp;spm=1001.2101.3001.7020)/pkgs/free/\n安装镜像源#查看当前conda配置conda config --show channels#设置通道conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/#设置搜索是显示通道地址conda config --set show_channel_urls yesconda install pytorch torchvision cudatoolkit=10.0  # 删除安装命令最后的 -c pytorch，才会采用清华源安装。# 删除源conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/tensorflow/linux/cpu/\n将anaconda的python设置成环境变量1.修改~/.bashrc\n2.添加export PATH=/home/lishanliao/anaconda3/bin:$PATH\n3.source ~/.bashrc\n然后输入python得到显示\n如果不是主用户\n\nvim .bash_profile\n\n添加export PATH=/home/lishanliao/anaconda3/bin:$PATH（esc后:wq退出）\n\n\nsource ~/.bashrc\n\n\n虚拟环境# 创建conda create -n env python=3.6 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/# 进入conda activate env# 退出conda deactivate\n 使用pycharm同步本地与服务器代码\nhttps://blog.51cto.com/u_15080021/4201546\npython保存虚拟环境requirement.txt保存\npip freeze &gt; requirement.txt\n安装\npip install -r requirement.txt\n安装requirements.txt文件的两种方式（pip与conda）PIP\npip install -r requirements.txt\nCONDA\nwhile read requirement; do conda install --yes $requirement; done &lt; requirements.txtconda install --yes --file requirements.txt\n【编程工具】Pycharm 开启远程SSH Terminalhttps://blog.csdn.net/j790675692/article/details/52798953\n","categories":["linux"]},{"title":"linux常见命令","url":"/2022/11/06/linux/linux%20%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/","content":"删除文件返回上级 \ncd ..\n删除当前目录下的文件\nrm -f *\n删除指定目录下的文件\nrm -f 指定目录*\n删除文件夹和文件的命令\n-r 就是向下递归，不管有多少级目录，一并删除-f 就是直接强行删除，不作任何提示的意思删除文件夹实例：rm -rf /var/log/httpd/access将会删除/var/log/httpd/access目录以及其下所有文件、文件夹删除文件使用实例：rm -f /var/log/httpd/access.log将会强制删除/var/log/httpd/access.log这个文件\n编辑文件后保存退出按ESC进入Command模式输入 :wq 后回车即可。\n查看cuda版本nvidia-sminvcc -Vcat /usr/local/cuda/version.txt\n详见：ubuntu 之 查看 cuda，cudnn版本\n安装gcc 、g++ /gfortran编译器\ngcc\n\n\nubuntu下自带gcc编译器。\n\n通过gcc -v命令来查看是否安装。\n\n\n\ng++\n\n\n安装命令：sudo apt-get install build-essentialbuild-essential是一整套工具，gcc，libc等等。\n\n通过 g++ -v可以查看g++是否安装成功。\n\n\n注：“sudo apt-get install build-essential —fix-missing”，这个命令是修补安装build-essential，即已安装了部分build-essential，但没有安装完全，此时可以使用该命令继续安装build-essential。\n\ngfortran\n\n\n安装命令：sudo apt-get install gfortran\n通过gfortran -v可以查看gfortran是否安装成功。\n\nLinux下安装gcc 、g++ /gfortran编译器,mac\n远程连接screen命令# 创建窗口screen -S name# 执行任务....# 查看远程服务器存在的screenscreen -ls# 重连screen -r name# 断开当前screenCtrl + A + Kscreen -S name -X quit\n","categories":["linux"]},{"title":"linux安装pytorch","url":"/2022/11/06/linux/linux%E5%AE%89%E8%A3%85pytorch/","content":"注意⚠️：\n\ntorch，torchvision，torchaudio和python版本需要对应\n安装的torch要与gpu对应（可以向下兼容）\n反复安装要重启能解决一些问题\n\n下载速度太慢添加镜像源（清华、科大），在官网下载时去掉命令后面的 -c pytorchconda install pytorch torchvision cudatoolkit=9.0# 无gpuconda install pytorch torchvision cpuonly\n下载对应的whl文件（也可以先下好）pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl\n在清华源里找对应的bz2（也可以先下好）conda install + 链接\n详见：conda安装Pytorch下载过慢解决办法(11月26日更新ubuntu下pytorch1.3安装方法)\ntorch，torchvision，torchaudio和python版本需要对应报错：from torch._C import  ImportError:*torch/undefined symbol: Py│Slice_Unpack\n详见： ImportError: /home/gp/………/torch/lib/libtorch_python.so: undefined symbol: PySlice_Unpack\n各版本对应关系：官网\n安装的torch要与gpu、算力对应（可以向下兼容）CUDA error: no kernel image is available for execution on the device解决办法：安装老版本pytorch \n算力对应版本详见：pytorch 报错 RuntimeError: CUDA error: no kernel image is available for execution on the device\n","categories":["linux"]},{"title":"four 7 kyu in codewars","url":"/2022/02/24/%E9%A2%98%E8%A7%A3/%E9%A2%98%E8%A7%A3-four%207%20kyu%20in%20codewars/","content":"Is n divisible by (…)?Create a function isDivisible(n,...) that checks if the first argument n is divisible by all other arguments (return true if no other arguments)\nExampleisDivisible(6,1,3)--&gt; true because 6 is divisible by 1 and 3isDivisible(12,2)--&gt; true because 12 is divisible by 2isDivisible(100,5,4,10,25,20)--&gt; trueisDivisible(12,7)--&gt; false because 12 is not divisible by 7\nThis kata is following kata: http://www.codewars.com/kata/is-n-divisible-by-x-and-y\nAnswerdef is_divisible(*arg):    ls = list(arg)    for i in ls[1:]:        if ls[0] % i == 0 :            pass        else:            return False    return True\ndef is_divisible(n, *args):    return all(not n % i for i in args)\ndef is_divisible(n, *args):  return all(n % a == 0 for a in args)\nDescending OrderYour task is to make a function that can take any non-negative integer as an argument and return it with its digits in descending order. Essentially, rearrange the digits to create the highest possible number.\nExamplesInput: 42145 Output: 54421\nInput: 145263 Output: 654321\nInput: 123456789 Output: 987654321\nAnswerdef descending_order(num):    # Bust a move right here    return int(&#x27;&#x27;.join(sorted(str(num))[::-1]))\ndef Descending_Order(num):    if isinstance(num, int) and num &gt;= 0:        return int(&#x27;&#x27;.join(sorted(str(num),reverse=True)))    else:        raise ValueError(&#x27;Non-negative integer expected&#x27;)\nWorking with arrays II (and why your code fails in some katas)n this kata the function returns an array/list like the one passed to it but with its nth element removed (with 0 &lt;= n &lt;= array/list.length - 1). The function is already written for you and the basic tests pass, but random tests fail. Your task is to figure out why and fix it.\nGood luck!\nSome good reading: Python Docs about lists\nAnswerdef remove_nth_element(lst, n):    # Fix it    lst_copy = lst.copy()    #import copy    #copy.deepcopy([1,2,3,4,4])    del lst_copy[n]    return lst_copy\ndef remove_nth_element(a, n):    return a[:n] + a[n+1:]\ndef remove_nth_element(lst, n):    # Fix it    lst_copy = lst[:]    del lst_copy[n]    return lst_copy\nsomething to read : https://www.codewars.com/kumite/5a7b4db7fd5777bf81000016?sel=5a84b63afd5777e6520001b8\nuser_ans = remove_nth_element(test_lst, test_n)my_ans = remove_nth_element_wYkP(test_lst, test_n)\nRobinson CrusoeRobinson Crusoe decides to explore his isle. On a sheet of paper he plans the following process.\nHis hut has coordinates origin = [0, 0]. From that origin he walks a given distance d on a line that has a given angle ang with the x-axis. He gets to a point A. (Angles are measured with respect to the x-axis)\nFrom that point A he walks the distance d multiplied by a constant distmult on a line that has the angle ang multiplied by a constant angmult and so on and on.\nWe have d0 = d, ang0 = ang; then d1 = d * distmult, ang1 = ang * angmult etc …\nLet us suppose he follows this process n times. What are the coordinates lastx, lasty of the last point?\nThe function crusoe has parameters;\n\nn : numbers of steps in the process\nd : initial chosen distance\nang : initial chosen angle in degrees\ndistmult : constant multiplier of the previous distance\nangmult : constant multiplier of the previous angle\n\ncrusoe(n, d, ang, distmult, angmult) should return lastx, lasty as an array or a tuple depending on the language.\nExample:crusoe(5, 0.2, 30, 1.02, 1.1) -&gt;\nThe successive x are : 0.0, 0.173205, 0.344294, 0.511991, 0.674744, 0.830674 (approximately)\nThe successive y are : 0.0, 0.1, 0.211106, 0.334292, 0.47052, 0.620695 (approximately)\nand\nlastx: 0.8306737544381833lasty: 0.620694691344071\nA drawingSuccessive points:\n\nx: 0.0, 0.9659..., 1.8319..., 2.3319..., 1.8319...\ny: 0.0, 0.2588..., 0.7588..., 1.6248..., 2.4908...\n\nNotePlease could you ask before translating?\nAnswerimport mathdef crusoe(n, d, ang, dist_mult, ang_mult):    # your code    lastx = 0    lasty = 0    ang = math.radians(ang)    for i in  range(1,n+1):        lasty += math.sin(ang) * d        lastx += math.cos(ang) * d        d = d * dist_mult        ang = ang * ang_mult    return lastx, lasty\nimport mathdef crusoe(n, d, ang, dist_mult, ang_mult):    x, y = 0, 0    for i in range(n):        x += d*dist_mult**i*math.cos(math.radians(ang*ang_mult**i))        y += d*dist_mult**i*math.sin(math.radians(ang*ang_mult**i))    return x, y\n","categories":["题解"]},{"title":"双指针","url":"/2022/03/04/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8F%8C%E6%8C%87%E9%92%88/","content":"双指针双指针，指的是在遍历对象的过程中，不是普通的使用单个指针进行访问，而是使用两个相同方向（快慢指针）或者相反方向（对撞指针）的指针进行扫描，从而达到相应的目的。\n换言之，双指针法充分使用了数组有序这一特征，从而在某些情况下能够简化一些运算。\n双向双指针/对撞指针881. Boats to Save PeopleYou are given an array people where people[i] is the weight of the ith person, and an infinite number of boats where each boat can carry a maximum weight of limit. Each boat carries at most two people at the same time, provided the sum of the weight of those people is at most limit.\nReturn the minimum number of boats to carry every given person.\nExample 1:\nInput: people = [1,2], limit = 3Output: 1Explanation: 1 boat (1, 2)\nExample 2:\nInput: people = [3,2,2,1], limit = 3Output: 3Explanation: 3 boats (1, 2), (2) and (3)\nExample 3:\nInput: people = [3,5,3,4], limit = 5Output: 4Explanation: 4 boats (3), (3), (4), (5)\nConstraints:\n\n1 &lt;= people.length &lt;= 5 * 104\n1 &lt;= people[i] &lt;= limit &lt;= 3 * 104\n\nAnswerclass Solution:    def numRescueBoats(self, people: List[int], limit: int) -&gt; int:        #如果最重的，重到连最轻的都加不上去，那它只能自己一个人一条船。        ans = 0        people.sort()        light, heavy = 0, len(people) - 1        while light &lt;= heavy:            if people[light] + people[heavy] &gt; limit:                heavy -= 1            else:                light += 1                heavy -= 1            ans += 1        return ans\n同向双指针/快慢指针快慢指针也是双指针，但是两个指针从同一侧开始遍历数组，将这两个指针分别定义为快指针（fast）和慢指针（slow），两个指针以不同的策略移动，直到两个指针的值相等（或其他特殊条件）为止，如fast每次增长两个，slow每次增长一个。\n141. Linked List CycleGiven head, the head of a linked list, determine if the linked list has a cycle in it.\nThere is a cycle in a linked list if there is some node in the list that can be reached again by continuously following the next pointer. Internally, pos is used to denote the index of the node that tail’s next pointer is connected to. Note that pos is not passed as a parameter.\nReturn true if there is a cycle in the linked list. Otherwise, return false.\n\nExample 1:\n\nInput: head = [3,2,0,-4], pos = 1Output: trueExplanation: There is a cycle in the linked list, where the tail connects to the 1st node (0-indexed).\nExample 2:\n\nInput: head = [1,2], pos = 0Output: trueExplanation: There is a cycle in the linked list, where the tail connects to the 0th node.\nExample 3:\n\nInput: head = [1], pos = -1Output: falseExplanation: There is no cycle in the linked list.\nConstraints:\n\nThe number of the nodes in the list is in the range [0, 104].\n-105 &lt;= Node.val &lt;= 105\npos is -1 or a valid index in the linked-list.\n\nAnswer# Definition for singly-linked list.# class ListNode:#     def __init__(self, x):#         self.val = x#         self.next = Noneclass Solution:    def hasCycle(self, head: ListNode) -&gt; bool:        # 空链表或链表只有一个节点，无环        if not head or head.next == None:            return False        # 初始化快慢指针        fast = slow = head        # 如果不存在环，肯定 fast 先指向 null        # 细节：fast 每次走 2 步，所以要确定 fast 和 fast.next 不为空，不然会报执行出错。        while fast and fast.next:            # 快指针移动 2 步，慢指针移动 1 步            fast = fast.next.next            slow = slow.next            # 快慢指针相遇，有环            if fast == slow:                return True        return False\n26. Remove Duplicates from Sorted ArrayGiven an integer array nums sorted in non-decreasing order, remove the duplicates in-place such that each unique element appears only once. The relative order of the elements should be kept the same.\nSince it is impossible to change the length of the array in some languages, you must instead have the result be placed in the first part of the array nums. More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.\nReturn k after placing the final result in the first k slots of nums.\nDo not allocate extra space for another array. You must do this by modifying the input array in-place with O(1) extra memory.\nCustom Judge:\nThe judge will test your solution with the following code:\nint[] nums = [...]; // Input arrayint[] expectedNums = [...]; // The expected answer with correct lengthint k = removeDuplicates(nums); // Calls your implementationassert k == expectedNums.length;for (int i = 0; i &lt; k; i++) &#123;    assert nums[i] == expectedNums[i];&#125;\nIf all assertions pass, then your solution will be accepted.\nExample 1:\nInput: nums = [1,1,2]Output: 2, nums = [1,2,_]Explanation: Your function should return k = 2, with the first two elements of nums being 1 and 2 respectively.It does not matter what you leave beyond the returned k (hence they are underscores).\nExample 2:\nInput: nums = [0,0,1,1,1,2,2,3,3,4]Output: 5, nums = [0,1,2,3,4,_,_,_,_,_]Explanation: Your function should return k = 5, with the first five elements of nums being 0, 1, 2, 3, and 4 respectively.It does not matter what you leave beyond the returned k (hence they are underscores).\nConstraints:\n\n1 &lt;= nums.length &lt;= 3 * 104\n-100 &lt;= nums[i] &lt;= 100\nnums is sorted in non-decreasing order.\n\nAnswer1class Solution:    def removeDuplicates(self, nums: List[int]) -&gt; int:        left = 0        for right in range(len(nums)):            # 如果相等, 说明right指向的元素是重复元素，不保留            if nums[right] == nums[left]:                continue            # 如果不相等, 说明right指向的元素不是重复元素，保留，然后右移left一个单位，再把right的值赋给left            left += 1            nums[left] = nums[right]        return left + 1\nAnswer2class Solution:    def removeDuplicates(self, nums: List[int]) -&gt; int:        if not nums:            return 0                n = len(nums)        fast = slow = 1        while fast &lt; n:            if nums[fast] != nums[fast - 1]:                nums[slow] = nums[fast]                slow += 1            fast += 1                return slow\n1. 两数之和\n剑指 Offer 48. 最长不含重复字符的子字符串\n参考：\n数据结构和算法-双指针法\n","categories":["数据结构与算法设计"]},{"title":"如何理解python的类与对象？——改自知乎答案","url":"/2022/02/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3python%E7%9A%84%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%EF%BC%9F%E2%80%94%E2%80%94%E6%94%B9%E8%87%AA%E7%9F%A5%E4%B9%8E%E7%AD%94%E6%A1%88/","content":"python教程来源：知乎答案\n\n类、对象在实际编程中有啥好处，为啥要用它们\n\n再给你解释怎么去理解它们\n\n\n1 类的好处\n方便复用（如果你用函数写，就要复制整块代码，增加了代码量，增加了出错率）\n方便扩展（函数写段代码，若要升级、扩展，都十分复杂，容易出错，用类来扩展，则方便清晰）\n方便维护（因为类是把抽象的东西映射成我们常见的，摸得到的东西，容易理解，维护也方便）\n\n\n举几个例子制作鸭子比如你是玩具厂的工程师，你要制作一个机器，可以批量制作鸭子形状的玩具，也许你的思路如下：\ndef makeDuckMouth():    #这里可以放其他制作细节    print(&#x27;鸭子嘴巴制作完毕&#x27;)def makeDuckEar():    #这里可以放其他制作细节    print(&#x27;鸭耳朵制作完毕&#x27;)def makeDuckEye():    #这里可以放其他制作细节    print(&#x27;鸭眼睛制作完毕&#x27;)def makeDuckHead():    #这里可以放其他制作细节    print(&#x27;鸭头制作完毕&#x27;)def makeDuckWing():    #这里可以放其他制作细节    print(&#x27;鸭翅膀制作完毕&#x27;)def makeDuckBody():    #这里可以放其他制作细节    print(&#x27;鸭身体制作完毕&#x27;)def makeDuckFoot():    #这里可以放其他制作细节    print(&#x27;鸭脚制作完毕&#x27;)def makeDuckMerge():    #这里可以放其他制作细节    print(&#x27;鸭子组装完毕&#x27;)makeDuckMoth() #制作鸭嘴巴makeDuckEar() #制作鸭耳朵makeDuckEye() #制作鸭眼睛makeDuckHead() #制作鸭头makeDuckWing()  #制作鸭翅膀，注意，猪是没有翅膀的makeDuckBody() #制作鸭身体makeDuckFoot() #制作鸭脚makeDuckMerge() #合并鸭子的各个部位，组装成最终鸭子\n你发现，自己好厉害，居然学会了整个鸭子的制作流程\n制作猪第二天，老板让你，制作猪形状的玩具，难道你的代码如下？\ndef makePigMouth():    #这里可以放其他制作细节    print(&#x27;猪嘴巴制作完毕&#x27;)def makePigEar():    #这里可以放其他制作细节    print(&#x27;猪耳朵制作完毕&#x27;)def makePigEye():    #这里可以放其他制作细节    print(&#x27;猪眼睛制作完毕&#x27;)def makePigHead():    #这里可以放其他制作细节    print(&#x27;猪头制作完毕&#x27;)def makePigBody():    #这里可以放其他制作细节    print(&#x27;猪身体制作完毕&#x27;)def makePigHand():    #这里可以放其他制作细节    print(&#x27;猪手制作完毕&#x27;)def makePigFoot():    #这里可以放其他制作细节    print(&#x27;4只猪脚制作完毕&#x27;)def makePigMerge():    #这里可以放其他制作细节    print(&#x27;猪组装完毕&#x27;)makePigMoth() #制作猪嘴巴makePigEar() #制作猪耳朵makePigEye() #制作猪眼睛makePigHead() #制作猪头makePigBody() #制作猪身体makePigFoot() #制作4只猪脚makePigMerge() #合并猪的各个部位，组装成最终猪\n这样来，你确实完成了工作，但有没有觉得 有点累？\n是不是觉得，猪和鸭子有很多相似之处？\n2 如何去理解类类就像是基因的制造图纸，我们人类，生的小孩，也是两只手，两只脚，一个嘴巴，除非变异，否则不会多出一只脚的，之所以这样有序的发展，是因为我们身体里，有基因这张图纸控制着我们的繁衍。\nAnimal现在我们给上面例子中的制作 鸭子、猪 建立一个通用的制作基因表，利用他们相似制作流程建立基因表。\nclass Animal(object):    &#x27;&#x27;&#x27;    猪和鸭子的基类（基因图纸表）    &#x27;&#x27;&#x27;    def __init__(self, name): # 实例化的时候传入要制作的东西名字，如猪、鸭子        self.name = name     def makeMouth(self):        #这里可以放其他制作细节          print(self.name+&#x27;的嘴巴 制作完毕&#x27;) #这里的self.name就是获取我们传入的name    def makeEar(self):        #这里可以放其他制作细节          print(self.name+&#x27;的耳朵 制作完毕&#x27;)     def makeEye(self):        #这里可以放其他制作细节          print(self.name+&#x27;的眼睛 制作完毕&#x27;)     def makeHead(self):        #这里可以放其他制作细节          print(self.name+&#x27;的头 制作完毕&#x27;)     def makeBody(self):        #这里可以放其他制作细节          print(self.name+&#x27;的身体 制作完毕&#x27;)     def makeFoot(self):        #这里可以放其他制作细节          print(self.name+&#x27;的脚 制作完毕&#x27;)     def makeMerge(self):        #这里可以放其他制作细节          print(self.name+&#x27;合并完毕&#x27;)     def makeAll(self):         # 一条龙。直接跑完整个流水线        self.makeMoth()        self.makeEar()        self.makeEye()        self.makeHead()        self.makeBody()        self.makeFoot()        self.makeMerge()\n你可以看到，上面的图纸里，我们没有放 makeWing() #制作翅膀，那是因为猪不会飞，我们只放猪和鸭子共有的、相同的东西。\nPig接下来，用这个类，继承给Pig，代码如下：\nclass Pig(Animal):  # 括号里写Animal，意思是Pig继承Animal的所有特性，类似你继承你老爸的基因    def makeMoth(self):        #这里加详细的猪嘴巴制作流程，如长嘴巴，有两个孔          print(self.name+&#x27;的嘴巴 制作完毕&#x27;)     def makeEar(self):        #耳朵大大的        print(self.name+&#x27;的耳朵 制作完毕&#x27;)     def makeEye(self):        #眼睛小小的         print(self.name+&#x27;的眼睛 制作完毕&#x27;)     def makeHead(self):        #很大的头         print(self.name+&#x27;的头 制作完毕&#x27;)     def makeBody(self):        #略         print(self.name+&#x27;的身体 制作完毕&#x27;)     def makeFoot(self):        #略          print(self.name+&#x27;的脚 制作完毕&#x27;)     def makeMerge(self):        #略          print(self.name+&#x27;合并完毕&#x27;) \n在class Pig中，如 makeMoth里面，只加入详细制作猪嘴巴的代码，不要把制作猪头的代码放这里哦\n你发现，class Pig中 没有了这2段代码，这是因为，如果你继承别的对象，如Animal后，若完全不修改某些方法属性，就可以不写出来（实际还是存在Pig中的）\ndef __init__(self, name):      self.name = name   def makeAll(self):       # 一条龙。直接跑完整个流水线      self.makeMoth()      self.makeEar()      self.makeEye()      self.makeHead()      self.makeBody()      self.makeFoot()      self.makeMerge()\n写出来意味着，你要覆盖以前代码块里的行为，如 我们写出了makeMoth，makeEar …\n意味着我们要把makeMoth里的制作细节改成我们想要的，\n猪和鸭子的嘴巴长得不一样，制作细节肯定不同，对吧？\n最后我们用以下代码，让机器开始造猪\npig1 = Pig(&#x27;猪&#x27;)  #实例化，相当于怀孕pig1.makeAll()  # 开始造猪，相当于在子宫里形成整体\nDuck现在开始造鸭子的图纸，继承Animal，覆盖某些代码\nclass Duck(Animal):  #     def makeMoth(self):        #这里加详细的鸭子嘴巴制作流程，如长嘴巴，嘴巴硬         print(self.name+&#x27;的嘴巴 制作完毕&#x27;)     def makeEar(self):        #耳朵很小        print(self.name+&#x27;的耳朵 制作完毕&#x27;)     def makeEye(self):        #眼睛小小的         print(self.name+&#x27;的眼睛 制作完毕&#x27;)     def makeHead(self):        #头很小         print(self.name+&#x27;的头 制作完毕&#x27;)     def makeBody(self):        #略         print(self.name+&#x27;的身体 制作完毕&#x27;)     def makeFoot(self):        #略          print(self.name+&#x27;的脚 制作完毕&#x27;)         def makeMerge(self):        #略          print(self.name+&#x27;合并完毕&#x27;)     def makeWing(self): #增加翅膀的制作流程        #略          print(self.name+&#x27;的翅膀 制作完毕&#x27;)     def makeAll(self): #因为增加了翅膀，所以要覆写这个函数              self.makeMoth()        self.makeEar()        self.makeEye()        self.makeHead()        self.makeBody()        self.makeFoot()        self.makeWing()  #插入翅膀制作流程        self.makeMerge()\n然后用以下代码实例化，开始做鸭子\nduck1 = Duck(&#x27;鸭子&#x27;)duck1.makeAll()\n以后我们要狗、牛、企鹅，都可以从Animal继承，  Animal给我们提供了一个框架，我们只需要在这个框架上，扩展，延伸 就可以很方便的做出任何东西，这就是类的魅力\nOther如果你要做个别的咋办？\n可以建立一个 足够通用基类，慢慢扩展， 类似人类的老祖宗，最开始是微生物，后来是猴子。\nclass Weishengwu(object):    &#x27;&#x27;&#x27;假设一开始的老祖宗只会吃东西&#x27;&#x27;&#x27;    def __init__(self, name):         self.name = name     def eat(self):        print(&#x27;吃东西&#x27;)#开始进化成鱼类class Fish(Weishengwu):    &#x27;&#x27;&#x27;在老祖宗只会吃的基础上，学会了移动&#x27;&#x27;&#x27;    def move(self):        print(&#x27;移动&#x27;)#开始进化成猴子class Houzi(Fish):    &#x27;&#x27;&#x27;在鱼的基础上学会了爬树&#x27;&#x27;&#x27;    def __init__(self, name, love):  # 以前只有名字，现在有了爱好，或许是吃桃子、或许是荡秋千        self.name = name         self.love = love     def eat(self):        #覆写，因为吃东西的方式可能和鱼类不一样了        print(&#x27;牙齿吃东西&#x27;)    def move(self):        #覆写，不是在水里移动了，是陆地上移动        print(&#x27;脚移动&#x27;)    def Pashu(self):        print(&#x27;爬树&#x27;)#你可以继续进化，继续给他增加任何功能\n在python中，一个对象的特征也称为属性（attribute）。它所具有的行为也称为方法（method）。\n结论：对象=属性（特征）+方法（行为）\n类：在python中，把具有相同属性和方法的对象归为一个类（class）。比如人类，动物，植物等等，这些都是类的概念。\n","categories":["数据结构与算法设计"]},{"title":"栈和递归","url":"/2022/01/17/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88%E5%92%8C%E9%80%92%E5%BD%92/","content":"栈（Stack）栈对数据 “存” 和 “取” 的过程有特殊的要求：\n\n栈只能从表的一端存取数据，另一端是封闭的。\n在栈中，无论是存数据还是取数据，都必须遵循”先进后出”的原则，即最先进栈的元素最后出栈。\n\n因此，我们可以给栈下一个定义，即栈是一种只能从表的一端存取数据且遵循 “先进后出” 原则的线性存储结构。通常，栈的开口端被称为栈顶；相应地，封口端被称为栈底。栈的最重要一个特点——后进先出（LIFO，Last In First Out），也可以说是先进后出（FILO，First In Last Out），我们无论如何只能从一端去操作元素。\n栈又叫作堆栈，这里说明一下不要将它和堆混淆。实际上堆和栈是两个不同的概念，栈是一种只能在一端进行插入和删除的线性数据结构。\n一般来说，栈主要有两个操作：一个是进栈（PUSH），又叫作入栈、压栈；另一个是出栈（POP），或者叫作退栈。\nclass Stack:    def __init__(self):        self.stack = []        self.size = 0    def push(self, item):        self.stack.append(item) # 添加元素        self.size += 1 # 栈元素数量加 1    def pop(self):        pop = self.stack.pop() # 删除栈顶元素        self.size -= 1 # 栈元素数量减 1        return pop    def isEmpty(self):        return self.stack == []    def sizes(self):        return self.size    def peek(self):        return self.stack[-1]if __name__ = &#x27;__main__&#x27;:    # 这里假定 A 是 4，B 是 &#x27;dog&#x27;,建议每一步的结果用 print() 输出看一下    s = Stack()    s.isEmpty()    s.push(4)    s.push(&#x27;dog&#x27;)    s.peek()    s.pop()    s.isEmpty()\n递归函数递归函数（recursive function）是指会调用自身的函数。为了防止函数无限地重复调用自身，代码中必须至少有一条选择语句。这条用来查验条件的语句被称为基本情况（base case），用于确定接下来要继续递归还是停止递归。\ndef displayRange(lower, upper):     &quot;&quot;&quot;Outputs the numbers from lower to upper.&quot;&quot;&quot;     while lower &lt;= upper:         print(lower)         lower = lower + 1\n如何将这个函数转换为递归函数呢？首先，需要注意如下两点重要的情况。\n\nlower &lt;= upper时，循环的主体会继续执行。\n执行这个函数时，lower会不断地加1，但是upper不会有任何改变。\n\n等价的递归函数可以执行类似的基本操作，区别在于：循环被替换成了if语句；赋值语句被替换成了函数的递归调用。修改后的代码如下：\ndef displayRange(lower, upper):     &quot;&quot;&quot;Outputs the numbers from lower to upper.&quot;&quot;&quot;     if lower &lt;= upper:         print(lower)         displayRange(lower + 1, upper)\n尽管这两个函数的语法和设计是不一样的，但是它们执行的算法过程相同。递归函数的每次调用都像在迭代版本函数里的循环一样，每次都会访问整个序列里的下一个数。\n通常来说，递归函数至少有一个参数。这个参数的值会被用来对递归过程的基本情况进行判定，从而决定是否要结束整个调用。在每次递归调用之前，这个值也会被进行某种方式的修改。每次对这个值的修改，都应该产生一个新数据值，可以让函数最终达到基本情况。在displayRange这个示例里，每次递归调用之前都会增加参数lower的值，从而让它最终能够超过参数upper的值。\ndef ourSum(lower, upper, margin = 0):     &quot;&quot;&quot;Returns the sum of the numbers from lower to upper,     and outputs a trace of the arguments and return values     on each call.&quot;&quot;&quot;     blanks = &quot; &quot; * margin     print(blanks, lower, upper)         # Print the arguments    if lower &gt; upper:         print(blanks, 0)                # Print the returned value        return 0    else:        result = lower + ourSum(lower + 1, upper, margin + 4)        print(blanks, result)           # Print the returned value        return result&gt;&gt;&gt; ourSum(1, 4) 1 4   2 4       3 4          4 4             5 4             0          4       7    9 10 10\n从结果可以看出，随着对ourSum调用的进行，参数会不断向右缩进。注意，每次调用时，lower的值都增加1，而upper的值始终保持不变。对ourSum的最后一次调用返回0。随着递归的返回，所返回的每个值都与其上面的值对齐，并且会增加上lower的当前值。这样的跟踪，对于递归函数来说，是非常有用的调试工具。\n递归的三大要素整理自知乎\n第一要素：明确你这个函数想要干什么\n对于递归，我觉得很重要的一个事就是，这个函数的功能是什么，他要完成什么样的一件事，而这个，是完全由你自己来定义的。也就是说，我们先不管函数里面的代码什么，而是要先明白，你这个函数是要用来干什么。\n例如，我定义了一个函数\n#算n的阶乘(假设n不为0)def f(n):\t\tpass\n这个函数的功能是算 n 的阶乘。好了，我们已经定义了一个函数，并且定义了它的功能是什么，接下来我们看第二要素。\n第二要素：寻找递归结束条件\n所谓递归，就是会在函数内部代码中，调用这个函数本身，所以，我们必须要找出递归的结束条件，不然的话，会一直调用自己，进入无底洞。也就是说，我们需要找出当参数为啥时，递归结束，之后直接把结果返回，请注意，这个时候我们必须能根据这个参数的值，能够直接知道函数的结果是什么。\n例如，上面那个例子，当 n = 1 时，那你应该能够直接知道 f(n) 是啥吧？此时，f(1) = 1。完善我们函数内部的代码，把第二要素加进代码里面，如下\n#算 n 的阶乘(假设n不为0)def f(n):\t\tif n == 1:        return 1\n有人可能会说，当 n = 2 时，那我们可以直接知道 f(n) 等于多少啊，那我可以把 n = 2 作为递归的结束条件吗？\n当然可以，只要你觉得参数是什么时，你能够直接知道函数的结果，那么你就可以把这个参数作为结束的条件，所以下面这段代码也是可以的。\n#算 n 的阶乘(假设n&gt;=2)def f(n):\t\tif n == 2:        return 2\n注意我代码里面写的注释，假设 n &gt;= 2，因为如果 n = 1时，会被漏掉，当 n &lt;= 2时，f(n) = n，所以为了更加严谨，我们可以写成这样：\n# 算 n 的阶乘(假设n不为0)def f(n):\t\tif n &lt;= 2:        return n\n第三要素：找出函数的等价关系式\n第三要素就是，我们要不断缩小参数的范围，缩小之后，我们可以通过一些辅助的变量或者操作，使原函数的结果不变。\n例如，$f(n)$ 这个范围比较大，我们可以让 $f(n) = n * f(n-1)$。这样，范围就由 n 变成了 n-1 了，范围变小了，并且为了原函数f(n) 不变，我们需要让 f(n-1) 乘以 n。\n说白了，就是要找到原函数的一个等价关系式，f(n) 的等价关系式为 n * f(n-1)，即\n$f(n) = n * f(n-1)$。\n找出了这个等价，继续完善我们的代码，我们把这个等价式写进函数里。如下：\n# 算 n 的阶乘(假设n不为0)def f(n):\t\tif n &lt;= 2:        return n\t\treturn f(n-1) * n\n至此，递归三要素已经都写进代码里了，所以这个 f(n) 功能的内部代码我们已经写好了。\n这就是递归最重要的三要素，每次做递归的时候，你就强迫自己试着去寻找这三个要素。\n还是不懂？没关系，我再按照这个模式讲一些题。\n\n有些有点小基础的可能觉得我写的太简单了，没耐心看？少侠，请继续看，我下面还会讲\n如何优化递归\n\n案例1：斐波那契数列\n斐波那契数列的是这样一个数列：1、1、2、3、5、8、13、21、34….，即第一项 f(1) = 1,第二项 f(2) = 1…..,第 n 项目为 f(n) = f(n-1) + f(n-2)。求第 n 项的值是多少。\n\n1、第一递归函数功能\n假设 f(n) 的功能是求第 n 项的值，代码如下：\n2、找出递归结束的条件\n显然，当 n = 1 或者 n = 2 ,我们可以轻易着知道结果 f(1) = f(2) = 1。所以递归结束条件可以为  n &lt;= 2。代码如下：\ndef f(n):    if n &lt;= 2:        return 1\n3、找出函数的等价关系式\n题目已经把等价关系式给我们了，所以我们很容易就能够知道 $f(n) = f(n-1) + f(n-2)$。我说过，等价关系式是最难找的一个，而这个题目却把关系式给我们了，这也太容易，好吧，我这是为了兼顾几乎零基础的读者。\n所以最终代码如下：\ndef f(n):    # 1.先写递归结束条件    if n &lt;= 2:        return 1    # 2.接着写等价关系式    return f(n - 1) + f(n - 2)\n搞定，是不是很简单？\n\n零基础的可能还是不大懂，没关系，之后慢慢按照这个模式练习！好吧，有大佬可能在吐槽太简单了。\n\n案例2：小青蛙跳台阶\n一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。\n\n1、第一递归函数功能\n假设 f(n) 的功能是求青蛙跳上一个n级的台阶总共有多少种跳法，代码如下：\ndef f(n):\t\tpass\n2、找出递归结束的条件\n我说了，求递归结束的条件，你直接把 n 压缩到很小很小就行了，因为 n 越小，我们就越容易直观着算出 f(n) 的多少，所以当 n = 1时，你知道 f(1) 为多少吧？够直观吧？即 f(1) = 1。代码如下：\ndef f(n):    if n == 1:        return 1\n第三要素：找出函数的等价关系式\n每次跳的时候，小青蛙可以跳一个台阶，也可以跳两个台阶，也就是说，每次跳的时候，小青蛙有两种跳法。\n第一种跳法：第一次我跳了一个台阶，那么还剩下n-1个台阶还没跳，剩下的n-1个台阶的跳法有f(n-1)种。\n第二种跳法：第一次跳了两个台阶，那么还剩下n-2个台阶还没，剩下的n-2个台阶的跳法有f(n-2)种。\n所以，小青蛙的全部跳法就是这两种跳法之和了，即 f(n) = f(n-1) + f(n-2)。至此，等价关系式就求出来了。于是写出代码：\ndef f(n):    if n == 1:        return 1    return f(n - 1) + f(n - 2)\n大家觉得上面的代码对不对？\n答是不大对，当 n = 2 时，显然会有 f(2) = f(1) + f(0)。我们知道，f(0) = 0，按道理是递归结束，不用继续往下调用的，但我们上面的代码逻辑中，会继续调用 f(0) = f(-1) + f(-2)。这会导致无限调用，进入死循环。\n这也是我要和你们说的，关于递归结束条件是否够严谨问题，有很多人在使用递归的时候，由于结束条件不够严谨，导致出现死循环。也就是说，当我们在第二步找出了一个递归结束条件的时候，可以把结束条件写进代码，然后进行第三步，但是请注意，当我们第三步找出等价函数之后，还得再返回去第二步，根据第三步函数的调用关系，会不会出现一些漏掉的结束条件。就像上面，f(n-2)这个函数的调用，有可能出现 f(0) 的情况，导致死循环，所以我们把它补上。代码如下：\ndef f(n):    if n &lt;= 2:        return n    return f(n - 1) + f(n - 2)\n递归的缺点虽然递归的使用非常的简洁，但是也有很多缺点，也是我们在使用中需要额外注意的地方和优化的地方。\n1、递归堆栈溢出▉ 理解堆栈溢出\n1）递归的本质就是重复调用本身的过程，本身是什么？当然是一个函数，那好，函数中有参数以及一些局部的声明的变量，相信很多小伙伴只会用函数，而不知道函数中的变量是怎么存储的吧。没关系，等你听我分析完，你就会了。\n2）函数中变量是存储到系统中的栈中的，栈数据结构的特点就是先进后出，后进先出。一个函数中的变量的使用情况就是随函数的声明周期变化的。当我们执行一个函数时，该函数的变量就会一直不断的压入栈中，当函数执行完毕销毁的时候，栈内的元素依次出栈。还是不懂，没关系，看下方示意图。\n3）我们理解了上述过程之后，回到递归上来，我们的递归调用是在函数里调用自身，且当前函数并没有销毁，因为当前函数在执行自身层层递归进去了，所以递归的过程，函数中的变量一直不断的压栈，由于我们系统栈或虚拟机栈空间是非常小的，当栈压满之后，再压时，就会导致堆栈溢出。\n#报错def f(i):    if i == 1:        return 1    return i + f(i -1) result = f(200000)print(result)\n▉ 解决办法\n通常我们设置递归深度，简单的理解就是，如果递归超过我们设置的深度，我们就退出，不再递归下去。\ndepth=1def f(n):    global depth    depth+=1    if(depth &gt; 1000):        raise Exception(&#x27;堆栈溢出&#x27;)    if(n == 1):        return 1    print(depth)    return f(n-1) + 1\n解决方案 tail recursion\n2、递归重复元素有些递归问题中，存在重复计算问题，比如求斐波那契数列，我们画一下递归树如下图，我们会发现有很多重复递归计算的值，重复计算会导致程序的时间复杂度很高，而且是指数级别的，导致我们的程序效率低下。\n▉ 解决办法\n重复计算问题，我们应该怎么解决？有的小伙伴想到了，我们把已经计算过的值保存起来，每次递归计算之前先检查一下保存的数据有没有该数据，如果有，我们拿出来直接用。如果没有，我们计算出来保存起来。一般我们用散列表（hash）或者键值对来保存。\n3、递归高空间复杂度因为递归时函数的变量的存储需要额外的栈空间，当递归深度很深时，需要额外的内存占空间就会很多，所以递归有非常高的空间复杂度。\nhttps://leetcode-cn.com/problems/unique-paths/\nhttps://leetcode-cn.com/problems/shu-zhi-de-zheng-shu-ci-fang-lcof/\n","categories":["数据结构与算法设计"]},{"title":"树——概念","url":"/2022/03/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%91/","content":"多项集的另一种主要类别在第2章里被称为分层（hierarchical），它由各种类型的树结构组成。大多数编程语言不会把树纳入标准类型。但是，树结构有非常广泛的应用。它们可以非常自然地表示文件目录结构或书的目录等对象的多项集。树还可以用来实现其他需要高效搜索操作的多项集，例如有序集合和有序字典，这些多项集需要在元素上添加某些优先级顺序的多项集（如优先队列）。本章将介绍让树成为非常有用的数据结构的相关属性，并探讨树在实现几种类型的多项集里的作用。\n1　树的概述到目前为止，我们所研究的都是线性数据结构（链表、栈等）。其中，第一个元素之外的其他元素都有一个不同的前驱；最后一个元素之外的其他元素都有一个不同的后继。在树结构里，前驱和后继的这种关系被父节点（parent）和子节点（child）的关系所取代。\n树有两个主要特征：其一，每个元素可以有多个子节点；其二，除了一个名为根节点（root）的特殊元素，其他元素都有且只有一个父节点。根节点没有父节点。\n1　树的术语\n\n\n\n术语\n定义\n\n\n\n\n节点（node）\n存储在树里的元素\n\n\n根节点（root）\n树最上层的节点。唯一没有父节点的节点\n\n\n子节点（child）\n在某个给定节点的下方直接连接的节点。一个节点可以有多个子节点，并且它的子节点在查看的时候会按照从左到右的顺序进行组织。最左边的子节点称为第一个子节点，最右边的子节点称为最后一个子节点\n\n\n父节点（parent）\n在某个给定节点的上方直接连接的节点。一个节点只能有一个父节点\n\n\n兄弟节点（siblings）\n拥有共同父节点的一些子节点\n\n\n叶节点（leaf）\n没有子节点的节点\n\n\n内部节点（interior node）\n至少包含一个子节点的节点\n\n\n边/分支/链接（edge/branch/link）\n连接父节点和子节点的线\n\n\n后代（descendant）\n节点的子节点，子节点的子节点，以此类推，直到叶节点为止\n\n\n祖先（ancestor）\n节点的父节点，父节点的父节点，以此类推，直到根节点为止\n\n\n路径（path）\n连接节点和其中一个后代的一系列的边\n\n\n路径长度（path length）\n路径里边的数目\n\n\n深度或层数（depth or level）\n节点的深度或层数等于把它连接到根节点的路径长度。因此，根节点的深度或层数为0。它的子节点的层数是1，以此类推\n\n\n高度（height）\n树里最长路径的长度；换句话说，也是树里叶节点的最大层数\n\n\n子树（subtree）\n由一个节点和它的所有后代形成的树\n\n\n\n\n\n\n可以看到，树的高度和它所包含的节点数是不同的。只包含一个节点的树的高度为0，按照惯例，空树的高度为−1。\n1.2　普通树和二叉树上图所示的树有时称为普通树（general tree），故可以把它和二叉树（binary tree）区分开来。在二叉树里，每个节点最多有两个子节点，即左子节点（left child）和右子节点（right child）。在二叉树里，如果一个节点只有一个子节点，仍然可以把它区分为左子节点或右子节点。因此，图所示的是两棵树。如果把它们当作二叉树，那么就并不相同。但是若将其视为普通树，它们是相同的。\n\n\n\n普通树——普通树要么为空，要么就是由有限的一组节点组成的。其中节点和其他所有节点都不同，被称为根节点。除此之外，集合 $T-{r}$ 被分成了若干个不相交的子集，每个子集都是一棵普通树。\n二叉树——二叉树要么为空，要么就是由根节点加上左子树和右子树组成的，并且这些子树也都是二叉树。\n\n\n\n2　用树结构的原因前文提到，树能够很好地表示层次结构。解析（parsing）是指分析语言里特定句子的语法的过程。解析树（parse tree）根据句子的组成部分（如名词短语或动词短语）来描述句子的句法结构。图展示了句子“The girl hit the ball with a bat”的解析树。\n\n\n这棵树的根节点被标记为“句子”，用来表示这个结构里最顶层的解析。它的两个子节点分别被标记为“名词短语”和“动词短语”，代表了这个句子的组成部分。标记为“介词短语”的节点是“动词短语”的子节点，因此介词短语“with a bat”是用来修饰动词“hit”的而不是修饰名词短语“the ball”的。在最底层，“ball”这样的叶节点表示解析过程中的单词。\n文件系统结构也是树状的。如图所示，其中目录（现在通常称为文件夹）被标记为“D”，文件则被标记为“F”。\n\n\n一些有序多项集也可以表示为树状结构，这种树称为二叉查找树（Binary Search Tree，BST）。这种树的左子树里每个节点的值都小于根节点的值，并且右子树里每个节点的值都大于根节点的值。图展示了包含字母A～G的有序多项集表示的二叉查找树。\n\n\n上面3个例子表明，树的最重要和最有用的特征并不是树里元素的位置，而是父节点与子节点之间的关系。这些关系对于树结构里的数据具有非常重要的意义。它们可能会用来表示字母的顺序、短语的结构、子目录之间的包含关系，或者给定问题里的任何一对多关系。树中数据的处理基于数据之间的父/子关系。\n3　二叉树的形状自然界里的树有各种形状和大小，数据结构里的树也有各种形状和大小。简单地说，有些树是藤蔓状的且几乎是线性的，而另一些树则是茂密的。这些形状的两个极端如图所示。\n\n\n高度为 $H$ 的满二叉树里包含的节点数 $N$ 是多少？要用 $H$ 来表示 $N$，可以从根节点（1个节点）开始，添加它的子节点（2个节点），再添加子节点的子节点（4个节点)，以此类推：$N=2^{H+1}-1$。那么，具有 $N$ 个节点的满二叉树的高度 $H$ 是 $H=log_2(N+1)-1$。\n但并不是所有茂密的树都是满二叉树，比如，完美平衡二叉树（perfectly balanced binary tree）是除了最后一层其他每一层的节点都被填满的树。它足够茂密，并且也支持最坏情况下对叶节点在对数时间内的访问。另一个例子是完全二叉树（complete binary tree）。它是完美平衡二叉树的一个特例，会像满二叉树那样从左到右填充最后一层的节点。图总结了这些类型的二叉树形状，并给出了一些示例。\n一般而言，二叉树越平衡，访问、插入和删除操作的性能越高。\n\n\n\n一棵满二叉树的高度为5，它包含多少个节点？\n一棵完全二叉树包含125个节点，它的高度是多少？\n在满二叉树里，第$L$层上有多少个节点？用 $L$表示答案。\n\n4　二叉树的遍历在前面的章节里，我们介绍了如何通过for循环或迭代器来遍历线性多项集里的元素。二叉树的遍历有4种标准类型：前序、中序、后序以及层次遍历。每种遍历在访问树中的节点时都遵循特定的路径和方向。接下来我们将展示在二叉查找树上的遍历。遍历算法的相关内容参见后文。\n4.1　前序遍历前序遍历（preorder traversal）算法会先访问树的根节点，然后以相同的方式分别遍历它的左子树和右子树。前序遍历访问的节点序列如图所示。\n\n\n4.2　中序遍历中序遍历（inorder traversal）算法先遍历左子树，然后访问根节点，最后遍历右子树。在对节点进行访问之前，这个过程先从树的最左侧开始。中序遍历访问的节点序列如图所示。\n\n\n4.3　后序遍历后序遍历（postorder traversal）算法先遍历左子树，然后遍历右子树，最后访问根节点。后序遍历所经过的路径如图所示。\n\n\n4.4 层序遍历层次遍历（level order traversal）算法从第0层开始按照从左到右的顺序访问每一层里的节点。层次遍历所经过的路径如图所示。\n\n\n可以看到，中序遍历可以按照排序顺序访问二叉树里的元素。表达式树的前序、中序和后序遍历可以分别用来生成前缀、中缀以及后缀形式的表达式。\n5　二叉树的3种常见应用前文提到，树强调的是父子节点之间的关系，这就能够让用户根据位置以外的标准对数据进行排序。本节介绍二叉树的3种特殊用法：堆、二叉查找树和表达式树。这些特殊用法都对它们的数据加上了顺序。\n5.1 堆二叉树里的数据通常都取自有序集合，其中的元素是可以相互比较的。最小堆（min-heap）就是一个特殊的二叉树，其中每个节点的值都小于或等于它的两个子节点。最大堆（max-heap）把更大的节点放在更靠近根节点的位置。这两种对节点顺序的约束都称为堆属性（heap property）。这里提到的堆和计算机用来管理动态内存的堆并不一样，请勿混淆。图展示了两个最小堆的示例。\n就像图里展示的，最小的元素会在根节点处，最大的元素在叶节点里。可以看到，根据前面给出的定义，图里的堆都具有完全二叉树的形状。堆中的这种数据布局提供了一种被称为堆排序（heap sort）的高效排序方法。堆排序方法会先把一组数据构建成一个堆，然后不断地删除根节点的元素，并把它添加到列表的末尾。堆还可以实现优先队列。我们将在本章后面开发堆的实现。\n\n\n5.2 二叉查找树前文提到，BST在它的节点上添加了有序的顺序。这种方式和堆的方式是不一样的。在BST里，给定节点左子树里的节点会小于给定节点，而右子树里的节点会大于给定节点。当BST的形状接近完美平衡二叉树的形状时，在最坏情况下，搜索和插入操作的复杂度都是 $O(logn)$.\n图展示了在有序列表上二分搜索使用的所有可能的搜索路径，但是在实际搜索过程中只会用到其中一条路径。在每个子列表里，为了比较而访问的元素会加上阴影。\n\n\n如图所示，最长的搜索路径（元素4—7—8—9）需要在包含8个元素的列表里进行4次比较。因为列表是有序的，所以搜索算法在每次比较后都会把搜索空间减少一半。二叉树形式：\n\n\n5.3 表达式树在第 7 章，我们介绍了如何使用栈来把中缀表达式转换为后缀表达式，介绍了如何使用栈计算后缀表达式，还为算术表达式语言开发了解释器和计算器。在一种语言里翻译句子过程也被称为解析（parsing）。另一种处理句子的方法是在解析过程中构建解析树（parse tree）。对于表达式语言来说，这个结构也被称为表达式树（expression tree）。图展示了几个通过解析中缀表达式得到的表达式树。\n\n\n对于表达式树，需要注意以下几点。\n\n表达式树永远不会为空。\n每个内部节点都代表一个复合表达式，由一个运算符及其操作数组成。每个内部节点都恰好有两个子节点，它们代表这个运算符的操作数。\n每个叶节点都代表一个原子的数字操作数。\n优先级较高的运算符通常出现在树的底部附近，除非它们在源表达式里被括号改变了优先级。\n\n\n画出下面这些表达式的表达式树图：\n　　a．3*5 + 6\n　　b．3 + 5*6\n　　c．3*5*6\n\n\n\n","categories":["数据结构与算法设计"]},{"title":"算法复杂度","url":"/2022/08/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/","content":"算法复杂度关于时间复杂度的几个典型证明证明 $O ( f ) + O ( g ) = O ( f + g ) $\n证明 $θ ( f ) + θ ( g ) = θ ( f + g )$\n证明 $m a x ( θ ( f ) , θ ( g ) ) = θ ( f + g ) $\n证明 $θ ( f ) ⋅ θ ( g ) = θ ( f ⋅ g ) $\n证明 $θ ( m a x ( f ( n ) , g ( n ) ) ) = θ ( f + g )$\nO、Θ、Ω、o、ω，别再傻傻分不清了！算法复杂度分析中的符号（Θ、Ο、ο、Ω、ω）简介Θ，读音：theta、西塔；既是上界也是下界(tight)，等于的意思。\nΟ，读音：big-oh、欧米可荣（大写）；表示上界(tightness unknown)，小于等于的意思。\nο，读音：small-oh、欧米可荣（小写）；表示上界(not tight)，小于的意思。\nΩ，读音：big omega、欧米伽（大写）；表示下界(tightness unknown)，大于等于的意思。\nω，读音：small omega、欧米伽（小写）；表示下界(not tight)，大于的意思。\nΟ是渐进上界，Ω是渐进下界。Θ需同时满足大Ο和Ω，故称为确界（必须同时符合上界和下界）。Ο极其有用，因为它表示了最差性能。\n算法复杂度中的O(logN)底数是多少算法中log级别的时间复杂度都是由于使用了分治思想,这个底数直接由分治的复杂度决定。如果采用二分法,那么就会以2为底数,三分法就会以3为底数,其他亦然。不过无论底数是什么,log级别的渐进意义是一样的。\n","categories":["数据结构与算法设计"]},{"title":"面向对象 —— 数据结构 Python语言描述","url":"/2022/02/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","content":"定义\n类（class）用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。Python里的数据类型都是类。\n数据成员：类变量或者实例变量, 用于处理类及其实例对象的相关的数据。\n方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。\n局部变量：定义在方法中的变量，只作用于当前实例的类。\n继承：即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟”是一个（is-a）”关系（例图，Dog是一个Animal）。\n实例化：创建一个类的实例，类的具体对象。\n方法：类中定义的函数。\n对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。\n\n父类（parent class）的名称是可选的，在默认情况下，它会是object。所有Python类属于一个以object作为根节点的层次结构。在object里，Python定义了几种方法：__str__和__eq__，因此所有子类会自动继承这些方法。稍后你将看到，这些方法为任何新的类都提供了最基础的一些默认行为。\n\n实例方法（instance method）是在类的对象上运行的。它们包含用来访问或修改实例变量的代码。\n\n实例变量（instance variable）是指由单个对象所拥有的存储信息。在类的声明中，属性是用变量来表示的。这种变量就称为实例变量，是在类声明的内部但是在类的其他成员方法之外声明的。\n\n类变量（class variable）是指由类的所有对象存储所有的信息。类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。\n\n\n\n类可以理解为一种类型，对象是类中的一种实例。用类创建对象的过程叫做实例化。\n最常见的举例是：  \n类Class：鸟 \n类的方法（函数）：（鸟）会飞\n类的属性（变量）：爪子，翅膀  \n实例： \n对象：麻雀，是（类Class）鸟 的一种 \n对象方法（函数）：麻雀会飞 \n对象的变量：麻雀有2个爪子，一对翅膀\n\n创建类使用 class 语句来创建一个新类，class 之后为类的名称并以冒号结尾:\nclass ClassName:   &#x27;类的帮助信息&#x27;   #类文档字符串   class_suite  #类体\n类的帮助信息可以通过ClassName.doc查看。\nclass_suite 由类成员，方法，数据属性组成。\n实例以下是一个简单的 Python 类的例子:\nclass Employee:   &#x27;所有员工的基类&#x27;   empCount = 0    def __init__(self, name, salary):      self.name = name      self.salary = salary      Employee.empCount += 1      def displayCount(self):     print(&quot;Total Employee&quot;, Employee.empCount)    def displayEmployee(self):      print (&quot;Name : &quot;, self.name,  &quot;, Salary: &quot;, self.salary)\n\nempCount 变量是一个类变量，它的值将在这个类的所有实例之间共享。你可以在内部类或外部类使用 Employee.empCount 访问。\n第一种方法init()方法是一种特殊的方法，被称为类的构造函数或初始化方法，当创建了这个类的实例时就会调用该方法\nself 代表类的实例，self 在定义类的方法时是必须有的，虽然在调用时不必传入相应的参数。\n\nself代表类的实例，而非类类的方法与普通的函数只有一个特别的区别——它们必须有一个额外的第一个参数名称, 按照惯例它的名称是 self。\nclass Test:    def prt(self):        print(self)        print(self.__class__) t = Test()t.prt()\n以上实例执行结果为：\n&lt;__main__.Test instance at 0x10d066878&gt;__main__.Test\n从执行结果可以很明显的看出，self 代表的是类的实例，代表当前对象的地址，而 self.class 则指向类。\nself 不是 python 关键字，我们把他换成 runoob 也是可以正常执行的。\n创建实例对象实例化类其他编程语言中一般用关键字 new，但是在 Python 中并没有这个关键字，类的实例化类似函数调用方式。\n以下使用类的名称 Employee 来实例化，并通过 init 方法接收参数。\n&quot;创建 Employee 类的第一个对象&quot;emp1 = Employee(&quot;Zara&quot;, 2000)&quot;创建 Employee 类的第二个对象&quot;emp2 = Employee(&quot;Manni&quot;, 5000)\n访问属性您可以使用点号 . 来访问对象的属性。使用如下类的名称访问类变量:\nemp1.displayEmployee()emp2.displayEmployee()print(&quot;Total Employee %d&quot; % Employee.empCount)\n完整\nclass Employee:   &#x27;所有员工的基类&#x27;   empCount = 0    def __init__(self, name, salary):      self.name = name      self.salary = salary      Employee.empCount += 1      def displayCount(self):     print(&quot;Total Employee %d&quot; % Employee.empCount)    def displayEmployee(self):      print(&quot;Name : &quot;, self.name,  &quot;, Salary: &quot;, self.salary) &quot;创建 Employee 类的第一个对象&quot;emp1 = Employee(&quot;Zara&quot;, 2000)&quot;创建 Employee 类的第二个对象&quot;emp2 = Employee(&quot;Manni&quot;, 5000)emp1.displayEmployee()emp2.displayEmployee()print(&quot;Total Employee %d&quot; % Employee.empCount)\n你可以添加，删除，修改类的属性，如下所示：\nemp1.age = 7  # 添加一个 &#x27;age&#x27; 属性emp1.age = 8  # 修改 &#x27;age&#x27; 属性del emp1.age  # 删除 &#x27;age&#x27; 属性\n你也可以使用以下函数的方式来访问属性：\n\ngetattr(obj, name[, default]) : 访问对象的属性。\nhasattr(obj,name) : 检查是否存在一个属性。\nsetattr(obj,name,value) : 设置一个属性。如果属性不存在，会创建一个新属性。\ndelattr(obj, name) : 删除属性。\n\nhasattr(emp1, &#x27;age&#x27;)    # 如果存在 &#x27;age&#x27; 属性返回 True。getattr(emp1, &#x27;age&#x27;)    # 返回 &#x27;age&#x27; 属性的值setattr(emp1, &#x27;age&#x27;, 8) # 添加属性 &#x27;age&#x27; 值为 8delattr(emp1, &#x27;age&#x27;)    # 删除属性 &#x27;age&#x27;\n类的继承面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过继承机制。\n通过继承创建的新类称为子类或派生类，被继承的类称为基类、父类或超类。\n继承语法\nclass 派生类名(基类名)    ...\n在python中继承中的一些特点：\n\n1、如果在子类中需要父类的构造方法就需要显式的调用父类的构造方法，或者不重写父类的构造方法。详细说明可查看： python 子类继承父类构造函数说明。\n2、在调用基类的方法时，需要加上基类的类名前缀，且需要带上 self 参数变量。区别在于类中调用普通函数时并不需要带上 self 参数\n3、Python 总是首先查找对应类型的方法，如果它不能在派生类中找到对应的方法，它才开始到基类中逐个查找。（先在本类中查找调用的方法，找不到才去基类中找）。\n\n如果在继承元组中列了一个以上的类，那么它就被称作”多重继承” 。\n语法：\n派生类的声明，与他们的父类类似，继承的基类列表跟在类名之后，如下所示：\nclass Parent:        # 定义父类   parentAttr = 100   def __init__(self):      print(&quot;调用父类构造函数&quot;)    def parentMethod(self):      print(&#x27;调用父类方法&#x27;)    def setAttr(self, attr):      Parent.parentAttr = attr    def getAttr(self):      print(&quot;父类属性 :&quot;, Parent.parentAttr) class Child(Parent): # 定义子类   def __init__(self):      print(&quot;调用子类构造方法&quot;)    def childMethod(self):      print(&#x27;调用子类方法&#x27;) c = Child()          # 实例化子类c.childMethod()      # 调用子类的方法c.parentMethod()     # 调用父类方法c.setAttr(200)       # 再次调用父类的方法 - 设置属性值c.getAttr()          # 再次调用父类的方法 - 获取属性值\n你可以继承多个类\nclass A:        # 定义类 A.....class B:         # 定义类 B.....class C(A, B):   # 继承类 A 和 B.....\n方法重写如果你的父类方法的功能不能满足你的需求，你可以在子类重写你父类的方法：\nclass Parent:        # 定义父类   def myMethod(self):      print &#x27;调用父类方法&#x27; class Child(Parent): # 定义子类   def myMethod(self):      print &#x27;调用子类方法&#x27; c = Child()          # 子类实例c.myMethod()         # 子类调用重写方法\n基础重载方法下表列出了一些通用的功能，你可以在自己的类重写：\n\n\n\n\n序号\n方法, 描述 &amp; 简单的调用\n\n\n\n\n1\ninit ( self [,args…] ) 构造函数 简单的调用方法: obj = className(args)\n\n\n2\ndel( self ) 析构方法, 删除一个对象 简单的调用方法 : del obj\n\n\n3\nrepr( self ) 转化为供解释器读取的形式 简单的调用方法 : repr(obj)\n\n\n4\nstr( self ) 用于将值转化为适于人阅读的形式 简单的调用方法 : str(obj)\n\n\n5\ncmp ( self, x ) 对象比较 简单的调用方法 : cmp(obj, x)\n\n\n\n\n运算符重载Python同样支持运算符重载，实例如下：\nclass Vector:   def __init__(self, a, b):      self.a = a      self.b = b    def __str__(self):      return &#x27;Vector (%d, %d)&#x27; % (self.a, self.b)      def __add__(self,other):      return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10)v2 = Vector(5,-2)print(v1 + v2)\n类属性与方法类的私有属性__private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。\n类的方法在类的内部，使用 def 关键字可以为类定义一个方法，与一般函数定义不同，类方法必须包含参数 self,且为第一个参数\n类的私有方法\n__private_method：两个下划线开头，声明该方法为私有方法，不能在类的外部调用。在类的内部调用 self.__private_methods\nclass JustCounter:    __secretCount = 0  # 私有变量    publicCount = 0    # 公开变量     def count(self):        self.__secretCount += 1        self.publicCount += 1        print self.__secretCount counter = JustCounter()counter.count()counter.count()print(counter.publicCount)print(counter.__secretCount)  # 报错，实例不能访问私有变量\n单下划线、双下划线、头尾双下划线说明：\nfoo: 定义的是特殊方法，一般是系统定义名字 ，类似 init() 之类的。\n\n_foo: 以单下划线开头的表示的是 protected 类型的变量，即保护类型只能允许其本身与子类进行访问，不能用于 from module import *\n\n__foo: 双下划线的表示的是私有类型(private)的变量, 只能是允许这个类本身进行访问了。\n\n\n为了说明这些概念，我们将探讨定义Counter（计数器）类的代码。顾名思义，计数器对象会跟踪一个整数的计数。计数器的值最初为0，也可以随时重置为0。你可以对计数器进行递增或者递减、获取其当前的整数值、获取其字符串表达式以及比较两个计数器是否相等。相应代码如下。\nclass Counter(object):    &quot;&quot;&quot;Models a counter.&quot;&quot;&quot;    # Class variable    instances = 0    # Constructor    def __init__(self):        &quot;&quot;&quot;Sets up the counter.&quot;&quot;&quot;        Counter.instances += 1        self.reset()    # Mutator methods    def reset(self):        &quot;&quot;&quot;Sets the counter to 0.&quot;&quot;&quot;        self.value = 0    def increment(self, amount = 1):        &quot;&quot;&quot;Adds amount to the counter.&quot;&quot;&quot;        self.value += amount    def decrement(self, amount = 1):        &quot;&quot;&quot;Subtracts amount from the counter.&quot;&quot;&quot;        self.value -= amount    # Accessor methods    def getValue(self):        &quot;&quot;&quot;Returns the counter&#x27;s value.&quot;&quot;&quot;        return self.value    def __str__(self):        &quot;&quot;&quot;Returns the string representation of the counter.&quot;&quot;&quot;        return str(self._value)     def __eq__(self, other):        &quot;&quot;&quot;Returns True if self equals other        or False otherwise.&quot;&quot;&quot;        if self is other: return True        if type(self) != type(other): return False        return self.value == other.value\n在Python的Shell窗口里对计数器对象的操作结果如下。\n&gt;&gt;&gt; from counter import Counter&gt;&gt;&gt; c1 = Counter()&gt;&gt;&gt; print(c1)0&gt;&gt;&gt; c1.getValue()0&gt;&gt;&gt; str(c1)&#x27;0&#x27;&gt;&gt;&gt; c1.increment()&gt;&gt;&gt; print(c1)1&gt;&gt;&gt; c1.increment(5)&gt;&gt;&gt; print(c1)6&gt;&gt;&gt; c1.reset()&gt;&gt;&gt; print(c1)0&gt;&gt;&gt; c2 = Counter()&gt;&gt;&gt; Counter.instances2&gt;&gt;&gt; c1 == c1True&gt;&gt;&gt; c1 == 0False&gt;&gt;&gt; c1 == c2True&gt;&gt;&gt; c2.increment()&gt;&gt;&gt; c1 == c2False\n​    Counter类是object的子类。类变量instances会跟踪已创建的计数器对象的数量。除了对它进行最初赋值，类变量必须以类名作为前缀进行访问。\n定义实例方法的语法和定义函数的语法是类似的。但是实例方法会有一个名为self的额外的参数，并且这个参数总是出现在参数列表的开头。在方法定义的上下文里，self是指在运行时这个方法的对象本身。\n​    创建Counter的实例之后，实例方法__init__（也称为构造函数）将自动运行。这个方法用来初始化实例变量，并且对类变量进行更新。可以看到，__init__通过语法self.reset()调用reset实例方法，从而对单个实例变量进行初始化。\n​    其他实例方法可以分为两种：变异器（mutator）和访问器（accessor）。变异器会通过修改对象的实例变量对其内部状态进行修改或更改。访问器则只会查看或使用对象的实例变量的值，而不会去修改它们。\n​    在reset实例方法被首次调用时，它引入了实例变量self.value。之后，对这个方法的任何其他调用，都会将这个变量的值修改为0。\n​    使用实例变量都会加上前缀self。和参数或临时变量不同的地方是，实例变量在类的任何方法里是可见的。\n​    increment和decrement方法都包含默认参数，从而为程序员提供了指定数目的可能性。\n​    Counter类的__str__方法将覆盖object类里的这个方法。当把这个对象作为参数传递给str函数时，Python会调用对象的__str__方法。在运行对象上的方法时，Python会先在这个对象自己的类里查找相应方法的代码。如果找不到这个方法，那么Python将在它的父类里查找，以此类推。如果在最后（在查看object类之后）还是找不到这个方法的代码，Python就会引发异常。\n​    当Python的print函数接收到一个参数时，这个参数的__str__方法将自动运行，从而得到它的字符串表达式，以便用来输出。我们鼓励程序员为每个新定义的类实现__str__方法，从而对调试提供帮助。\n​    当看到==运算符时，Python将运行__eq__方法。在object类里，这个方法的默认定义是运行is运算符。这个运算符会对两个操作数的对象标识进行比较。在本例中，对于两个不同的计数器对象，只要它们具有相同的值，就应该被视为相等。==的第二个操作数可以是任意对象，因此__eq__方法会在访问实例变量之前先判断操作数的类型是否相同。注意，你可以通过对象上的点运算符访问它的实例变量。\n参考：\n\nPython 面向对象\n数据结构 Python语言描述 第2版\n\n","categories":["数据结构与算法设计"]},{"title":"教程推荐","url":"/2022/11/06/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1-%E6%95%99%E7%A8%8B%E6%8E%A8%E8%8D%90/","content":"动态规划：\n动态规划专题班 ACM总冠军、清华+斯坦福大神带你入门动态规划算法\n最大流：\n13-1: 网络流问题基础 Network Flow Problems\n13-2: Ford-Fulkerson Algorithm 寻找网络最大流\nNP-完全问题：\n算法设计与分析[0017] NP-完全问题：概述（两道证明习题）/#NPC-NP-Complete-%E9%97%AE%E9%A2%98)\n","categories":["数据结构与算法设计"]},{"title":"linux中ssh相关问题","url":"/2022/11/22/linux/linux%E4%B8%ADssh%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/","content":"SSH配置\n1.查询是否安装SSH.\nrpm -pa |grep ssh\n2.如果没有安装rmp:\nsudo apt-get install rpm          #ubuntu,debianyum -y instal rpm                 #centos,redhat\n3.安装SSH\nsudo apt-get install sshoryum -y install openssh\n4.启动服务:\nservice sshd startor/bin/systemctl restart sshd.serviceor/etc/init.d/sshd start\n5.配置端口:\nvim /etc/ssh/sshd_config\n6.将port 前面的#删除,也可以更改其它端口.\n\nimg\n\n7.允许root用户远程登录.\n\nimg\n\n8.查看服务是否启动:\nps -e | grep ssh\n\nimg\n\n9.远程登录:\nssh name@192.168.0.128\n\nimg\n\n服务器与主机之间的文件的上传与下载\n上传\nscp  /home/me/Desktop/test.txt  user_name@192.168.0.0:/opt\n下载\nscp user_name@192.168.0.0:/opt/test.txt  /home/me/Desktop\n上传目录\nscp -r  /home/me/Desktop/test  user_name@192.168.0.0:/opt\n下载目录\nscp -r user_name@192.168.0.0:/opt/test  /home/me/Desktop\nMac 连接ssh\n\n第一步：打开Mac的终端\n第二步：在终端输入\nssh -p 端口号 服务器用户名@ip\n回车，到这有可能会让你输入yes或者no来确认是否连接，输入yes回车，然后就会让你输入用户密码。\n第三步：输入远程服务器的用户密码即可。\n\nSSH远程连接wifi\n1、安装nmcli\nsudo apt-get install nmcli\n2、查看网络设备\nsudo nmcli dev\n3、开启wifi\nsudo nmcli r wifi on\n4、扫描wifi\nsudo nmcli dev wifi\n5、连接wifi\nsudo nmcli dev wifi connect &quot;wifi名&quot; password &quot;密码&quot;\n重启网卡失败\n问题：使用systemctl restart network 或 service network restart 命令重启网卡失败。\n\nimg\n\n分析：原因其实也很简单，命令用错了，造成了找不到相应的网卡服务。\n解决：\n1、可以尝试使用以下命令：\nservice network-manager restart \n2、如果是 Kali Linux（Debian），则需要用以下命令：\nservice networking restart\n3、如果是Centos 8，则需要用以下命令：\nnmcli c reload\n","categories":["linux"]},{"title":"评价类模型链接汇总","url":"/2022/11/23/%E6%95%99%E8%82%B2%E7%A7%91%E7%A0%94%E6%96%B9%E6%B3%95/%E8%AF%84%E4%BB%B7%E7%B1%BB%E6%A8%A1%E5%9E%8B%E9%93%BE%E6%8E%A5%E6%B1%87%E6%80%BB/","content":"模糊分析\n它具有结果清晰，系统性强的特点，能较好地解决模糊的、难以量化的问题，适合各种非确定性问题的解决。\n模糊层次综合分析法Python实践及相关优缺点分析\nTOPSIS\nTOPSIS法(优劣解距离法)介绍及 python3 实现\n数学建模笔记——评价类模型之TOPSIS\n因子分析\nSPSS数据分析，基于因子分析学生成绩综合评价\npython实现因子分析及用于综合评分且配上碎石图（实例分析）\n层次分析\n整套算法实际上是用了两次重要性权重。\n准则层，从准则的重要性矩阵（nxn矩阵）中，抽取重要性权重。它的现实意义是 每个准则的重要程度。 也就是说，输入的是nxn一个矩阵，值是每个准则两两之间重要度，输出的是这n个准则各自的权重。\n方案层，对每个准则，m个方案都有个mxm矩阵（总共是n个mxm矩阵）。也就是说，对每个准则，都可以算出m个方案的重要性权重。\n然后n个重要性权重组合起来，与准则层的重要性权重相乘。就得到了每个方案的重要性权重。\n【AHP】层次分析法原理与Python实现\n熵权法\n熵值法原理、应用及其Python实现\n主成分分析\n主成分分析(Principal component analysis, PCA)例子–Python\nPython数模笔记-Sklearn（3）主成分分析\n客观赋权法\n客观赋权法的使用（R）\n\n熵权法：指标变异性的大小\n标准离差法：指标变异性的大小\nCRⅢC法：不仅考虑了指标变异大小对权重的影响，还考虑了各指标之间的冲突性，因此可以说CRⅢC法是一种比熵权法和标准离差法更好的客观赋权法。因此，当对选取指标比较多的项目进行评价时，可以在正相关程度较高的指标中去除一些指标，这样可以减少计算量而不会多评价结果产生很大的影响。\n\n","categories":["教育科研方法"]},{"title":"假设检验链接汇总","url":"/2022/11/23/%E6%95%99%E8%82%B2%E7%A7%91%E7%A0%94%E6%96%B9%E6%B3%95/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E9%93%BE%E6%8E%A5%E6%B1%87%E6%80%BB/","content":"方差分析\n数模系列(6)：方差分析（ANOVA）\nSPSS单因素方差分析\nt检验\n一文详解t检验\n","categories":["教育科研方法"]},{"title":"探索性因素分析与验证性因素分析","url":"/2022/11/23/%E6%95%99%E8%82%B2%E7%A7%91%E7%A0%94%E6%96%B9%E6%B3%95/%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90%E4%B8%8E%E9%AA%8C%E8%AF%81%E6%80%A7%E5%9B%A0%E7%B4%A0%E5%88%86%E6%9E%90/","content":"探索性因素分析与验证性因素分析的差异\n验证性因子分析\n基本思想不同\n探索性因子分析主要是为了找出影响观测变量的因子个数,以及各个因子和各个观测变量之间的相关程度,以试图揭示一套相对比较大的变量的内在结构。研究者的假定是每个指标变量都与某个因子匹配,而且只能通过因子载荷凭知觉推断数据的因子结构。\n验证性因子分析的主要目的是决定事前定义因子的模型拟合实际数据的能力,以试图检验观测变量的因子个数和因子载荷是否与基于预先建立的理论的预期一致。验证性因子分析的主要目的是决定事前定义因子的模型拟合实际数据的能力,以试图检验观测变量的因子个数和因子载荷是否与基于预先建立的理论的预期一致。其先验假设是每个因子都与一个具体的指示变量子集对应,并且至少要求预先假设模型中因子的数目,但有时也预期哪些变量依赖哪个因子。\n应用前提不同\n在进行探索性因子分析之前,不必知道要用几个因子,以及各因子和观测变量之间的关系。在进行探索性因子分析时,由于没有先验理论,只能通过因子载荷凭知觉推断数据的因子结构。上述数学模型中的公共因子数m在分析前并未确定,而是在分析过程中视中间结果而决定,各个公共因子Ni统一地规定为均影响每个观测变量xi。探索性因子分析更适合于在没有理论支持的情况下对数据的试探性分析。\n验证性因子分析则是基于预先建立的理论,要求事先假设因子结构,其先验假设是每个因子都与一个具体的指示变量子集对应,以检验这种结构是否与观测数据一致。也就是在上述数学模型中,首先要根据先验信息判定公共因子数m,同时还要根据实际情况将模型中某些参数设定为某一定值。这样,验证性因子分析也就充分利用了先验信息,在已知因子的情况下检验所搜集的数据资料是否按事先预定的结构方式产生作用。\n理论假设不同\n探索性因子分析的假设主要包括：①所有的公共因子都相关(或都不相关)；②所有的公共因子都直接影响所有的观测变量；③ 特殊(唯一性)因子之间相互独立；④ 所有观测变量只受一个特殊(唯一性)因子的影响；⑤ 公共因子与特殊因子(唯一性)相互独立。验证性因子分析克服了探索性因子分析假设条件约束太强的缺陷，其假设主要包括：① 公共因子之间可以相关，也可以无关；② 观测变量可以只受一个或几个公共因子的影响，而不必受所有公共因子的影响；③特殊因子之间可以相关，还可以出现不存在误差因素的观测变量；④ 公共因子与特殊因子之间相互独立。\n主要应用范围不同\n探索性因子分析主要应用于三个方面：①寻求基本结构，解决多元统计分析中的变量间强相关问题；② 数据化简；③发展测量量表。验证性因子分析允许研究者将观察变量依据理论或先前假设构成测量模式，然后评价此因子结构和该理论界定的样本资料间符合的程度。因此，主要应用于以下三个方面：① 验证量表的维度或面向性(dimensionality)，或者称因子结构，决定最有效因子结构；② 验证因子的阶层关系；③ 评估量表的信度和效度。\n\nimg\n\n超全整理！你一定要知道的效度分析攻略\n\nimg\n\n小白须知之探索性因子分析\n","categories":["教育科研方法"]},{"title":"神经网络中的剪枝","url":"/2023/03/10/%E5%89%AA%E6%9E%9D/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%89%AA%E6%9E%9D/","content":"神经网络中的剪枝\n简介\n剪枝顾名思义，就是删去一些不重要的节点，来减小计算或搜索的复杂度。剪枝在很多算法中都有很好的应用，如：决策树，神经网络，搜索算法，数据库的设计等。在决策树和神经网络中，剪枝可以有效缓解过拟合问题并减小计算复杂度；在搜索算法中，可以减小搜索范围，提高搜索效率。\n在神经网络的训练过程中，为了选择合适的大小的神经网络，有两种方法。\n\n第一种，由小的神经网络慢慢的加入新的隐含层来扩大神经网络结构。\n第二种，由一个较大的神经网络结构通过剪枝来获得较为何时大小的神经网络。\n\n而剪枝算法就是应用于第二种方法中。剪枝采用自顶而下的设计方式，先构建一个足够大的网络，然后通过在训练时删除或合并某些节点或者权值来达到精简网络结构的目的。\n过参数化\n具体来说，深度学习网络模型从卷积层到全连接层存在着大量冗余的参数，大量神经元激活值趋近于0，将这些神经元去除后可以表现出同样的模型表达能力，这种情况被称为过参数化，而对应的技术则被称为模型剪枝。\n必要性\n\n在《To prune, or not to prune: exploring the efficacy of pruning for model compression》中探讨了具有同等参数量的稀疏大模型和稠密小模型的性能对比，在图像和语音任务上表明稀疏大模型普遍有更好的性能。\n神经网络的高计算量和高存储花费使得它在边缘计算很难部署。\n\n主要步骤\n（1） 进行正常的网络训练；\n（2） 删除所有权重小于一定阈值的连接；\n（3） 对上面得到的稀疏连接网络再训练；\n经典方法（简单介绍）\n权衰减法：它通过在网络目标函数中引入代表结构复杂性的正则化项来达到减低网络结构复杂性的目的。通常的目标函数如下，其中 \\(E(W)\\) 通常取网络误差平方和，\\(C(W)\\) 为网络的复杂性。 \\[\nJ(W)=E(W)+\\lambda C(W)\n\\] 灵敏度计算方法（Sensitivity Calculations）：该方法是指在网络进行训练时，或在网络训练结束后，计算节点（输入节点及隐节点）或连接权对网络误差的贡献，即灵敏度，删除那些贡献小的节点或权。\n相关性剪枝方法：最常见的相关性剪枝方法是先判断隐节点输出之间的相关性，然后合并具有较大相关性的隐节点。\nDropout 和 DropConnect ：\n\nDropout中随机的将一些神经元的输出置零，这就是神经元剪枝。\nDropConnect则随机的将一些神经元之间的连接置零，使得权重连接矩阵变得稀疏，这便是权重连接剪枝。\n它们是最细粒度的剪枝技术。\n\nFor CNN\n根据粒度的不同，可以粗分为4个粒度\n\n细粒度剪枝(fine-grained)：即对连接或者神经元进行剪枝，它是粒度最小的剪枝。\n向量剪枝(vector-level)：它相对于细粒度剪枝粒度更大，属于对卷积核内部(intra-kernel)的剪枝。\n核剪枝(kernel-level)：即去除某个卷积核，它将丢弃对输入通道中对应计算通道的响应。\n滤波器剪枝(Filter-level)：对整个卷积核组进行剪枝，会造成推理过程中输出特征通道数的改变。\n\n结构化剪枝 OR 非结构化剪枝？\n细粒度剪枝(fine-grained)，向量剪枝(vector-level)，核剪枝(kernel-level)方法在参数量与模型性能之间取得了一定的平衡，但是网络的拓扑结构本身发生了变化，需要专门的算法设计来支持这种稀疏的运算，被称之为非结构化剪枝。\n而滤波器剪枝(Filter-level)只改变了网络中的滤波器组和特征通道数目，所获得的模型不需要专门的算法设计就能够运行，被称为结构化剪枝。除此之外还有对整个网络层的剪枝，它可以被看作是滤波器剪枝(Filter-level)的变种，即所有的滤波器都丢弃。\n历史\n神经网络的剪枝已经广泛的应用于CNN模型中。Mozer, Smol ensky 1989提出删除输入节点或隐节点的方法，当某节点的灵敏度低于预定的阈值时，便可以删除该节点，也就是灵敏度计算方法的经典算法。Sefee and carter 研究了该算法发现，即使系统有更小的参数也不会使得这个系统过于敏感。1990年，Karnin 提出一种删除权值的方法，该方法在神经网络学习中动态计算每个权值的灵敏度，因此计算量较小。Weight 等人基于Rissanen的最短描述长度来描述学习机器的复杂性，并提出权消去法（Weight-Elimination），该方法用于剪除网络中冗余的权值。Reed在1993年曾对早期的剪枝算法进行简单的分类。1993年，Hassibi等人提出的 Optimal Brain Surgeon 剪枝网络通过Hessian损失函数来减少关联，并且实验表明，算法比权衰减法要好。\n但这些方法都需要事先知道一些信息才能断定神经网络的大小。之后，GA遗传，PSO等算法被应用于剪枝中，GA或PSO可以找出合适的隐含层数目或者节点数目。优化算法和神经网络的结合大大提高了神经网络的计算效率，也被称为iterative pruning。例如1997, Slawomir 等人使用随机优化算法对神经网络进行剪枝；\n最近，因为深度学习是计算密集型和存储密集型的，这使得它难以被部署到只有有限硬件资源的嵌入式系统上。2015年，Han等人发现剪枝后的网络的参数的数目减小了一个数量级；并且提出了一个新的去除冗余的方式，“密集-稀疏-密集”（DSD）训练方法。\n\n\n\n\n\n\n\n\n年份\n事件/神经网络\n论文\n\n\n\n\n1989\nMozer, Smol ensky等人提出一种灵敏度计算方法应用于剪枝方法中\nMozer, M. C., &amp; Smolensky, P. (1989). Skeletonization: A technique for trimming the fat from a network via relevance assessment. In Advances in neural information processing systems (pp. 107-115).\n\n\n1991\nWeigend等人提出权衰弱法进行剪枝\nWeigend, A. S., Rumelhart, D. E., &amp; Huberman, B. A. (1991). Generalization by weight-elimination with application to forecasting. In Advances in neural information processing systems (pp. 875-882).\n\n\n1993\nReed对经典的剪枝算法进行归纳验证，经典的pruning servey之一\nReed, R. (1993). Pruning algorithms-a survey. IEEE transactions on Neural Networks, 4(5), 740-747.\n\n\n2015\nHan S基于剪枝提出了一个压缩神经网络的新方法\nHan, S., Mao, H., &amp; Dally, W. J. (2015). A deep neural network compression pipeline: Pruning, quantization, huffman encoding. arXiv preprint arXiv:1510.00149, 10.\n\n\n2015\nHan S为了减小神经网络的高消耗，提出了DSD的三步骤剪除冗余的神经网络\nHan, S., Pool, J., Tran, J., &amp; Dally, W. (2015). Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems (pp. 1135-1143).\n\n\n\n发展分析\n近年来，人工神经网络正向模拟人类认知的道路上更加深入发展，与模糊系统、遗传算法，成为人工智能的一个重要方向，将在实际应用中得到发展。将信息几何应用于人工神经网络的研究，为人工神经网络的理论研究开辟了新的途径。神经计算机的研究发展很快，已有产品进入市场。光电结合的神经计算机为人工神经网络的发展提供了良好条件。\n神经网络在很多领域已得到了很好的应用，但其需要研究的方面还很多。其中，具有分布存储、并行处理、自学习、自组织以及非线性映射等优点的神经网络与其他技术的结合以及由此而来的混合方法和混合系统，已经成为一大研究热点。由于其他方法也有它们各自的优点，所以将神经网络与其他方法相结合，取长补短，继而可以获得更好的应用效果。目前这方面工作有神经网络与模糊逻辑、专家系统、遗传算法、小波分析、混沌、粗集理论、分形理论、证据理论和灰色系统等的融合。\n1. 重要性因子选择\n通过某种准则来判断一个连接或者通道是否重要，比如范数（L1，L2）。\n缺陷：但这类方法的假设前提条件太强，需要权重和激活值本身满足一定的分布。\n2. 剪枝流程优化\n当前大部分框架都是逐层进行剪枝，而没有让各层之间进行联动。\n在当前阶段冗余的模块，并不意味着对其他阶段也是冗余的。\n以NISP为代表的方法就通过反向传播来直接对整个网络神经元的重要性进行打分，一次性完成整个模型的剪枝。\n3. 个性化剪枝\n模型在剪枝完后进行推理时不会发生变化，即对于所有的输入图片来说都是一样的计算量，但是有的样本简单，有的样本复杂。\n----&gt;动态推理框架，可以对不同的输入样本图配置不同的计算量，剪枝框架也可以采用这样的思路。\n以Runtime Neural Pruning 为代表。\n4. 自动化剪枝\n在提取低级特征的参数较少的第一层中剪掉更少的参数，对冗余性更高的FC层剪掉更多的参数。\n除此之外，还有训练前剪枝，注意力机制增强等等许多方向。\nAutoML for Model Compression(AMC)是其中的代表。\nReference\n\n机器之心——剪枝：剪枝（决策树，神经网络，搜索）的介绍、历史和发展分析\n概览：快速入门神经网络剪枝！\n\n一些词语解释\n\n从头训练(Trrain From Scratch)：指只保留剪枝后的模型的结构，而不使用其剪枝后的权重。并随机初始化权重，再进行训练（通常使用和训练大模型时相同的学习率计划）。\n微调(Finetune)：剪枝后的模型使用小学习率继续训练。\n\n补充一点模型压缩（中的剪枝）\n可以理解成剪枝的上层概念。目的和背景基本和剪枝是一致的，压缩模型以最大限度地减小模型对于计算空间和时间的消耗。而且它是软件方法，应用成本低，而且与硬件加速方法并不矛盾，可以相互加成。\n目前方法\n从数据，模型和硬件多维度的层面来分析，压缩和加速模型的方法\n1、 压缩已有的网络，包含：张量分解，模型剪枝，模型量化；（针对既有模型）\n2、 构建新的小型网络，包含：知识蒸馏，紧凑网络设计；（针对新模型）\n\n剪枝（Pruning）、量化（Quantization）、低秩分解（Low-rank factorization）、知识蒸馏（Knowledge distillation）\n1）pruning 2）quantization 3） knowledge distillation 4）low-rank decomposition 5）compact architecture design\n\n剪枝思路\n\n最简单的启发就是按参数（或特征输出）绝对值大小来评估重要性，这种方法叫magnitude-based weight pruning。（通常靠L1、group LASSO、activation）\n\n有个假设是参数绝对值越小，其对最终结果影响越小。我们称之为’‘smaller-norm-less-important’'准则。然而这个假设未必成立（如2018年的论文《Rethinking the Smaller-Norm-Less-Informative Assumption in Channel Pruning of Convolution Layers》有对其的讨论）。\n\n考虑参数裁剪对loss的影响。\n\n其实前面提到的始祖级的OBD和OBS就是属于此类。但这两种方法需要计算Hessian矩阵或其近似比较费时。近年来有一些基于该思路的方法被研究和提出。如2016年论文《Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning》也是基于Taylor expansion，但采用的是目标函数相对于activation的展开式中一阶项的绝对值作为pruning的criteria。这样就避免了二阶项（即Hessian矩阵）的计算。2018年论文《SNIP: Single-shot Network Pruning based on Connection Sensitivity》将归一化的目标函数相对于参数的导数绝对值作为重要性的衡量指标。\n\n考虑对特征输出的可重建性的影响，即最小化裁剪后网络对于特征输出的重建误差。它的intuition是如果对当前层进行裁剪，然后如果它对后面输出还没啥影响，那说明裁掉的是不太重要的信息。\n\n典型的如2017年论文《ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression》和《Channel pruning for accelerating very deep neural networks》都是通过最小化特征重建误差（Feature reconstruction error）来确定哪些channel需要裁剪。前者采用贪心法，后者用的LASSO regression。2017年论文《NISP: Pruning Networks using Neuron Importance Score Propagation》提出只考虑后面一两层啥的不够，于是提出NISP（Neuron importance score propagation）算法通过最小化分类网络倒数第二层的重建误差，并将重要性信息反向传播到前面以决定哪些channel需要裁剪。2018年论文《Discrimination-aware Channel Pruning for Deep Neural Networks》提出一种比较有意思的变体。它提出DCP（Discrimination-aware channel pruning）方法一方面在中间层添加额外的discrimination-aware loss（用以强化中间层的判别能力），另一方面也考虑特征重建误差的loss，综合两方面loss对于参数的梯度信息，决定哪些为需要被裁剪的channel。\n\n基于其它的准则对权重进行重要性排序。\n基于梯度的方法。回顾上面问题定义中的数学最优化问题，其最恶心的地方在于regularizer中那个L0-norm，使目标不可微，从而无法用基于梯度的方法来求解。如2017年的论文《Learning Sparse Neural Networks through L0 Regularization》的思路是用一个连续的分布结合 hard-sigmoid recification去近似它，从而使目标函数平滑，这样便可以用基于梯度的方法求解。\n\nSparsity Ratio\n上面研究的是给定裁剪量的前提下如何做pruning，如采用什么样的criteria去做参数选取。然而其实还有个核心问题是在哪里裁剪多少，即sparsity ratio的确定。这里的sparsity ratio定义为层中为0参数所占比例，有些文章中也称为pruning rate等。从目标结构或者sparsity ratio的指定方式来说，按2018年论文《Rethinking the Value of Network Pruning》中的说法可分为预定义（predifined）和自动（automatic）两种方式。Predefined方法由人工指定每一层的比例进行裁剪，因此目标结构是提前确定。而automatic方法会根据所有的layer信息（即全局信息）由pruning算法确定每层裁剪比例，因此目标结构一开始并不确定。\n精度恢复\n当模型经过pruning，一般会带来精度损失，因此我们在pruning的同时也需要考虑精度的恢复：前面提到的论文《Channel Pruning for Accelerating Very Deep Neural Networks》中在进行channel pruning后，直接通过least square来得到最小化特征重建精度下的新参数，因此不需要fine-tuning来恢复精度，是一种inference-time pruning方法。\n人们发现裁完后进行fine-tuning可以弥补pruning带来的精度损失，因此很多方法会在pruning后做fine-tuning。比较经典的是training，pruning，fine-tuning三段式。后面两个阶段交替进行，每次pruning后损失的精度可以由后面的fine-tuning来弥补，该过程也称为iterative pruning。简单说就是砍一刀回点血，再砍一刀再回点血这样，不一步到位是因为有些实验表明一下子砍太狠就难回血了。当然现实中可以衍生出很多玩法。而在时间维度上，每步砍多少也是有艺术的。\n\n如2017年论文《To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression》提出一种automated gradual pruning算法，它基于开始阶段冗余多可以裁得快，越到后面越裁得快的指导思想，给出在n步的pruning中，如何从初始sparsity ratio渐变到目标sparsity ratio的方法。\n\n重新审视假设\n比较有意思的是，最近不少工作开始反思一些之前固有的假设。比如一开始提到的over-parameterization对训练是否真的那么有益，还有原网络的权重是否在pruning中很重要。\n\n在ICLR2019的best paper《The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks》提出The Lottery Ticket Hypothesis，即一个随机初始化，密集的网络包含一个子网络，这个子网络如果沿用原网络的权重初始化，在至多同样迭代次数训练后就可以比肩原网络的测试精度。同时它还给出了找这种子网络结构的方法。文章认为这个子结构和它的初始值对训练的有效性至关重要，它们被称为『winning logttery tickets』。\n另一篇论文2018年的《Rethinking the Value of Network Pruning》提出不仅over-parameterization对于训练不是重要的，而且从原网络中重用其权重也未必是很好的选择，它可能会使裁剪后的模型陷入局部最小。如果原网络的权重或其初始值不重要的话，那剩下最重要的就是pruning后的网络结构了。换句话说，某种意义上来说，pruning即是neural architecture search（NAS），只是由于它只涉及层的维度，搜索空间相比小一些。但这也是它的优点，搜索空间小了自然搜索就高效了。（详见：为什么要压缩模型，而不是直接训练一个小的CNN？的高赞回答）\n\n保留模型Capacity\n之前的主流pruning方法中，一旦有参数被不适当地裁剪掉，便无法被恢复。而这两年，学界正在尝试在模型压缩过程中保留被裁剪部分能力或者扩充能力的方法。\n\n2018年论文《Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks》提出SFP（Soft filter pruning）让被裁剪的filter在训练中仍能被更新，这样它仍有机会被恢复回来。\n2016年论文《Dynamic Network Surgery for Efficient DNNs》在pruning的基础上加了splicing操作，避免不合适的pruning带来的影响。\n2017年的论文《Morphnet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks》也是基于这种思路，它会迭代地进行shrink和expand的操作。\n\nPruning的副作用就是可能会损害模型的capacity。尽管前面的各种方法让该影响尽可能小，但我们往往只能在有限的数据集上做。因此，很可能对于大部分简单的样本pruning对其没有影响，但对于小部分难的数据会有较大影响。那有没有可能在保留网络capacity的同时又能精简计算呢？一些学者尝试结合dynamic NN来达到该目的，即网络执行哪些模块由输出决定。\n\n如2017年论文《Dynamic Deep Neural Networks: Optimizing Accuracy-Efficiency Trade-offs by Selective Execution》引入Dynamic Deep Neural Networks，对于给定输入，哪部分神经元被执行由网络本身中的controller module来确定。这个module通过强化学习进行训练。\n另一篇2017年论文《Runtime Neural Pruning》将pruning建模为马尔可夫决策过程（Markov decision process）并通过强化学习来学习pruning策略。当然，这类方法也有些局限，如由于保留了网络原始capacity，因此size不会减少。另外由于执行哪部分是动态的，因此对于硬件加速会有影响（如算子间的fusion）。\n\n方向\n方向一\n如前面提到，network pruning方法与NAS的界限已经模糊了。事实上，NAS分支上也有一类搜索加速方法，如One-Shot Architecture Search是先有一个大网络，然后做减法。NAS与模型压缩两个一开始看似关系不是那么大的分支最后似乎走到一块去了。这两个分支今天有了更多的交集，也必将擦出更多的火花。\n方向二\n挑战已有的固有的假设。如前面对于over-parameterization与重用已有参数是否有有益的反思非常有意思。这样的工作会给我们非常大的启发，从而根本改变解决问题的思路。\n方向三\n随着AutoML的大潮，越来越多的东西开始自动化。模型压缩能拉下吗？当然不能。经过前面的介绍我们知道，像ADC，RNP，N2N Learning这些工作都是试图将pruning中部分工作自动化。而且对于其它的模型压缩方法，如quantization，也有一些空间可以自动化，如2018年论文《HAQ: Hardware-Aware Automated Quantization》考虑网络中不同层信息的冗余程度不一样，因此可以用不同位数进行量化。\n方向四\n这几年机器学习最火热的分支之一GAN，正在不断渗透到已有领域，在pruning中也开始有它的身影。如2019年论文《Towards Optimal Structured CNN Pruning via Generative Adversarial Learning》采用了GAN的思想，让generator生成裁剪后网络，discrimintor来判别是否属于原网络还是裁剪后网络，从而进行更有效的网络结构化裁剪。\n方向五\n与硬件结合，如稀疏计算的支持。现在虽然有像cuSPARSE这样的计算库，但底层硬件GPU本身设计并不是专门为稀疏数据处理打造的。如果能将稀疏计算和处理能力做进芯片那必将极大提高计算效率，如早些年有像EIE这样的尝试。在现在神经网络芯片的大潮下，相信这样的案例会越来越多。\n方向六\n如文章一开始提到的，模型压缩方法中pruning只是其中一种，它与其它方法并不冲突，因此与其它方法，如knowledge distillation，quantization等的深度结合，是值得研究的方向。和其它机器学习分支一样，很多人提出很多算法，各家都说自家的好。一个分支发展到一定时候，就需要benchmark来进行客观的横向比较。Google在2019年论文《The State of Sparsity in Deep Neural Networks》正是这方面的尝试。相信以后也会有越来越多的benchmark，和针对性的竞赛。\n更多细节详见：深度学习网络模型压缩剪枝详细分析\n其他资源\n\nDistiller：一个使用PyTorch实现的剪枝工具包\n\n","categories":["剪枝"]},{"title":"元分析链接汇总","url":"/2023/02/18/%E6%95%99%E8%82%B2%E7%A7%91%E7%A0%94%E6%96%B9%E6%B3%95/%E5%85%83%E5%88%86%E6%9E%90%E9%93%BE%E6%8E%A5%E6%B1%87%E6%80%BB/","content":"\n元分析教程 | 手把手教你使用CMA 2.0：工具使用\n系统性文献综述&amp;元分析数据分析及报告的29个建议\nMeta分析异质性大，如何处理？\n心理学meta分析/元分析简介\n\n","categories":["教育科研方法"]},{"title":"Apriori","url":"/2022/04/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0-Apriori/","content":"目前常用的关联规则方法有很多，如贝叶斯网络、决策树、Apriori算法等。其中Apriori算法是关联规则挖掘频繁项集的经典算法，最早由R.Agrawal等人在1993年提出来的，是一种挖掘单维布尔型的关联规则算法，很多算法也是以其为核心进行改进的。\n1 基本介绍\n1.1 概述\nApriori算法是一种通过频繁项集来挖掘关联规则的算法。该算法既可以发现频繁项集，又可以挖掘物品之间关联规则。分别采用支持度和置信度来量化频繁项集和关联规则。其核心思想是通过候选集生成和情节的向下封闭检验检测两个阶段来挖掘频繁项集。\n其最常见的改进算法为AprioriTid算法，该改进算法与原算法的主要区别在于对数据集的更新方式不一样。当数据量较大时，使用改进算法得到的新数据集会比原始数据集小很多，这样在进行遍历的时候就节省了很多时间。\n1.2 相关概念\n（1）项（Item）：是关联规则最基础的元素，用 \\(i_n（n=1,2,3…..）\\) 表示某一项。\n（2）项集：是项的集合，假设项集 \\(I={i_1，i_2,...,i_n}\\)，包括 \\(n\\) 项的项集成为 \\(n\\) 项集。\n（3）事务（TID）：是由一个唯一的事务标识号和一个组成事务的项的列表构成的。\n（4）事务集：是所有事务构成的集合。假设事务数据集 \\(T=\\{t_1,t_2，..., t_n\\}\\)\n（5）关联规则：可以表示为行如所示的蕴涵式。其中，\\(X \\subset I,Y\\subset I\\) 且 \\(X \\cap Y = \\emptyset\\)。该关系式表示如果项集 \\(X\\) 在某一事物中出现时，那么项集 \\(Y\\) 也可能以某一概率出现在该事务中。\n（6）支持度：是某一项集的频繁程度，是关联规则重要性的衡量准则，用于表示该项集的重要性。假设包含项集X、Y的事务数量与事务集总数countAll的比值,反映了X、Y项集同时出现的频率。 \\[\nsupport(X\\Rightarrow Y)=\\frac{count(X\\cup Y)}{countAll}\n\\] （7）置信度：用来确定Y在包含X的事务中出现的频繁程度，即Y在X条件下的条件概率，是对关联规则准确度的衡量准则，表示规则的可靠程度。假设包含项集X、Y的事务数量与项集X事务数量的比值。 \\[\nConfidence(X\\Rightarrow Y)=\\frac{support(X\\Rightarrow Y)}{support(X)}=\\frac{count(X\\cup Y)}{count(X)}\n\\] （8）频繁项集与最小支持度：最小支持度是预先设置好的项集满足支持度的下限，用Min_sup表示，反映了所关注的项集的最低重要性。当项集X的支持度不小于最小支持度阈值时，X为频繁项集。 \\[\nsupport(X)=\\frac{count(X)}{countAll} \\geq min_{sup}\n\\] 频繁项集中有两个重要的特性：\n\n如果一个项集是频繁的，那么它的所有非空子集都是频繁的。\n如果一个项集不是频繁的，那么它的所有超集也必然不是频繁的。\n\nEXAMPLE:\n\n\n\nTID\nItems\n\n\n\n\n1\nBread, Milk\n\n\n2\nBread, Diaper, Beer, Eggs\n\n\n3\nMilk, Diaper, Beer, Coke\n\n\n4\nBread, Milk, Diaper, Beer\n\n\n5\nBread, Milk, Diaper, Coke\n\n\n\n\\[\n\\{Milk,Diaper\\} \\Rightarrow Beer\\\\\ns=\\frac{count(Milk,Diaper,Beer)}{countAll}=\\frac{2}{5}\\\\\nc=\\frac{count(Milk,Diaper,Beer)}{count(Milk,Diaper)}=\\frac{2}{3}\n\\]\n（9）强关联规则与最小置信度：最小置信度是预先设置好的项集满足置信度的下限，用 \\(Min_{conf}\\) 表示，反映了所关注的项集的最低可靠程度。当关联规则 \\(R\\) 同时满足支持度与置信度不小于最小阈值，则称其为强关联规则： \\[\n\\left\\{\\begin{array}{}\nSupport(X\\Rightarrow Y)\\geq min_{sup}\\\\\nConfidence(X\\Rightarrow Y)\\geq min_{conf}\n\\end{array}\n\\right.\n\\] 挖掘关联规则的主要任务就是为了找出满足条件的各种强关联规则。\n（10）提升度：置信度与期望可信度的比值 \\[\nlift(X\\Rightarrow Y)=\\frac{count(X \\cup Y)}{count(X)count(Y)}=\\frac{Confidence(X\\Rightarrow Y)}{support(Y)}\n\\]\n说明了物品集X的出现，对物品集Y出现的概率有多大的变化；\n\n正常数值区间大于1；\n如果lift=1 说明X,Y事件为互相独立事件，物品集X的出现不会改变物品集Y出现的概率；\n\n1.3 优点\n[1] 该算法的关联规则关联规则是在频繁项集基础上产生的，这可以保证这些规则的支持度达到指定的水平，具有普遍性和令人信服的水平；\n[2] 算法简单，易于理解，对数据的要求低。\n1.4 缺点\n[1] 在每一步产生候选项目集的时候循环产生的组合过多，没有排除不应参与组合的项；\n[2] 每次计算项集的支持度的时候，都对数据库中的全部数据进行了一遍扫描比较，I/O负载很大。\n2 算法流程\n2.1 问题说明\n【已知】：数据集 \\(X=(x_1,x_2,...,x_n)\\) ，其中每一个 \\(x_i\\) 对应 \\(k\\) 个变量（即 \\(k\\) 个统计指标），如果该变量的取值满足设定的条件， 则\\(x_i\\) 在该变量上可转化成分类数据或者 \\(0-1\\) 式数据。\n【待求】：数据集的频繁项集和关联规则。\n2.2 算法步骤（文字描述版）\n[1] 第一步：输入数据集 \\(X\\)\n[2] 第二步：确定数据集 \\(X\\) 中所包含的项集，并具体化到每一个数据点（即每一个 \\(x_i\\) 中所包含的变量情况）\n[3] 第三步：进行第一次迭代，把每个项集中的项目单独扫描统计（即某个项在多少个数据点中出现了），将每个项都作为候选 \\(1-\\) 项集 \\(C_1\\) 的成员，并计算每个项的支持度\n[4] 第四步：设定最小支持度，根据候选 \\(1-\\) 项集 \\(C_1\\) 的成员、支持度和最小支持度，采用扫描过滤的方式得候选 \\(2-\\) 项集 \\(C_2\\) ，候选项集需满足所有真子集的支持度都 \\(≥\\) 最小支持度\n[5] 第五步：保持最小支持度不变,重复进行第四步，直到没有办法再合并(即候选项集无法满足条件)，形成新的候选项集，此时输出最终的频繁项集结果并给关联规则。\n机器学习—关联规则分析之Apriori算法及其python实现\n数据挖掘关联规则挖掘改进算法DHP\n参考：\n数据挖掘十大算法—— Apriori：算法步骤（数学描述）和例子、R语言\n数据分析方法—Apriori算法简介：例子好像错了\n","categories":["机器学习小记"]},{"title":"Dropout","url":"/2022/04/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0-Dropout/","content":"什么是Dropout\n训练一个大型的网络，训练数据很少则很容易引起过拟合，可能我们会想到用L2正则化、或者减小网络规模。然而深度学习领域大神Hinton认为过拟合，可以通过阻止某些特征的协同作用来缓解，他在2012年文献：《Improving neural networks by preventing co-adaptation of feature detectors》提出：在每次训练的时候，让一半的特征检测器停过工作（每个神经元有百分之50的几率被移除，这样可以让一个神经元的出现不应该依赖于另外一个神经元），以此提高网络的泛化能力，称为dropout。\n另外，我们可以把dropout理解为 模型平均。 假设我们要实现一个图片分类任务，我们设计出了100000个网络，这100000个网络，我们可以设计得各不相同，然后我们对这100000个网络进行训练，训练完后我们采用平均的方法，进行预测，这样肯定可以提高网络的泛化能力，或者说可以防止过拟合，因为这100000个网络，它们各不相同，可以提高网络的稳定性。而所谓的dropout我们可以这么理解，这n个网络，它们权值共享，并且具有相同的网络层数(这样可以大大减小计算量)。我们每次dropout后，网络模型都可以看成是整个网络的子网络。(需要注意的是如果采用dropout，训练时间大大延长，但是对测试阶段没影响)。\n\n计算公式： \\[\nz_i^{(l+1)}=w_i^{(l+1)}y^l+b_i^{(l+1)},\\\\\ny^{(l+1)}=f(z^{(l+1)}_i)\n\\] 变成了： \\[\nr_j^{l}\\sim Bernoulli(p) \\\\\n\\pmb{\\widetilde y}^{(l)}=\\pmb{r}^{(l)}*\\pmb{y}^{(l)}\\\\\nz_i^{(l+1)}=\\pmb{w}_i^{(l+1)}\\pmb{\\widetilde y}^l+b_i^{(l+1)},\\\\\ny^{(l+1)}=f(z^{(l+1)}_i)\n\\] 上面公式中Bernoulli函数，是为了以概率p，随机生成一个0、1的向量。\n算法实现概述：\n\n设置dropout比例，确定多少检测器停止工作\nrescale，乘以 \\(1/(1-p)\\)\n\n源码实现\n#dropout函数的实现def dropout(x, level):\tif level &lt; 0. or level &gt;= 1:#level是概率值，必须在0~1之间\t    raise Exception(&#x27;Dropout level must be in interval [0, 1[.&#x27;)\tretain_prob = 1. - level    #我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样\t#硬币 正面的概率为p，n表示每个神经元试验的次数\t#因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。\tsample=np.random.binomial(n=1,p=retain_prob,size=x.shape)#即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了\tprint sample\tx *=sample#0、1与x相乘，我们就可以屏蔽某些神经元，让它们的值变为0\tprint x\tx /= retain_prob \treturn x#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)dropout(x,0.4)&lt;/span&gt;\n函数中，x是本层网络的激活值。Level就是dropout就是每个神经元要被丢弃的概率。\n在Dropout之前，正则化是主要的用来缓解模型过拟合的策略，例如l1正则和l2正则。但是它们并没有完全解决模型的过拟合问题，原因就是网络中存在co-adaption（共适应）问题。\n所谓co-adaption，是指网络中的一些节点会比另外一些节点有更强的表征能力。这时，随着网络的不断训练，具有更强表征能力的节点被不断的强化，而更弱的节点则不断弱化直到对网络的贡献可以忽略不计。这时候只有网络中的部分节点才会被训练，浪费了网络的宽度和深度，进而导致模型的效果上升收到限制。\n而Dropout的提出解决了co-adaption问题，从而使得训练更宽的网络成为可能。\n参考\n\n深度学习（二十二）Dropout浅层理解与实现\n详细推导见：Dropout详解\n\n","categories":["机器学习小记"]},{"title":"机器学习中的判别模型和生成模型","url":"/2022/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/","content":"机器学习模型可以分为 两类模型——判别模型和生成模型。简而言之，判别模型基于条件概率对看不见的数据进行预测，可用于分类或回归问题陈述。相反，生成模型专注于数据集的分布，以返回给定示例的概率。\n\n\n\n问题表述\n假设我们正在处理一个分类问题，我们的任务是根据特定电子邮件中出现的单词来确定电子邮件是垃圾邮件还是非垃圾邮件。为了解决这个问题，我们有一个联合模型\n\n标签： \\(Y=y\\)\n特征： \\(X={x_1 , x_2 , …x_n }\\)\n\n因此，模型的联合分布可以表示为 \\[\np(Y,X) = P(y,x_1 ,x_2 …x_n)\n\\] 现在，我们的目标是估计垃圾邮件的概率，即 \\(P(Y=1|X)\\)。生成模型和判别模型都可以解决这个问题，但方式不同。\n生成模型的方法\n在生成模型的情况下，为了找到条件概率\\(P(Y|X)\\)，他们在训练数据的帮助下估计先验概率\\(P(Y)\\)和似然概率 \\(P(X|Y)\\)\n并使用贝叶斯定理计算后验概率 \\(P(Y |X)\\)： \\[\nposterior=\\frac{prior \\times likelihood}{evidence} \\rightarrow P(Y|X)=\\frac{P(Y) \\cdot P(X|Y)}{P(X)}\n\\]\n判别模型的方法\n在判别模型的情况下，为了找到概率，他们直接假设\\(P(Y|X)\\)的某种函数形式，然后在训练数据的帮助下 估计\\(P(Y|X)\\)的参数。\n什么是判别模型？\n判别模型是指统计分类中使用的一类模型，主要用于有监督的机器学习。这些类型的模型也称为条件模型，因为它们学习数据集中类或标签之间的边界。\n判别模型（就像字面意思一样）分离类而不是对条件概率进行建模，并且不对数据点做出任何假设。但是这些模型无法生成新的数据点。因此，判别模型的最终目标是将一个类与另一类分开。\n如果数据集中存在一些异常值，则判别模型与生成模型相比效果更好，即判别模型对异常值更稳健。然而，这些模型的一个主要缺点是 错误分类问题，即错误地分类数据点。\n判别模型中涉及的数学问题\n‌训练判别分类器涉及估计函数\\(f: X -&gt; Y\\)或概率\\(P(Y|X)\\)\n\n假设概率的某种函数形式，例如 \\(P(Y|X)\\)\n在训练数据的帮助下，我们估计了\\(P(Y|X)\\)的参数\n\n判别模型的一些例子\n\nLogistic regression\nScalar Vector Machine (SVMs)\n‌Traditional neural networks\n‌Nearest neighbor\nConditional Random Fields (CRFs)\nDecision Trees and Random Forest\n\n什么是生成模型？\n生成模型被认为是一类可以生成新数据实例的统计模型。这些模型用于无监督机器学习，作为执行任务的一种手段，例如\n\n概率和似然估计，\n建模数据点，\n为了描述数据中的现象，\n根据这些概率区分类别。\n\n由于这些类型的模型通常依靠贝叶斯定理来找到联合概率，因此生成模型可以处理比类似判别模型更复杂的任务。\n因此，生成模型专注于数据集中各个类的分布，而学习算法倾向于对数据点的基本模式或分布进行建模。这些模型使用联合概率的概念，并创建给定特征 ( x )或输入和所需输出或标签 ( y )同时存在的实例。\n这些模型使用概率估计和可能性对数据点进行建模，并区分数据集中存在的不同类别标签。与判别模型不同，这些模型还能够生成新的数据点。\n但是，它们也有一个主要缺点——如果数据集中存在异常值，那么它会在很大程度上影响这些类型的模型。\n生成模型中涉及的数学问题\n‌训练生成分类器涉及估计函数 \\(f: X \\rightarrow Y\\)或概率 \\(P(Y|X)\\)\n\n假设概率的某种函数形式，例如 \\(P(Y), P(X|Y)\\)\n借助训练数据，我们估计 \\(P(X|Y)、P(Y)\\)的参数\n使用贝叶斯定理计算后验概率 \\(P(Y |X)\\)\n\n生成模型的一些示例\n\nNaive Bayes\nBayesian networks\nMarkov random fields\n‌Hidden Markov Models (HMMs)\nLatent Dirichlet Allocation (LDA)\nGenerative Adversarial Networks (GANs)\nAutoregressive Model\n\n判别模型和生成模型之间的区别\n核心理念\n判别模型在数据空间中绘制边界，而生成模型试图模拟数据在整个空间中的放置方式。生成模型侧重于解释数据是如何生成的，而判别模型侧重于预测数据的标签。\n数学直觉\n用数学术语来说，判别式机器学习通过学习最大化条件概率P(Y|X)的参数来训练模型，而另一方面，生成模型通过最大化P(X,Y）。\n应用\n判别模型识别现有数据，即判别建模识别标签并对数据进行排序，并可用于对数据进行分类，而生成建模则产生一些东西。\n由于这些模型使用不同的机器学习方法，因此两者都适用于特定任务，即生成模型对无监督学习任务有用，而判别模型对监督学习任务有用。\n异常值\n生成模型比判别模型对异常值的影响更大。\n计算成本\n与生成模型相比，判别模型的计算成本较低。\n判别模型和生成模型之间的比较\n基于性能\n与判别模型相比，生成模型需要更少的数据来训练，因为生成模型在做出更强的假设（即条件独立性的假设）时更具偏见。\n基于缺失数据\n一般来说，如果我们的数据集中有缺失的数据，那么生成模型可以处理这些缺失的数据，而判别模型则不能。这是因为，在生成模型中，我们仍然可以通过边缘化看不见的变量来估计后验。然而，对于判别模型，我们通常要求观察所有特征 X。\n基于准确度得分\n如果违反条件独立性的假设，那么在那个时候生成模型不如判别模型准确。\n基于应用\n判别模型被称为“判别” ，因为它们对于区分 Y 的标签（即目标结果）很有用，因此它们只能解决分类问题，而生成模型除了分类之外还有更多应用，例如，\n\nSamplings,\nBayes learning,\nMAP inference, etc.\n\n参考\n如果您想深入了解判别模型和生成模型的深层概念，请阅读Andrew Ng 教授的以下论文。\n生成和判别模型\n原文连接：深度理解机器学习中的判别模型和生成模型\n","categories":["机器学习小记"]},{"title":"分类任务及一点决策树","url":"/2022/03/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0-%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E5%8F%8A%E4%B8%80%E7%82%B9%E5%86%B3%E7%AD%96%E6%A0%91/","content":"\n\n\n\n\n\n\n\n一些链接：\n\n【机器学习】决策树（上）——ID3、C4.5、CART（非常详细）\n【决策树算法1】ID3算法 数据挖掘 期末考试 计算题 详细步骤讲解\n什么是决策树（Decision Tree）？【知多少】\n\n","categories":["机器学习小记"]},{"title":"学习率衰减","url":"/2023/02/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B0%8F%E8%AE%B0/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/","content":"torch.optim.lr_scheduler.StepLR\nclass torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)\n\noptimizer （Optimizer）：要更改学习率的优化器；\nstep_size（int）：每训练step_size个epoch，更新一次参数；\ngamma（float）：更新lr的乘法因子；\nlast_epoch （int）：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始。\n\n","categories":["机器学习小记"]},{"title":"Learning both Weights and Connections for Efficient Neural Networks","url":"/2023/03/13/%E5%89%AA%E6%9E%9D/Learning-both-Weights-and-Connections-for-Efficient-Neural-Networks/","content":"Learning both Weights and Connections for Efficient Neural Networks链接：论文  code\n这篇论文是 Han Song 在模型压缩方面很重要的论文。\n\n目的：减少存储量和计算量，使网络更适合在移动设备上运行\n\n方法：简单地说是只学习重要的连接，修剪冗余的连接减少模型参数\n\n步骤：train，学习哪些连接是重要的　　　prune，剪去不重要的连接   　　　retrain，finetune剩下的连接的参数   　　　prune和retrain可以反复迭代进行\n\n结果：AlexNet减少模型参数量61million-&gt;6.7million，没有精度损失　　　VGG-16减少模型参数量138million-&gt;10.3million，没有精度损失\n\nLottery Ticket Hypothesis，翻译成彩票假设，它的核心思想是说传统的剪枝技术，虽然能够剪掉网络参数的90%以上，大幅减小模型尺寸，提升推理的效率同时也不咋影响精度，但是剪出来的稀疏的网络存在一个问题，就是很不好从头训练。这句话具体是说：如果现在得到了剪枝之后的网络，我们如果直接从大网络中继承权重去训练，那么就可以得到相似的性能。但是，如果是去从头训练这个网络，就无法得到相似的性能，而且训练过程不如上一种训练顺滑。\n\n\n\n背景——为什么要剪枝？CNN模型虽然很好用，但是面临着模型比较大，在手机等移动设备上时，对能耗和网络带宽都是负担。因此需要剪枝去减小算力的消耗，节省资源。\n论文细节剪枝流程论文的剪枝方法包括三步，见图1，和卷积训练不同，模型学习的是连接的重要性，而不是权重（虽然权重大小表征了重要性）， 接着我们对那些比较小的权重连接进行剪枝，并移除在网络上的连接，这样是把一个 dense 网络变成了 sparse 网络，如图1右所示。最后对保留下来的权重连接进行再训练，这一步是比较关键的，如果不进行再训练，精度损失可能比较大。\n\n\n伪代码：\n\n\n剪枝过程 Regularization 的选取选择一个正确的正则项会你对剪枝和再训练性能有所影响，使用不同的正则方法会对pruning和retraining产生影响。L1和L2都可以用来做正则，惩罚模型的复杂度，L1正则会惩罚那些非零参数，导致更多的参数为零，这个特性就导致在剪枝后再训练之前的精度比L2好。但是留下来的连接却不如使用L2正则的好。实验发现，采用L2做正则项较好。即：\n\n在剪枝后，再训练之前：L1的精度比L2好。\n留下来的连接：使用L2正则更好。\n\n\n\nDropout 的影响Dropout 是一项常见的防止过拟合的技术。要注意的是，在retraining的时候，我们需要对Dropout ratio做出调整。因为网络中的很多连接都被剪枝剪下来了，模型容量变小，网络减小了过拟合，所以dropout的比例要变小。\n下面给出定量的估计。\n对于FC层来说，如果第 $i$ 层的神经元个数是 $N_i$，那么该层的连接数 $C_i$ 如公式1，dropout是作用于神经元的（dropout是将 $N_i$ 个神经元输出按照概率dropout掉）\n\nC_i=N_iN_{i-1}\\\\D_r=D_o\\sqrt{\\frac{C_{ir}}{C_{io}}}其中，下标 $r$ 表示 retraining，$o$ 表示初始模型。\nLocal Pruning and Parameter Co-adaptation再训练期间，最好保留从训练模型剪枝余下的连接的权重，而不是重新初始化剪枝后的层。其实这个也是绝大多数剪枝的操作。（这里其实后期的论文讨论还蛮多，到底是权重重要还是网络的架构重要）\n深层网络容易梯度消失，难以恢复精度。为了解决这一问题，我们在训练FC层的时候，可以将CONV的参数固定住。反之亦然。\nIterative Pruning迭代地进行多次剪枝配合重新训练效果会很好，多次迭代剪枝能达到9倍的压缩，而单步压缩只能达到5倍。\n对神经元进行剪枝一些输入或者输出为零的 neurons 也会被剪枝.\nFC层比conv的敏感度更低FC层的参数量比较多，剪枝后的损失会更小，因此可以根据敏感度进行设置剪枝比率。\n实验结果作者在caffe做了剪枝的实验，添加了mask来表示剪枝，它会忽略剪枝过的参数，pruning threshold 选择了和层权重标准差乘系数的方式。使用Caffe实现，需要加入一个mask来表示剪枝。剪枝的阈值，是该layer的权重标准差乘上某个超参数。最终在mnist和imagenet上的效果：\n\n\n\n\n最后是将剪枝过的层作为稀疏矩阵存储，其存储开销仅为原来的15.6%。存储相对索引而不是绝对索引，将FC层索引减少到5位存储，CONV层索引用8位存储。这样可以大大减少存储的大小，其实这样在预测阶段需要进行还原，可能耗时上得不偿失。\nReference\n论文分享｜从零入门剪枝（一）\n\n","categories":["剪枝"]},{"title":"To prune, or not to prune exploring the efficacy of pruning for model compression","url":"/2023/03/13/%E5%89%AA%E6%9E%9D/To-prune,-or-not-to-prune-exploring-the-efficacy-of-pruning-for-model-compression/","content":"To prune, or not to prune: exploring the efficacy of pruning for model compression\n链接：论文 code\n作者是斯坦福和谷歌的大牛，提出了一个问题：在给定模型内存占用的情况下，我们如何才能得出最准确的模型？\n作者对比以下模型：\n\n对大模型通过剪枝来获取只有少量参数非零对稀疏模型(large-sparse)\n训练一个 small-dense 模型，但是模型大小和large-sparse相当.\n\n这两种方法权衡了模型准确性和模型大小，但是就其对基础硬件体系结构的设计而言，它们的意义却截然不同。作者选择了多个应用领域的模型：图像识别任务的InceptionV3和MobileNets，用于语言建模的Stack LSTM和用于Google机器翻译系统的seq2seq模型。提出了一种简单的逐步剪枝的方法，该方法需要最小的调整，并且可以无缝地整合到训练过程中，并在各种神经网络体系结构上证明其适用性和性能。\n具体方法\n近水楼台，作者在tensorflow上进行了训练时候的剪枝。对于每一层，作者添加了二值的mask，和权重的维度都是一样的，来决定那些权重会参与前向的运算。\n我们将ops注入TensorFlow训练图中以按其绝对值对该层中的权重进行排序，并将最小权重屏蔽为零，直到达到预设的稀疏度s为止。反向传播的时候，考虑到梯度，梯度同样会被二值的mask遮蔽，前向被mask的权重也不会在反向传播的时候进行更新的。\n作者引入了一个新的 automated gradual pruning algorithm，稀疏度在 \\(n\\) 个跨度内，从原始值 \\(s_i\\)（通常为0）增加到最终的稀疏度 \\(s_f\\) ， 剪枝间隔为 \\(Δt\\)，共进行 \\(n\\) 次： \\[\ns_t=s_f+(s_i-S_f)(1-\\frac{t-t_0}{nΔt})^3 \\text{   }\\text{ }for \\text{ }\\text{   }t \\in \\{t_0,\\text{   }t_0+Δt,\\text{   }\\dots,\\text{   }t_0+nΔt\\}\n\\]\n二值的mask每隔 \\(∆t\\) 步进行更新，这样会减小由剪枝带来的损失，一旦稀疏率达到目标值，二值mask将不会进行变化了。当冗余连接丰富时，可以在初始阶段快速修剪网络，并随着权重越来越少而逐渐减少每次修剪的权重数量，如图所示。即，它基于开始阶段冗余多可以裁得快，越到后面越裁得快的指导思想，给出在n步的pruning中，如何从初始sparsity ratio渐变到目标sparsity ratio的方法。\n一般来说，剪枝的模型要么是经过几个epoch的训练要么直接是预训模型。还有要注意学习率和剪枝率的配合。\n\n\n\n结论\n比较了large模型经过prune之后得到的large-sparse模型和相似memory footprint但更compact-small模型的性能，对于很多网络结构（CNN，stacked LSTM, seq-to-seq LSTM）等，large-sparse 性能更好。注意，这个也是非结构化剪枝，但是思想很容易迁移到结构化剪枝里的。\n启发\n\n人们发现裁完后进行fine-tuning可以弥补pruning带来的精度损失，因此很多方法会在pruning后做fine-tuning。比较经典的是training，pruning，fine-tuning三段式。后面两个阶段交替进行，每次pruning后损失的精度可以由后面的fine-tuning来弥补，该过程也称为iterative pruning。简单说就是砍一刀回点血，再砍一刀再回点血这样，不一步到位是因为有些实验表明一下子砍太狠就难回血了。当然现实中可以衍生出很多玩法。而在时间维度上，每步砍多少也是有艺术的。如文章里的 “它基于开始阶段冗余多可以裁得快，越到后面越裁得快的指导思想，给出在n步的pruning中，如何从初始sparsity ratio渐变到目标sparsity ratio的方法”。\n具有同等参数量的稀疏大模型和稠密小模型的性能对比，在图像和语音任务上表明稀疏大模型普遍有更好的性能。\n\n\n《To prune, or not to prune: exploring the efficacy of pruning for model compression》研究的是不同结构small dense vs. large sparse，《Rethinking the Value of Network Pruning》是相同结构的下fine-tune vs. scratch，所以结论也不是相反\n\nReference\n\n论文分享｜从零入门剪枝（三）\n\n","categories":["剪枝"]}]